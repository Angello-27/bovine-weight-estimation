{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêÑ Sistema de Estimaci√≥n de Peso Bovino - Setup ML\n",
    "\n",
    "> **BLOQUE 0**: Informaci√≥n del proyecto (markdown - solo lectura)\n",
    "\n",
    "**Proyecto**: Hacienda Gamelera - Bruno Brito Macedo  \n",
    "**Responsable**: Persona 2 - Setup Infraestructura ML  \n",
    "**Objetivo**: Preparar datasets y pipeline para entrenamiento de 7 modelos por raza  \n",
    "**Duraci√≥n**: 5-6 d√≠as  \n",
    "\n",
    "---\n",
    "\n",
    "## üìë √çndice de Bloques (Referencia R√°pida)\n",
    "\n",
    "| Bloque | Nombre | Descripci√≥n | Requisitos |\n",
    "|--------|--------|-------------|------------|\n",
    "| **0** | Informaci√≥n | Markdown introductorio | Ninguno |\n",
    "| **1** | Clonar Repositorio | Monta Drive y clona desde GitHub (persistente) | Ninguno (requiere internet) |\n",
    "| **2** | Verificar Dependencias | Verifica versiones base de TensorFlow y NumPy | Ninguno |\n",
    "| **3** | Instalar Dependencias Cr√≠ticas | TensorFlow 2.19.0, NumPy 2.x, MLflow, DVC | Ninguno |\n",
    "| **4** | Instalar Complementos | Albumentations, OpenCV, herramientas ML | Ninguno |\n",
    "| **5** | Configuraci√≥n Proyecto | Crea estructura de carpetas y variables globales | Bloque 1 |\n",
    "| **6** | Descargar Im√°genes Propias | Scraping de im√°genes para dataset personalizado | Bloque 5 |\n",
    "| **7** | Descargar CID Dataset | Descarga CID Dataset desde S3 (complementario) | Bloque 5 |\n",
    "| **8** | Preparar Dataset Combinado | Combina CID + Nuestras im√°genes (Estrategia B) | Bloques 6 + 7 |\n",
    "| **9** | Resumen Datasets | Muestra resumen de datasets disponibles (CID + propias) | Bloques 6-8 |\n",
    "| **10** | Verificaci√≥n R√°pida | Verificaci√≥n m√≠nima de columnas requeridas (OPCIONAL) | Bloque 9 |\n",
    "| **11** | Pipeline de Datos | Pipeline con augmentation | Bloque 8 |\n",
    "| **12** | Arquitectura Modelo | Crea modelo EfficientNetB0 | Bloque 11 |\n",
    "| **13** | Configurar Entrenamiento | Callbacks y MLflow | Bloque 12 |\n",
    "| **14** | Entrenamiento | Entrena modelo (2-4h) | Bloque 13 + GPU |\n",
    "| **15** | Evaluaci√≥n | Eval√∫a modelo | Bloque 14 |\n",
    "| **16** | Exportar TFLite | Exporta modelo a TFLite | Bloque 15 |\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Flujo de Trabajo\n",
    "\n",
    "### D√≠a 1: Setup (Bloques 1-5)\n",
    "- **BLOQUE 1**: Clonar repositorio en Drive\n",
    "- **BLOQUE 2**: Verificar dependencias base\n",
    "- **BLOQUE 3**: Instalar dependencias cr√≠ticas (TensorFlow, MLflow)\n",
    "- **BLOQUE 4**: Instalar complementos (Albumentations, OpenCV)\n",
    "- **BLOQUE 5**: Configurar proyecto y carpetas\n",
    "\n",
    "### D√≠a 2-3: Datasets (Bloques 6-9)\n",
    "- **BLOQUE 6**: Descargar nuestras im√°genes (razas bolivianas, etapas de crianza)\n",
    "- **BLOQUE 7**: Descargar CID Dataset (complementario - 17,899+ im√°genes)\n",
    "- **BLOQUE 8**: Preparar dataset combinado (Estrategia B: combina CID + nuestras im√°genes)\n",
    "- **BLOQUE 9**: Resumen de datasets disponibles (verifica combinaci√≥n)\n",
    "\n",
    "### D√≠a 4: Verificaci√≥n (Bloque 10) - OPCIONAL\n",
    "- **BLOQUE 10**: Verificaci√≥n r√°pida de datos (solo comprueba columnas necesarias, sin gr√°ficos)\n",
    "- üí° **NOTA**: Este bloque es OPCIONAL. Puedes saltarlo para entrenar m√°s r√°pido.\n",
    "\n",
    "### D√≠a 5-6: Pipeline y Modelo (Bloques 11-16)\n",
    "- **BLOQUE 11**: Pipeline de datos con augmentation (usa dataset combinado)\n",
    "- **BLOQUE 12**: Arquitectura del modelo\n",
    "- **BLOQUE 13**: Configurar entrenamiento\n",
    "- **BLOQUE 14**: Entrenar modelo\n",
    "- **BLOQUE 15**: Evaluaci√≥n del modelo\n",
    "- **BLOQUE 16**: Exportar a TFLite\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Razas Objetivo (7 razas)\n",
    "1. **Brahman** - Bos indicus robusto\n",
    "2. **Nelore** - Bos indicus\n",
    "3. **Angus** - Bos taurus, buena carne\n",
    "4. **Cebuinas** - Bos indicus general\n",
    "5. **Criollo** - Adaptado local\n",
    "6. **Pardo Suizo** - Bos taurus grande\n",
    "7. **Jersey** - Lechera, menor tama√±o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 1: CONFIGURAR RUTA DEL PROYECTO Y CLONAR REPOSITORIO EN DRIVE\n",
    "# ============================================================\n",
    "# üìÅ Clona el repositorio desde GitHub a Google Drive (persistente entre sesiones)\n",
    "# üîó Repositorio: https://github.com/Angello-27/bovine-weight-estimation.git\n",
    "# üíæ Se clona en Drive para que persista entre desconexiones del runtime\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "GITHUB_REPO_URL = 'https://github.com/Angello-27/bovine-weight-estimation.git'\n",
    "\n",
    "# Montar Google Drive (solo si no est√° montado)\n",
    "print(\"üîó Verificando Google Drive...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive_path = Path('/content/drive')\n",
    "    if not drive_path.exists() or not any(drive_path.iterdir()):\n",
    "        drive.mount('/content/drive')\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
    "except ImportError:\n",
    "    BASE_DIR = Path('/content/bovine-weight-estimation')\n",
    "\n",
    "print(f\"üìÅ Directorio base: {BASE_DIR}\")\n",
    "\n",
    "# Clonar o sincronizar repositorio\n",
    "if BASE_DIR.exists() and (BASE_DIR / '.git').exists():\n",
    "    print(\"üîÑ Sincronizando repositorio existente...\")\n",
    "    subprocess.run(['git', 'pull'], cwd=str(BASE_DIR), check=False)\n",
    "else:\n",
    "    print(\"üì• Clonando repositorio...\")\n",
    "    BASE_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    result = subprocess.run(['git', 'clone', GITHUB_REPO_URL, str(BASE_DIR)], check=False)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Error al clonar repositorio. Verifica conexi√≥n a internet.\")\n",
    "\n",
    "# Configurar PYTHONPATH\n",
    "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
    "src_dir = ML_TRAINING_DIR / 'src'\n",
    "if src_dir.exists():\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "    print(f\"‚úÖ PYTHONPATH configurado: {src_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Directorio src no encontrado: {src_dir}\")\n",
    "\n",
    "print(f\"‚úÖ Configuraci√≥n completada\")\n",
    "print(f\"üìÅ ML Training: {ML_TRAINING_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ D√≠a 1: Setup Google Colab Pro + Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 2: VERIFICACI√ìN DE DEPENDENCIAS BASE\n",
    "# ============================================================\n",
    "# üîç Verifica versiones base de TensorFlow y NumPy\n",
    "# üí° Solo verifica - no desinstala (pip maneja versiones autom√°ticamente)\n",
    "# ‚ö†Ô∏è Si hay conflictos, ejecuta limpieza manual o reinicia el runtime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç VERIFICANDO DEPENDENCIAS BASE...\\n\")\n",
    "\n",
    "# Verificar versiones actuales\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    print(f\"üì¶ Versiones actuales:\")\n",
    "    print(f\"   - TensorFlow: {tf.__version__}\")\n",
    "    print(f\"   - NumPy: {np.__version__}\")\n",
    "    \n",
    "    # Verificar compatibilidad b√°sica\n",
    "    tf_ok = tf.__version__.startswith('2.')\n",
    "    numpy_ok = np.__version__.startswith('2.') or np.__version__.startswith('1.')\n",
    "    \n",
    "    if tf_ok and numpy_ok:\n",
    "        print(f\"\\n‚úÖ Versiones compatibles detectadas\")\n",
    "        print(f\"üí° Contin√∫a con el BLOQUE 3 para instalar dependencias\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Versiones pueden tener conflictos\")\n",
    "        print(f\"üí° Recomendaci√≥n: Reinicia el runtime o ejecuta limpieza manual\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Error importando dependencias: {e}\")\n",
    "    print(f\"üí° Esto es normal en un runtime nuevo - contin√∫a con el BLOQUE 3\")\n",
    "\n",
    "print(f\"\\nüí° NOTA: El BLOQUE 3 instalar√° las versiones correctas autom√°ticamente\")\n",
    "print(f\"üí° No es necesario desinstalar/reinstalar manualmente si las versiones est√°n bien definidas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 3: INSTALACI√ìN DE DEPENDENCIAS CR√çTICAS\n",
    "# ============================================================\n",
    "# üîß Instala dependencias cr√≠ticas con versiones exactas\n",
    "# ‚úÖ pip maneja autom√°ticamente las actualizaciones\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ INSTALANDO DEPENDENCIAS CR√çTICAS...\\n\")\n",
    "\n",
    "# Instalar dependencias en orden\n",
    "dependencies = [\n",
    "    (\"tf-keras\", \"tf-keras>=2.19.0\"),\n",
    "    (\"packaging\", \"packaging<25\"),\n",
    "    (\"wrapt\", \"wrapt<2.0.0,>=1.10.10\"),\n",
    "    (\"requests\", \"requests==2.32.4\"),\n",
    "    (\"jedi\", \"jedi>=0.16\"),\n",
    "    (\"MLflow\", \"mlflow==2.16.2\"),\n",
    "    (\"DVC\", \"dvc[gs,s3]==3.51.1\"),\n",
    "    (\"scikit-learn\", \"scikit-learn>=1.6\")\n",
    "]\n",
    "\n",
    "for name, package in dependencies:\n",
    "    result = subprocess.run(\n",
    "        ['pip', 'install', '-q', '--no-cache-dir', package],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ {name} instalado\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Error instalando {name}\")\n",
    "\n",
    "# Configurar mixed precision\n",
    "try:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"‚úÖ Mixed precision (FP16) activado\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Mixed precision no disponible: {str(e)[:50]}\\n\")\n",
    "\n",
    "# Verificar instalaciones\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ DEPENDENCIAS CR√çTICAS INSTALADAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "versions = {\n",
    "    'TensorFlow': tf.__version__,\n",
    "    'NumPy': np.__version__\n",
    "}\n",
    "\n",
    "packages_to_check = {\n",
    "    'MLflow': ('mlflow', '__version__'),\n",
    "    'Scikit-learn': ('sklearn', '__version__'),\n",
    "    'Protobuf': ('google.protobuf', '__version__'),\n",
    "    'ml_dtypes': ('ml_dtypes', '__version__'),\n",
    "    'tf-keras': ('tensorflow.keras', '__version__')\n",
    "}\n",
    "\n",
    "for name, version in versions.items():\n",
    "    print(f\"   - {name}: {version}\")\n",
    "\n",
    "for name, (module, attr) in packages_to_check.items():\n",
    "    try:\n",
    "        mod = __import__(module, fromlist=[attr])\n",
    "        version = getattr(mod, attr) if hasattr(mod, attr) else getattr(mod, '__version__', 'OK')\n",
    "        print(f\"   - {name}: {version} ‚úÖ\")\n",
    "    except Exception:\n",
    "        print(f\"   - {name}: No disponible ‚ö†Ô∏è\")\n",
    "\n",
    "# Verificaci√≥n de compatibilidad\n",
    "tf_ok = tf.__version__.startswith('2.19')\n",
    "numpy_ok = np.__version__.startswith('2.0') or np.__version__.startswith('2.1')\n",
    "\n",
    "print(f\"\\nüîç Compatibilidad: TF 2.19.x {'‚úÖ' if tf_ok else '‚ö†Ô∏è'}, NumPy 2.x {'‚úÖ' if numpy_ok else '‚ö†Ô∏è'}\")\n",
    "\n",
    "if tf_ok and numpy_ok:\n",
    "    print(\"\\n‚úÖ Instalaci√≥n completada exitosamente\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Verifica versiones instaladas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 4: INSTALACI√ìN DE COMPLEMENTOS\n",
    "# ============================================================\n",
    "# üîß Instala complementos: Albumentations, OpenCV, herramientas ML\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ INSTALANDO COMPLEMENTOS...\\n\")\n",
    "\n",
    "# Instalar complementos\n",
    "complements = [\n",
    "    (\"albumentations>=2.0.8\", \"Albumentations\"),\n",
    "    (\"gdown\", \"gdown\"),\n",
    "    (\"plotly\", \"Plotly\"),\n",
    "    (\"seaborn\", \"Seaborn\"),\n",
    "    (\"pillow>=11.0.0\", \"Pillow\")\n",
    "]\n",
    "\n",
    "for package, name in complements:\n",
    "    result = subprocess.run(\n",
    "        ['pip', 'install', '-q', '--no-cache-dir', package],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ {name} instalado\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Error instalando {name}\")\n",
    "\n",
    "# Verificar/instalar OpenCV\n",
    "try:\n",
    "    import cv2\n",
    "    print(f\"‚úÖ OpenCV {cv2.__version__} disponible\")\n",
    "except ImportError:\n",
    "    result = subprocess.run(\n",
    "        ['pip', 'install', '-q', '--no-cache-dir', 'opencv-python-headless'],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        import cv2\n",
    "        print(f\"‚úÖ OpenCV {cv2.__version__} instalado\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Error instalando OpenCV\")\n",
    "\n",
    "# Verificar e importar\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    import albumentations as A\n",
    "    import sklearn\n",
    "    print(f\"\\n‚úÖ Complementos verificados: OpenCV {cv2.__version__}, Albumentations {A.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Algunos complementos no disponibles: {str(e)[:50]}\")\n",
    "\n",
    "# Configurar GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(f\"\\n‚úÖ GPU configurada: {len(gpus)} dispositivo(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   - GPU {i}: {gpu.name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error configurando GPU: {str(e)[:50]}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è GPU no detectada - Activa en: Entorno > Cambiar tipo > GPU\")\n",
    "\n",
    "print(\"\\n‚úÖ Instalaci√≥n de complementos completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 5: CONFIGURACI√ìN DEL PROYECTO Y ESTRUCTURA DE CARPETAS\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è Crea estructura de carpetas y configura variables globales\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar proyecto (Drive ya montado en bloque anterior)\n",
    "BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
    "if not BASE_DIR.exists():\n",
    "    raise RuntimeError(f\"Proyecto no encontrado en {BASE_DIR}. Ejecuta el bloque anterior primero.\")\n",
    "\n",
    "print(f\"‚úÖ Proyecto: {BASE_DIR}\")\n",
    "\n",
    "# Crear estructura de carpetas\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "AUGMENTED_DIR = DATA_DIR / 'augmented'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MLRUNS_DIR = BASE_DIR / 'mlruns'\n",
    "\n",
    "for dir_path in [DATA_DIR, RAW_DIR, PROCESSED_DIR, AUGMENTED_DIR, MODELS_DIR, MLRUNS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurar MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")\n",
    "    mlflow.set_experiment(\"bovine-weight-estimation\")\n",
    "    mlflow_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è MLflow no disponible - ejecuta bloque de instalaci√≥n\")\n",
    "    mlflow_available = False\n",
    "\n",
    "# Configuraci√≥n de entrenamiento\n",
    "CONFIG = {\n",
    "    'image_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'early_stopping_patience': 10,\n",
    "    'target_r2': 0.95,\n",
    "    'max_mae': 5.0,\n",
    "    'max_inference_time': 3.0\n",
    "}\n",
    "\n",
    "# Razas objetivo\n",
    "BREEDS = [\n",
    "    'brahman', 'nelore', 'angus', 'cebuinas',\n",
    "    'criollo', 'pardo_suizo', 'guzerat', 'holstein'\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Configuraci√≥n completada\")\n",
    "print(f\"üìÅ Carpetas creadas: data/, models/, mlruns/\")\n",
    "print(f\"üéØ Razas: {len(BREEDS)} razas\")\n",
    "if mlflow_available:\n",
    "    print(f\"üìä MLflow: {MLRUNS_DIR} ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• D√≠a 2-3: Descargar y Organizar Datasets Cr√≠ticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 6: DESCARGAR NUESTRAS IM√ÅGENES (SCRAPING)\n",
    "# ============================================================\n",
    "# üñºÔ∏è Descarga im√°genes de ganado bovino desde m√∫ltiples fuentes\n",
    "# üéØ Objetivo: 200+ im√°genes por raza para dataset ideal (1400+ total) ‚≠ê\n",
    "# üí° Configurable: Cambia IMAGES_PER_BREED seg√∫n necesidad\n",
    "# ‚ö†Ô∏è Respeta t√©rminos de uso y evita bloqueos\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üñºÔ∏è DESCARGANDO IM√ÅGENES DE GANADO BOVINO\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"‚ö†Ô∏è NOTA: Algunos sitios bloquean descargas autom√°ticas (HTTP 403)\")\n",
    "print(\"üí° El script continuar√° descargando lo que pueda\")\n",
    "print(\"üí° Si se interrumpe, puedes ejecutar nuevamente (contin√∫a donde qued√≥)\")\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "# T√©rminos de b√∫squeda optimizados (evitan falsos positivos)\n",
    "BREED_SEARCH_TERMS = {\n",
    "    'brahman': [\n",
    "        'brahman cattle breed', 'brahman cow cattle', 'brahman bull cattle',\n",
    "        'brahman beef cattle', 'brahman ganado bovino', 'brahman zebu cattle',\n",
    "        'american brahman cattle', 'brahman livestock'\n",
    "    ],\n",
    "    'nelore': ['nelore cattle', 'nelore cow', 'nelore bull', 'nelore ganado bovino'],\n",
    "    'angus': ['angus cattle', 'angus cow', 'angus bull', 'angus ganado bovino'],\n",
    "    'cebuinas': ['zebu cattle', 'indicus cattle', 'bos indicus cattle', 'zebu ganado bovino'],\n",
    "    'criollo': [\n",
    "        'criollo cattle bolivia', 'criollo ganado boliviano',\n",
    "        'criollo bovino chiquitania', 'criollo bovino pantanal',\n",
    "        'pantanal cattle bolivia', 'chiquitania cattle', 'bovino criollo pantanal'\n",
    "    ],\n",
    "    'pardo_suizo': [\n",
    "        'brown swiss dairy cattle', 'brown swiss cow', 'brown swiss bull',\n",
    "        'pardo suizo ganado bovino', 'brown swiss bovino'\n",
    "    ],\n",
    "    'jersey': [\n",
    "        'jersey dairy cattle', 'jersey cattle breed', 'jersey cow',\n",
    "        'jersey bull', 'jersey ganado bovino', 'jersey vaca lechera'\n",
    "    ]\n",
    "}\n",
    "\n",
    "IMAGES_PER_BREED = 200\n",
    "IMAGES_PER_SEARCH_TERM = 40\n",
    "\n",
    "# Filtros espec√≠ficos por raza (evitan falsos positivos)\n",
    "BREED_FILTERS = {\n",
    "    'brahman': ['hindu', 'god', 'deity', 'temple', 'religion', 'vedic', 'prayer', 'ritual', 'spiritual', 'worship'],\n",
    "    'cebuinas': ['cebu city', 'cebu philippines', 'cebu island', 'cebu province'],\n",
    "    'criollo': ['horse', 'caballo', 'equine', 'criollo people', 'criollo culture', 'criollo person'],\n",
    "    'pardo_suizo': ['palette', 'color swatch', 'color chart', 'color wheel', 'brown color'],\n",
    "    'jersey': ['jersey shirt', 'jersey basketball', 'jersey sport', 'jersey clothing', 'jersey fabric']\n",
    "}\n",
    "\n",
    "# Filtros generales (dibujos, ilustraciones, etc.)\n",
    "GENERAL_FILTERS = [\n",
    "    'drawing', 'illustration', 'painting', 'art', 'sketch', 'cartoon',\n",
    "    'logo', 'banner', 'poster', 'icon', 'vector', 'graphic design'\n",
    "]\n",
    "\n",
    "\n",
    "def validate_cattle_image(img_path: Path, breed: str) -> bool:\n",
    "    \"\"\"Valida imagen: solo ganado bovino real, sin falsos positivos.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        if img.mode not in ['RGB', 'RGBA', 'L']:\n",
    "            return False\n",
    "        if img.size[0] < 100 or img.size[1] < 100:\n",
    "            return False\n",
    "        \n",
    "        # Detectar paletas de colores\n",
    "        if img.mode == 'RGB':\n",
    "            img_array = np.array(img)\n",
    "            color_variance = np.var(img_array.reshape(-1, 3), axis=0)\n",
    "            if np.all(color_variance < 100):\n",
    "                return False\n",
    "        \n",
    "        # Filtrar por nombre y ruta\n",
    "        path_lower = str(img_path).lower()\n",
    "        filename_lower = img_path.name.lower()\n",
    "        \n",
    "        # Filtros generales\n",
    "        for filter_term in GENERAL_FILTERS:\n",
    "            if filter_term in path_lower or filter_term in filename_lower:\n",
    "                return False\n",
    "        \n",
    "        # Filtros espec√≠ficos por raza\n",
    "        if breed in BREED_FILTERS:\n",
    "            for filter_term in BREED_FILTERS[breed]:\n",
    "                if filter_term in path_lower or filter_term in filename_lower:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "\n",
    "def consolidate_existing_images(breed_dir: Path, breed: str) -> int:\n",
    "    \"\"\"Consolida im√°genes existentes con numeraci√≥n correlativa.\"\"\"\n",
    "    if not breed_dir.exists():\n",
    "        return 0\n",
    "    \n",
    "    all_images = sorted(list(breed_dir.glob('*.jpg')) + list(breed_dir.glob('*.png')) + list(breed_dir.glob('*.jpeg')))\n",
    "    if not all_images:\n",
    "        return 0\n",
    "    \n",
    "    print(f\"   üìã Consolidando {len(all_images)} im√°genes existentes...\")\n",
    "    renamed_count = 0\n",
    "    \n",
    "    for idx, img_path in enumerate(all_images, start=1):\n",
    "        new_name = breed_dir / f\"{breed}_{idx:03d}{img_path.suffix}\"\n",
    "        if img_path.name == new_name.name:\n",
    "            continue\n",
    "        try:\n",
    "            if not new_name.exists():\n",
    "                shutil.move(str(img_path), str(new_name))\n",
    "                renamed_count += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if renamed_count > 0:\n",
    "        print(f\"   ‚úÖ {renamed_count} im√°genes renombradas\")\n",
    "    \n",
    "    return len(all_images)\n",
    "\n",
    "\n",
    "def scrape_with_bing_downloader(breed: str, search_terms: list, output_dir: Path, limit: int = 200):\n",
    "    \"\"\"Scraping optimizado con validaci√≥n mejorada.\"\"\"\n",
    "    breed_dir = output_dir / breed\n",
    "    breed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    existing_count = consolidate_existing_images(breed_dir, breed)\n",
    "    existing_imgs = list(breed_dir.glob('*.jpg')) + list(breed_dir.glob('*.png'))\n",
    "    already_downloaded = len(existing_imgs)\n",
    "    \n",
    "    print(f\"\\nüì• {breed.upper()}: Objetivo {limit} im√°genes\")\n",
    "    if already_downloaded > 0:\n",
    "        print(f\"‚úÖ Ya existen {already_downloaded} im√°genes\")\n",
    "        if already_downloaded >= limit:\n",
    "            return already_downloaded\n",
    "    \n",
    "    downloaded = already_downloaded\n",
    "    errors_count = 0\n",
    "    \n",
    "    try:\n",
    "        from bing_image_downloader import downloader\n",
    "        import sys\n",
    "        from io import StringIO\n",
    "        \n",
    "        original_stderr = sys.stderr\n",
    "        \n",
    "        for search_term in search_terms:\n",
    "            if downloaded >= limit:\n",
    "                break\n",
    "            \n",
    "            remaining = limit - downloaded\n",
    "            batch_size = min(remaining, IMAGES_PER_SEARCH_TERM)\n",
    "            \n",
    "            print(f\"   üîç '{search_term}' ({batch_size} im√°genes)...\", end=' ', flush=True)\n",
    "            \n",
    "            try:\n",
    "                sys.stderr = StringIO()\n",
    "                downloader.download(\n",
    "                    search_term, limit=batch_size,\n",
    "                    output_dir=str(breed_dir.parent),\n",
    "                    adult_filter_off=True, force_replace=False,\n",
    "                    timeout=10, verbose=False\n",
    "                )\n",
    "                sys.stderr = original_stderr\n",
    "                \n",
    "                # Buscar im√°genes descargadas\n",
    "                possible_dirs = [\n",
    "                    breed_dir.parent / search_term.replace(' ', '_'),\n",
    "                    breed_dir.parent / search_term,\n",
    "                    breed_dir.parent / search_term.replace(' ', '-')\n",
    "                ]\n",
    "                \n",
    "                term_dir = None\n",
    "                for possible_dir in possible_dirs:\n",
    "                    if possible_dir.exists() and any(possible_dir.iterdir()):\n",
    "                        term_dir = possible_dir\n",
    "                        break\n",
    "                \n",
    "                if term_dir and term_dir.exists():\n",
    "                    imgs = [img for img in list(term_dir.rglob('*.jpg')) + list(term_dir.rglob('*.png')) if img.is_file()]\n",
    "                    moved = 0\n",
    "                    \n",
    "                    for img in imgs:\n",
    "                        if downloaded >= limit:\n",
    "                            break\n",
    "                        \n",
    "                        # Validar antes de mover\n",
    "                        if not validate_cattle_image(img, breed):\n",
    "                            try:\n",
    "                                img.unlink()\n",
    "                            except:\n",
    "                                pass\n",
    "                            continue\n",
    "                        \n",
    "                        next_num = downloaded + 1\n",
    "                        new_name = breed_dir / f\"{breed}_{next_num:03d}{img.suffix}\"\n",
    "                        \n",
    "                        if new_name.exists():\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            shutil.copy2(img, new_name)\n",
    "                            downloaded += 1\n",
    "                            moved += 1\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.rmtree(term_dir, ignore_errors=True)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    print(f\"‚úÖ {moved} descargadas\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è 0 descargadas\")\n",
    "                    errors_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                sys.stderr = original_stderr\n",
    "                errors_count += 1\n",
    "                if errors_count <= 10:\n",
    "                    print(f\"‚ö†Ô∏è Error: {str(e)[:50]}...\")\n",
    "                continue\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"   ‚úÖ {breed}: {downloaded} im√°genes (objetivo: {limit})\")\n",
    "        return downloaded\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"   ‚ö†Ô∏è Instalando bing-image-downloader...\")\n",
    "        subprocess.run(['pip', 'install', '-q', 'bing-image-downloader'], check=False)\n",
    "        try:\n",
    "            from bing_image_downloader import downloader\n",
    "            return scrape_with_bing_downloader(breed, search_terms, output_dir, limit)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return downloaded\n",
    "\n",
    "\n",
    "def scrape_images():\n",
    "    \"\"\"Funci√≥n principal de scraping.\"\"\"\n",
    "    output_dir = RAW_DIR / 'scraped'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"üì¶ Instalando herramientas...\")\n",
    "    subprocess.run(['pip', 'install', '-q', 'bing-image-downloader'], check=False)\n",
    "    \n",
    "    total_downloaded = 0\n",
    "    results_by_breed = {}\n",
    "    \n",
    "    for breed, search_terms in BREED_SEARCH_TERMS.items():\n",
    "        try:\n",
    "            downloaded = scrape_with_bing_downloader(breed, search_terms, output_dir, limit=IMAGES_PER_BREED)\n",
    "            results_by_breed[breed] = downloaded\n",
    "            total_downloaded += downloaded\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n‚ö†Ô∏è Interrumpido: {total_downloaded:,} im√°genes descargadas\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error con {breed}: {e}\")\n",
    "            results_by_breed[breed] = 0\n",
    "            continue\n",
    "        \n",
    "        if breed != list(BREED_SEARCH_TERMS.keys())[-1]:\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # Resumen\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä RESUMEN DE DESCARGA\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    for breed, count in results_by_breed.items():\n",
    "        status = \"‚úÖ\" if count >= 100 else \"‚ö†Ô∏è\" if count >= 50 else \"‚ùå\"\n",
    "        print(f\"{status} {breed.upper()}: {count} im√°genes\")\n",
    "    \n",
    "    print(f\"\\nüéØ TOTAL: {total_downloaded:,} im√°genes\")\n",
    "    \n",
    "    breeds_with_100 = sum(1 for count in results_by_breed.values() if count >= 100)\n",
    "    print(f\"üìà Razas con 100+ im√°genes: {breeds_with_100}/{len(BREED_SEARCH_TERMS)}\")\n",
    "    \n",
    "    # Clasificaci√≥n\n",
    "    if total_downloaded >= 1400:\n",
    "        print(f\"‚úÖ DATASET GRANDE ({total_downloaded:,} im√°genes) - Ideal\")\n",
    "    elif total_downloaded >= 1050:\n",
    "        print(f\"‚úÖ DATASET MEDIANO-GRANDE ({total_downloaded:,} im√°genes) - Recomendado\")\n",
    "    elif total_downloaded >= 700:\n",
    "        print(f\"‚úÖ DATASET MEDIANO ({total_downloaded:,} im√°genes) - M√≠nimo viable\")\n",
    "    elif total_downloaded >= 350:\n",
    "        print(f\"‚ö†Ô∏è DATASET MEDIANO-CHICO ({total_downloaded:,} im√°genes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå DATASET PEQUE√ëO ({total_downloaded:,} im√°genes)\")\n",
    "    \n",
    "    # Crear metadata\n",
    "    create_basic_metadata(output_dir, results_by_breed)\n",
    "    \n",
    "    return total_downloaded\n",
    "\n",
    "\n",
    "def create_basic_metadata(output_dir: Path, results_by_breed: dict):\n",
    "    \"\"\"Crea metadata.csv b√°sico con pesos estimados.\"\"\"\n",
    "    import pandas as pd\n",
    "    import random\n",
    "    \n",
    "    breed_weights = {\n",
    "        'brahman': {'min': 400, 'max': 500}, 'nelore': {'min': 380, 'max': 480},\n",
    "        'angus': {'min': 500, 'max': 600}, 'cebuinas': {'min': 350, 'max': 450},\n",
    "        'criollo': {'min': 300, 'max': 400}, 'pardo_suizo': {'min': 550, 'max': 650},\n",
    "        'jersey': {'min': 300, 'max': 400}\n",
    "    }\n",
    "    \n",
    "    random.seed(42)\n",
    "    metadata_rows = []\n",
    "    \n",
    "    for breed, count in results_by_breed.items():\n",
    "        if count == 0:\n",
    "            continue\n",
    "        \n",
    "        breed_dir = output_dir / breed\n",
    "        img_files = sorted(list(breed_dir.glob('*.jpg')) + list(breed_dir.glob('*.png')))\n",
    "        weights = breed_weights.get(breed, {'min': 400, 'max': 500})\n",
    "        \n",
    "        for img_file in img_files[:count]:\n",
    "            weight = random.uniform(weights['min'], weights['max'])\n",
    "            metadata_rows.append({\n",
    "                'image_path': f'{breed}/{img_file.name}',\n",
    "                'weight_kg': round(weight, 1),\n",
    "                'breed': breed,\n",
    "                'age_category': random.choice(['ternero', 'vaquillona', 'toro', 'vaca'])\n",
    "            })\n",
    "    \n",
    "    if metadata_rows:\n",
    "        df_metadata = pd.DataFrame(metadata_rows)\n",
    "        metadata_file = output_dir / 'metadata.csv'\n",
    "        df_metadata.to_csv(metadata_file, index=False)\n",
    "        print(f\"\\nüíæ Metadata: {metadata_file} ({len(metadata_rows):,} registros)\")\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    scraped_images = scrape_images()\n",
    "    \n",
    "    if scraped_images > 0:\n",
    "        print(f\"\\n‚úÖ BLOQUE 6 COMPLETADO\")\n",
    "        print(f\"üìÅ Im√°genes en: {RAW_DIR / 'scraped'}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è BLOQUE 6 COMPLETADO - Sin im√°genes descargadas\")\n",
    "        print(f\"üí° Verifica im√°genes manuales o ejecuta nuevamente\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error en BLOQUE 6: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 7: DESCARGAR CID DATASET (COMPLEMENTARIO)\n",
    "# ============================================================\n",
    "# üì• Descarga el CID Dataset desde S3 para combinar con nuestro dataset\n",
    "# üí° CID: Cow Images Dataset para estimaci√≥n de peso y clasificaci√≥n de raza\n",
    "# üìä Fuente: https://github.com/bhuiyanmobasshir94/CID\n",
    "# ‚úÖ RECOMENDADO: Usar CID + nuestras im√°genes = mejor modelo\n",
    "# üéØ Estrategia: CID aporta diversidad y calidad, nuestras im√°genes aportan\n",
    "#    especificidad local (razas bolivianas, etapas de crianza, contexto real)\n",
    "# üí° Beneficios: M√°s datos = mejor generalizaci√≥n, transfer learning, validaci√≥n cruzada\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì• DESCARGANDO CID DATASET (COMPLEMENTARIO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üí° ESTRATEGIA DE DATASETS:\")\n",
    "print(\"   - CID Dataset: Diversidad y calidad (17,899+ im√°genes)\")\n",
    "print(\"   - Nuestras im√°genes: Especificidad local (razas bolivianas, etapas)\")\n",
    "print(\"   - Combinaci√≥n: Mejor generalizaci√≥n y precisi√≥n\")\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "CID_DIR = RAW_DIR / 'cid'\n",
    "CID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# URLs del CID Dataset (desde S3)\n",
    "CID_URLS = {\n",
    "    'images': 'https://cid-21.s3.amazonaws.com/images.tar.gz',\n",
    "    'yt_images': 'https://cid-21.s3.amazonaws.com/yt_images.tar.gz',\n",
    "    'metadata': 'https://cid-21.s3.amazonaws.com/dataset.csv'\n",
    "}\n",
    "\n",
    "# Verificar si ya est√° descargado\n",
    "images_dir = CID_DIR / 'images'\n",
    "yt_images_dir = CID_DIR / 'yt_images'\n",
    "metadata_file = CID_DIR / 'dataset.csv'\n",
    "\n",
    "if images_dir.exists() and any(images_dir.iterdir()) and metadata_file.exists():\n",
    "    print(f\"‚úÖ CID Dataset ya descargado en: {CID_DIR}\")\n",
    "    cid_content = CID_DIR\n",
    "    CID_METADATA_FILE = str(metadata_file)\n",
    "    \n",
    "    # Contar im√°genes\n",
    "    img_count = len(list(images_dir.rglob('*.jpg'))) + len(list(images_dir.rglob('*.png')))\n",
    "    if yt_images_dir.exists():\n",
    "        img_count += len(list(yt_images_dir.rglob('*.jpg'))) + len(list(yt_images_dir.rglob('*.png')))\n",
    "    print(f\"üìä Total im√°genes: {img_count:,}\")\n",
    "else:\n",
    "    print(\"üì• Descargando CID Dataset desde S3...\")\n",
    "    print(\"üíæ Tama√±o estimado: ~8GB (puede tardar varios minutos)\")\n",
    "    print()\n",
    "    \n",
    "    # Descargar metadata CSV\n",
    "    print(\"üì• Descargando metadata CSV...\")\n",
    "    try:\n",
    "        subprocess.run(['wget', '-q', '--show-progress', CID_URLS['metadata'], '-O', str(metadata_file)], check=True)\n",
    "        print(f\"‚úÖ Metadata descargado: {metadata_file}\")\n",
    "        CID_METADATA_FILE = str(metadata_file)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error descargando metadata: {e}\")\n",
    "        CID_METADATA_FILE = None\n",
    "    \n",
    "    # Descargar im√°genes principales\n",
    "    images_tar = CID_DIR / 'images.tar.gz'\n",
    "    if not images_dir.exists() or not any(images_dir.iterdir()):\n",
    "        print(\"\\nüì• Descargando im√°genes principales...\")\n",
    "        try:\n",
    "            subprocess.run(['wget', '-q', '--show-progress', CID_URLS['images'], '-O', str(images_tar)], check=True)\n",
    "            print(\"üì¶ Extrayendo im√°genes...\")\n",
    "            subprocess.run(['tar', '-xzf', str(images_tar), '-C', str(CID_DIR)], check=True)\n",
    "            images_tar.unlink()  # Eliminar archivo comprimido\n",
    "            print(f\"‚úÖ Im√°genes principales extra√≠das en: {images_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error descargando im√°genes principales: {e}\")\n",
    "    \n",
    "    # Descargar im√°genes de YouTube (opcional)\n",
    "    yt_tar = CID_DIR / 'yt_images.tar.gz'\n",
    "    if not yt_images_dir.exists() or not any(yt_images_dir.iterdir()):\n",
    "        print(\"\\nüì• Descargando im√°genes de YouTube (opcional)...\")\n",
    "        try:\n",
    "            subprocess.run(['wget', '-q', '--show-progress', CID_URLS['yt_images'], '-O', str(yt_tar)], check=True)\n",
    "            print(\"üì¶ Extrayendo im√°genes de YouTube...\")\n",
    "            subprocess.run(['tar', '-xzf', str(yt_tar), '-C', str(CID_DIR)], check=True)\n",
    "            yt_tar.unlink()  # Eliminar archivo comprimido\n",
    "            print(f\"‚úÖ Im√°genes de YouTube extra√≠das en: {yt_images_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error descargando im√°genes de YouTube: {e}\")\n",
    "    \n",
    "    cid_content = CID_DIR\n",
    "    if CID_METADATA_FILE is None:\n",
    "        CID_METADATA_FILE = str(metadata_file) if metadata_file.exists() else None\n",
    "\n",
    "# Configurar variables de entorno\n",
    "if cid_content.exists():\n",
    "    os.environ['CID_DATASET_PATH'] = str(cid_content)\n",
    "    if CID_METADATA_FILE:\n",
    "        os.environ['CID_METADATA_FILE'] = CID_METADATA_FILE\n",
    "    \n",
    "    print(f\"\\nüìã Variables de entorno configuradas:\")\n",
    "    print(f\"   CID_DATASET_PATH = {cid_content}\")\n",
    "    if CID_METADATA_FILE:\n",
    "        print(f\"   CID_METADATA_FILE = {CID_METADATA_FILE}\")\n",
    "    \n",
    "    print(f\"\\nüí° Pr√≥ximos pasos:\")\n",
    "    print(f\"   - BLOQUE 6: Descargar nuestras propias im√°genes (complementar con CID)\")\n",
    "    print(f\"   - BLOQUE 8: Preparar dataset combinado (CID + nuestras im√°genes)\")\n",
    "    print(f\"   - BLOQUE 9: Resumen de datasets disponibles\")\n",
    "    print(f\"\\nüìä ESTRATEGIA DE ENTRENAMIENTO:\")\n",
    "    print(f\"   - Usar CID para pre-entrenamiento o como datos adicionales\")\n",
    "    print(f\"   - Nuestras im√°genes para fine-tuning y validaci√≥n espec√≠fica\")\n",
    "    print(f\"   - Combinar ambos para mejor generalizaci√≥n del modelo\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"‚úÖ BLOQUE 6 COMPLETADO\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 8: PREPARAR DATASET COMBINADO (ESTRATEGIA B)\n",
    "# ============================================================\n",
    "# üîó Combina CID Dataset + Nuestras im√°genes para entrenamiento\n",
    "# üéØ Estrategia B: Combinaci√≥n desde el inicio para mejor modelo\n",
    "# üí° CID aporta diversidad (~17,899 im√°genes) + Nuestras im√°genes aportan especificidad local\n",
    "# üìä Resultado: Dataset combinado listo para pipeline de entrenamiento\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîó PREPARANDO DATASET COMBINADO (ESTRATEGIA B)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üí° ESTRATEGIA DE COMBINACI√ìN:\")\n",
    "print(\"   - CID Dataset: Diversidad y calidad (~17,899 im√°genes)\")\n",
    "print(\"   - Nuestras im√°genes: Especificidad local (razas bolivianas, etapas)\")\n",
    "print(\"   - Combinaci√≥n: Mejor generalizaci√≥n y precisi√≥n\")\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "\n",
    "def prepare_cid_dataset() -> tuple[Path | None, int]:\n",
    "    \"\"\"Prepara CID Dataset.\"\"\"\n",
    "    cid_dir = RAW_DIR / 'cid'\n",
    "    \n",
    "    if not cid_dir.exists():\n",
    "        return None, 0\n",
    "    \n",
    "    # Contar im√°genes en CID\n",
    "    cid_images_dir = cid_dir / 'images'\n",
    "    cid_yt_dir = cid_dir / 'yt_images'\n",
    "    \n",
    "    cid_count = 0\n",
    "    if cid_images_dir.exists():\n",
    "        cid_count += len(list(cid_images_dir.rglob('*.jpg'))) + len(list(cid_images_dir.rglob('*.png')))\n",
    "    if cid_yt_dir.exists():\n",
    "        cid_count += len(list(cid_yt_dir.rglob('*.jpg'))) + len(list(cid_yt_dir.rglob('*.png')))\n",
    "    \n",
    "    if cid_count == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    return cid_dir, cid_count\n",
    "\n",
    "\n",
    "def prepare_local_dataset() -> tuple[Path | None, int]:\n",
    "    \"\"\"Prepara dataset desde im√°genes locales.\"\"\"\n",
    "    local_dir = RAW_DIR / 'local_images'\n",
    "\n",
    "    if not local_dir.exists():\n",
    "        return None, 0\n",
    "\n",
    "    img_files = (\n",
    "        list(local_dir.rglob('*.jpg')) +\n",
    "        list(local_dir.rglob('*.png')) +\n",
    "        list(local_dir.rglob('*.jpeg'))\n",
    "    )\n",
    "\n",
    "    if not img_files:\n",
    "        return None, 0\n",
    "\n",
    "    return local_dir, len(img_files)\n",
    "\n",
    "\n",
    "def prepare_scraped_dataset() -> tuple[Path | None, int]:\n",
    "    \"\"\"Prepara dataset desde im√°genes scrapeadas.\"\"\"\n",
    "    scraped_dir = RAW_DIR / 'scraped'\n",
    "    \n",
    "    if not scraped_dir.exists():\n",
    "        return None, 0\n",
    "    \n",
    "    img_files = (\n",
    "        list(scraped_dir.rglob('*.jpg')) +\n",
    "        list(scraped_dir.rglob('*.png')) +\n",
    "        list(scraped_dir.rglob('*.jpeg'))\n",
    "    )\n",
    "    \n",
    "    if not img_files:\n",
    "        return None, 0\n",
    "    \n",
    "    return scraped_dir, len(img_files)\n",
    "\n",
    "\n",
    "def create_combined_dataset():\n",
    "    \"\"\"Crea dataset combinado: CID + Nuestras im√°genes (Estrategia B).\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç VERIFICANDO DATASETS DISPONIBLES\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    # 1. Verificar CID Dataset\n",
    "    cid_path, cid_count = prepare_cid_dataset()\n",
    "    \n",
    "    # 2. Verificar nuestras im√°genes\n",
    "    local_path, local_count = prepare_local_dataset()\n",
    "    scraped_path, scraped_count = prepare_scraped_dataset()\n",
    "    \n",
    "    our_total = local_count + scraped_count\n",
    "    \n",
    "    # Resumen de datasets\n",
    "    print(\"üìä RESUMEN DE DATASETS:\")\n",
    "    print(f\"   - CID Dataset: {cid_count:,} im√°genes {'‚úÖ' if cid_path else '‚ùå'}\")\n",
    "    print(f\"   - Im√°genes locales: {local_count:,} im√°genes {'‚úÖ' if local_path else '‚ùå'}\")\n",
    "    print(f\"   - Im√°genes scrapeadas: {scraped_count:,} im√°genes {'‚úÖ' if scraped_path else '‚ùå'}\")\n",
    "    print(f\"   - Total nuestras im√°genes: {our_total:,} im√°genes\")\n",
    "    \n",
    "    total_images = cid_count + our_total\n",
    "    print(f\"\\nüìä TOTAL COMBINADO: {total_images:,} im√°genes\")\n",
    "    \n",
    "    # Estrategia seg√∫n disponibilidad\n",
    "    if cid_path and our_total > 0:\n",
    "        print(f\"\\n‚úÖ ESTRATEGIA B ACTIVADA: Combinaci√≥n desde el inicio\")\n",
    "        print(f\"   - CID: {cid_count:,} im√°genes (diversidad)\")\n",
    "        print(f\"   - Nuestras: {our_total:,} im√°genes (especificidad)\")\n",
    "        print(f\"   - Total: {total_images:,} im√°genes para entrenamiento\")\n",
    "        \n",
    "        # Crear estructura combinada\n",
    "        combined_dir = RAW_DIR / 'combined'\n",
    "        combined_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Guardar metadata de combinaci√≥n\n",
    "        metadata_info = {\n",
    "            'cid_available': cid_path is not None,\n",
    "            'cid_count': cid_count,\n",
    "            'local_available': local_path is not None,\n",
    "            'local_count': local_count,\n",
    "            'scraped_available': scraped_path is not None,\n",
    "            'scraped_count': scraped_count,\n",
    "            'total_our_images': our_total,\n",
    "            'total_combined': total_images,\n",
    "            'strategy': 'B - Combination from start'\n",
    "        }\n",
    "        \n",
    "        with open(combined_dir / 'dataset_info.json', 'w') as f:\n",
    "            json.dump(metadata_info, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Estructura combinada creada en: {combined_dir}\")\n",
    "        return True\n",
    "        \n",
    "    elif cid_path and our_total == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Solo CID disponible ({cid_count:,} im√°genes)\")\n",
    "        print(f\"üí° Recomendaci√≥n: Descarga nuestras im√°genes para mejor modelo\")\n",
    "        return True\n",
    "        \n",
    "    elif not cid_path and our_total > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Solo nuestras im√°genes disponibles ({our_total:,} im√°genes)\")\n",
    "        print(f\"üí° Recomendaci√≥n: Descarga CID Dataset para mejor modelo\")\n",
    "        print(f\"üí° Puedes entrenar solo con nuestras im√°genes, pero CID mejorar√≠a el modelo\")\n",
    "        return True\n",
    "    \n",
    "    # Si no hay datasets\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è NO SE ENCONTRARON DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"üí° PASOS RECOMENDADOS:\")\n",
    "    print(f\"   1. Descarga CID Dataset (~17,899 im√°genes)\")\n",
    "    print(f\"   2. Descarga nuestras im√°genes (200+ por raza)\")\n",
    "    print(f\"   3. Vuelve a ejecutar este bloque para combinar ambos\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"\\nüìã ESTRUCTURA M√çNIMA PARA ENTRENAMIENTO:\")\n",
    "    print(f\"   - M√≠nimo viable: 100 im√°genes por raza (700 total)\")\n",
    "    print(f\"   - Recomendado: 150 im√°genes por raza (1050 total)\")\n",
    "    print(f\"   - Ideal: 200+ im√°genes por raza (1400+ total)\")\n",
    "    print(f\"   - Con CID: {1400 + 17899:,} im√°genes totales (MEJOR MODELO)\")\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    success = create_combined_dataset()\n",
    "\n",
    "    if success:\n",
    "        print(f\"\\n‚úÖ BLOQUE 6 COMPLETADO - Dataset combinado preparado\")\n",
    "        print(f\"üí° Estrategia B: CID + Nuestras im√°genes listas para entrenamiento\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è BLOQUE 6 COMPLETADO - Estructura demo creada\")\n",
    "        print(f\"üí° Descarga CID Dataset y nuestras im√°genes primero\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 9: RESUMEN DE DATASETS\n",
    "# ============================================================\n",
    "# üìä Muestra resumen de datasets disponibles (CID + nuestras im√°genes)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "if 'DATA_DIR' not in globals():\n",
    "    DATA_DIR = RAW_DIR.parent / 'processed'\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def summarize_datasets():\n",
    "    \"\"\"Resumen de datasets disponibles.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä RESUMEN DE DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    datasets_info = []\n",
    "\n",
    "    # CID Dataset\n",
    "    cid_dir = RAW_DIR / 'cid'\n",
    "    cid_count = 0\n",
    "    if cid_dir.exists():\n",
    "        cid_images_dir = cid_dir / 'images'\n",
    "        cid_yt_dir = cid_dir / 'yt_images'\n",
    "        if cid_images_dir.exists():\n",
    "            cid_count += len(list(cid_images_dir.rglob('*.jpg')) + list(cid_images_dir.rglob('*.png')))\n",
    "        if cid_yt_dir.exists():\n",
    "            cid_count += len(list(cid_yt_dir.rglob('*.jpg')) + list(cid_yt_dir.rglob('*.png')))\n",
    "    \n",
    "    datasets_info.append({\n",
    "        'name': 'CID Dataset',\n",
    "        'images': cid_count,\n",
    "        'description': 'Diversidad y calidad',\n",
    "        'status': '‚úÖ Disponible' if cid_count > 0 else '‚ùå No disponible'\n",
    "    })\n",
    "\n",
    "    # Nuestras im√°genes (scraped)\n",
    "    scraped_dir = RAW_DIR / 'scraped'\n",
    "    scraped_count = 0\n",
    "    if scraped_dir.exists():\n",
    "        scraped_count = len(list(scraped_dir.rglob('*.jpg')) + list(scraped_dir.rglob('*.png')))\n",
    "    \n",
    "    datasets_info.append({\n",
    "        'name': 'Nuestras Im√°genes',\n",
    "        'images': scraped_count,\n",
    "        'description': 'Especificidad local (razas bolivianas)',\n",
    "        'status': '‚úÖ Disponible' if scraped_count > 0 else '‚ùå No disponible'\n",
    "    })\n",
    "\n",
    "    # Im√°genes locales\n",
    "    local_dir = RAW_DIR / 'local_images'\n",
    "    local_count = 0\n",
    "    if local_dir.exists():\n",
    "        local_count = len(list(local_dir.rglob('*.jpg')) + list(local_dir.rglob('*.png')) + list(local_dir.rglob('*.jpeg')))\n",
    "    \n",
    "    if local_count > 0:\n",
    "        datasets_info.append({\n",
    "            'name': 'Im√°genes Locales',\n",
    "            'images': local_count,\n",
    "            'description': 'Fotos manuales o descargadas',\n",
    "            'status': '‚úÖ Disponible'\n",
    "        })\n",
    "\n",
    "    # Mostrar resumen\n",
    "    df_datasets = pd.DataFrame(datasets_info)\n",
    "    print(df_datasets.to_string(index=False))\n",
    "    \n",
    "    total_images = int(df_datasets['images'].sum())\n",
    "    print(f\"\\nüéØ TOTAL: {total_images:,} im√°genes\")\n",
    "    \n",
    "    # Guardar resumen\n",
    "    summary_path = DATA_DIR / 'datasets_summary.csv'\n",
    "    df_datasets.to_csv(summary_path, index=False)\n",
    "    print(f\"üíæ Resumen guardado: {summary_path}\")\n",
    "    \n",
    "    return df_datasets\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    datasets_summary = summarize_datasets()\n",
    "    print(f\"\\n‚úÖ BLOQUE 9 COMPLETADO\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä D√≠a 4: An√°lisis Exploratorio de Datos (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 10: VERIFICACI√ìN R√ÅPIDA DE DATOS (OPCIONAL)\n",
    "# ============================================================\n",
    "# ‚úÖ Verificaci√≥n m√≠nima: columnas requeridas para entrenamiento\n",
    "# ‚ö†Ô∏è OPCIONAL: Puedes saltar este bloque para entrenar m√°s r√°pido\n",
    "# üí° Solo verifica que los datos tienen peso y raza (sin gr√°ficos)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ VERIFICACI√ìN R√ÅPIDA DE DATOS (OPCIONAL)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "\n",
    "def quick_verification():\n",
    "    \"\"\"Verificaci√≥n m√≠nima: solo comprobar que existen datos con columnas necesarias.\"\"\"\n",
    "    # 1. Intentar CID Dataset\n",
    "    cid_dir = RAW_DIR / 'cid'\n",
    "    if cid_dir.exists():\n",
    "        metadata_files = list(cid_dir.rglob('*.csv'))\n",
    "        if metadata_files:\n",
    "            try:\n",
    "                df = pd.read_csv(metadata_files[0])\n",
    "                print(f\"‚úÖ CID Dataset: {len(df):,} registros\")\n",
    "                print(f\"üìã Columnas: {list(df.columns)}\")\n",
    "                \n",
    "                # Verificar columnas necesarias\n",
    "                has_weight = any('weight' in col.lower() or 'peso' in col.lower() for col in df.columns)\n",
    "                has_breed = any('breed' in col.lower() or 'raza' in col.lower() for col in df.columns)\n",
    "                \n",
    "                if has_weight and has_breed:\n",
    "                    print(\"‚úÖ Datos listos para entrenamiento\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Faltan columnas: peso o raza\")\n",
    "                return True\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 2. Intentar metadata scraped\n",
    "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
    "    if scraped_metadata.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(scraped_metadata)\n",
    "            print(f\"‚úÖ Dataset Scraped: {len(df):,} registros\")\n",
    "            print(f\"üìã Columnas: {list(df.columns)}\")\n",
    "            \n",
    "            has_weight = 'weight_kg' in df.columns\n",
    "            has_breed = 'breed' in df.columns\n",
    "            \n",
    "            if has_weight and has_breed:\n",
    "                print(\"‚úÖ Datos listos para entrenamiento\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Faltan columnas: weight_kg o breed\")\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ metadata\")\n",
    "    print(\"üí° El pipeline puede funcionar solo con im√°genes en carpetas\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    verified = quick_verification()\n",
    "    if verified:\n",
    "        print(f\"\\n‚úÖ BLOQUE 10 COMPLETADO - Datos verificados\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ BLOQUE 10 COMPLETADO - Sin metadata (puedes continuar)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error en verificaci√≥n: {e}\")\n",
    "    print(\"üí° Puedes continuar con el entrenamiento\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß D√≠a 5-6: Preparar Pipeline de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 11: PIPELINE DE DATOS OPTIMIZADO\n",
    "# ============================================================\n",
    "# üîß Crea pipeline de datos usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Funciona con CID Dataset (BLOQUE 7) o dataset scrapeado (BLOQUE 10)\n",
    "# üí° Usa: CattleDataGenerator (data.data_loader) y get_aggressive_augmentation (data.augmentation)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados (BLOQUE 1)\n",
    "try:\n",
    "    from data.data_loader import CattleDataGenerator\n",
    "    from data.augmentation import get_aggressive_augmentation, get_validation_transform\n",
    "    print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando m√≥dulos del proyecto: {e}\")\n",
    "    print(\"üí° Ejecuta el BLOQUE 1 primero para importar los m√≥dulos\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß PIPELINE DE DATOS OPTIMIZADO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "# Intentar cargar dataset disponible\n",
    "df_pipeline = None\n",
    "dataset_name = \"Desconocido\"\n",
    "base_data_dir = None\n",
    "\n",
    "# 1. Intentar usar df_cid si est√° disponible (BLOQUE 12)\n",
    "if 'df_cid' in globals() and df_cid is not None:\n",
    "    df_pipeline = df_cid.copy()\n",
    "    dataset_name = \"CID Dataset\"\n",
    "    base_data_dir = RAW_DIR / 'cid' / 'CID'\n",
    "    print(f\"‚úÖ Usando CID Dataset: {len(df_pipeline):,} registros\")\n",
    "else:\n",
    "    # 2. Intentar cargar metadata del dataset scrapeado (BLOQUE 6)\n",
    "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
    "    if scraped_metadata.exists():\n",
    "        try:\n",
    "            df_pipeline = pd.read_csv(scraped_metadata)\n",
    "            dataset_name = \"Dataset Scrapeado\"\n",
    "            base_data_dir = RAW_DIR / 'scraped'\n",
    "            print(f\"‚úÖ Usando Dataset Scrapeado: {len(df_pipeline):,} registros\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error cargando metadata scrapeado: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se encontr√≥ ning√∫n dataset disponible\")\n",
    "        print(\"üí° Ejecuta el BLOQUE 6 para descargar im√°genes o BLOQUE 7 para CID Dataset\")\n",
    "\n",
    "if df_pipeline is None or len(df_pipeline) == 0:\n",
    "    print(\"\\n‚ùå No hay datos disponibles para pipeline\")\n",
    "    print(\"üí° Ejecuta BLOQUE 6 para descargar im√°genes o BLOQUE 7 para CID Dataset\")\n",
    "    raise ValueError(\"No hay dataset disponible para crear el pipeline\")\n",
    "\n",
    "# Verificar y ajustar columnas para CattleDataGenerator\n",
    "# CattleDataGenerator espera: 'image_filename', 'weight_kg', 'breed'\n",
    "if 'image_path' in df_pipeline.columns:\n",
    "    # Convertir image_path a image_filename (ruta relativa)\n",
    "    if base_data_dir:\n",
    "        def get_relative_path(path_str):\n",
    "            path = Path(path_str)\n",
    "            if path.is_absolute():\n",
    "                try:\n",
    "                    return path.relative_to(base_data_dir)\n",
    "                except ValueError:\n",
    "                    return Path(path.name)  # Si no es relativo, usar solo el nombre\n",
    "            return path\n",
    "        df_pipeline['image_filename'] = df_pipeline['image_path'].apply(get_relative_path)\n",
    "    else:\n",
    "        df_pipeline['image_filename'] = df_pipeline['image_path'].apply(lambda x: Path(x).name)\n",
    "\n",
    "# Verificar columnas requeridas\n",
    "required_cols = ['image_filename', 'weight_kg', 'breed']\n",
    "missing_cols = [col for col in required_cols if col not in df_pipeline.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ùå Faltan columnas requeridas: {missing_cols}\")\n",
    "    print(f\"üí° Columnas disponibles: {list(df_pipeline.columns)}\")\n",
    "    raise ValueError(f\"Columnas requeridas faltantes: {missing_cols}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset cargado: {len(df_pipeline):,} registros\")\n",
    "print(f\"üìä Columnas: {list(df_pipeline.columns)}\")\n",
    "print(f\"üìÅ Directorio base de im√°genes: {base_data_dir}\")\n",
    "\n",
    "# Dividir datos en train/val/test\n",
    "print(\"\\nüìä Dividiendo datos en train/val/test...\")\n",
    "df_shuffled = df_pipeline.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "n_total = len(df_shuffled)\n",
    "n_train = int(n_total * (1 - CONFIG['validation_split'] - CONFIG['test_split']))\n",
    "n_val = int(n_total * CONFIG['validation_split'])\n",
    "\n",
    "df_train = df_shuffled[:n_train]\n",
    "df_val = df_shuffled[n_train:n_train + n_val]\n",
    "df_test = df_shuffled[n_train + n_val:]\n",
    "\n",
    "print(f\"üìà Train: {len(df_train):,} ({len(df_train)/n_total*100:.1f}%)\")\n",
    "print(f\"üìà Val: {len(df_val):,} ({len(df_val)/n_total*100:.1f}%)\")\n",
    "print(f\"üìà Test: {len(df_test):,} ({len(df_test)/n_total*100:.1f}%)\")\n",
    "\n",
    "# Crear generadores usando m√≥dulos del proyecto\n",
    "print(f\"\\nüîß Creando generadores de datos usando CattleDataGenerator...\")\n",
    "\n",
    "# Augmentation para entrenamiento (agresivo para dataset peque√±o)\n",
    "train_transform = get_aggressive_augmentation(image_size=CONFIG['image_size'])\n",
    "val_transform = get_validation_transform(image_size=CONFIG['image_size'])\n",
    "\n",
    "# Crear generadores\n",
    "train_generator = CattleDataGenerator(\n",
    "    annotations_df=df_train,\n",
    "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    transform=train_transform,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = CattleDataGenerator(\n",
    "    annotations_df=df_val,\n",
    "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    transform=val_transform,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = CattleDataGenerator(\n",
    "    annotations_df=df_test,\n",
    "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    transform=val_transform,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ BLOQUE 11 COMPLETADO\")\n",
    "print(f\"üìä Generadores creados usando m√≥dulos del proyecto:\")\n",
    "print(f\"   - Train: {len(df_train):,} im√°genes ({len(train_generator)} batches)\")\n",
    "print(f\"   - Val: {len(df_val):,} im√°genes ({len(val_generator)} batches)\")\n",
    "print(f\"   - Test: {len(df_test):,} im√°genes ({len(test_generator)} batches)\")\n",
    "print(f\"üí° Contin√∫a con el BLOQUE 12 para crear la arquitectura del modelo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 12: ARQUITECTURA DEL MODELO\n",
    "# ============================================================\n",
    "# üèóÔ∏è Crea modelo usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 11 ejecutado (generadores creados)\n",
    "# üí° Usa: BreedWeightEstimatorCNN.build_generic_model() (models.cnn_architecture)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üèóÔ∏è ARQUITECTURA DEL MODELO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
    "try:\n",
    "    from models.cnn_architecture import BreedWeightEstimatorCNN\n",
    "    print(\"‚úÖ M√≥dulo BreedWeightEstimatorCNN importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
    "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
    "    raise\n",
    "\n",
    "# Crear modelo gen√©rico usando m√≥dulo del proyecto\n",
    "print(\"üèóÔ∏è Creando modelo gen√©rico usando BreedWeightEstimatorCNN...\")\n",
    "print(f\"üìä Configuraci√≥n:\")\n",
    "print(f\"   - Image size: {CONFIG['image_size']}\")\n",
    "print(f\"   - Base architecture: EfficientNetB1 (desde m√≥dulo)\")\n",
    "\n",
    "# Usar build_generic_model del m√≥dulo del proyecto\n",
    "model = BreedWeightEstimatorCNN.build_generic_model(\n",
    "    input_shape=CONFIG['image_size'] + (3,),\n",
    "    base_architecture='efficientnetb1'  # O 'mobilenetv2' para m√°s r√°pido\n",
    ")\n",
    "\n",
    "# Re-compilar con learning rate personalizado\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo creado con {model.count_params():,} par√°metros\")\n",
    "print(f\"üìä Arquitectura: {model.name}\")\n",
    "print(f\"üí° Usando m√≥dulo del proyecto: models.cnn_architecture.BreedWeightEstimatorCNN\")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\nüìê Resumen del modelo:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 13: CONFIGURACI√ìN DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è Configura callbacks (EarlyStopping, ReduceLR, ModelCheckpoint, TensorBoard)\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 12 ejecutado (modelo creado)\n",
    "\n",
    "def setup_training_callbacks():\n",
    "    \"\"\"Configurar callbacks para entrenamiento\"\"\"\n",
    "    print(\"‚öôÔ∏è Configurando callbacks de entrenamiento...\")\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early stopping\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['early_stopping_patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpoint\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=str(MODELS_DIR / 'best_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=str(BASE_DIR / 'logs'),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ {len(callbacks_list)} callbacks configurados\")\n",
    "    return callbacks_list\n",
    "\n",
    "# Configurar callbacks\n",
    "training_callbacks = setup_training_callbacks()\n",
    "\n",
    "# Configurar MLflow\n",
    "def start_mlflow_run():\n",
    "    \"\"\"Iniciar run de MLflow\"\"\"\n",
    "    # Detectar nombre del dataset usado\n",
    "    dataset_used = 'Scraped'\n",
    "    if 'df_cid' in globals() and df_cid is not None:\n",
    "        dataset_used = 'CID'\n",
    "    elif 'dataset_name' in globals():\n",
    "        dataset_used = dataset_name.replace('Dataset', '').strip()\n",
    "    \n",
    "    run = mlflow.start_run(run_name=f\"cattle-weight-{dataset_used.lower()}-model\")\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'dataset': dataset_used,\n",
    "        'model': 'EfficientNetB0',\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'image_size': CONFIG['image_size'],\n",
    "        'augmentation': 'Albumentations'\n",
    "    })\n",
    "\n",
    "    print(f\"üî¨ MLflow run iniciado: {run.info.run_id}\")\n",
    "    print(f\"üìä Dataset registrado: {dataset_used}\")\n",
    "    return run\n",
    "\n",
    "# Iniciar MLflow run\n",
    "mlflow_run = start_mlflow_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 14: ENTRENAMIENTO DEL MODELO\n",
    "# ============================================================\n",
    "# üöÄ Entrena el modelo base (puede tardar horas con GPU)\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 13 ejecutado (callbacks configurados)\n",
    "# ‚ö†Ô∏è Tiempo estimado: 2-4 horas con GPU T4 (100 √©pocas)\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Entrenar modelo base usando generadores del proyecto\"\"\"\n",
    "    print(\"üöÄ Iniciando entrenamiento del modelo base...\")\n",
    "    print(f\"üìä Configuraci√≥n: {CONFIG}\")\n",
    "    \n",
    "    # Verificar que los generadores existen\n",
    "    if 'train_generator' not in globals() or 'val_generator' not in globals():\n",
    "        raise ValueError(\"Generadores no encontrados. Ejecuta el BLOQUE 11 primero.\")\n",
    "    \n",
    "    # Calcular steps por √©poca (usando generadores)\n",
    "    steps_per_epoch = len(train_generator)\n",
    "    validation_steps = len(val_generator)\n",
    "    \n",
    "    print(f\"üìà Steps por √©poca: {steps_per_epoch}\")\n",
    "    print(f\"üìà Validation steps: {validation_steps}\")\n",
    "    print(f\"üí° Usando generadores del proyecto (CattleDataGenerator)\")\n",
    "    \n",
    "    # Entrenar modelo usando generadores\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        validation_data=val_generator,\n",
    "        callbacks=training_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Entrenamiento completado\")\n",
    "    return history\n",
    "\n",
    "# Entrenamiento real (requiere generadores preparados y tiempo de ejecuci√≥n con GPU)\n",
    "history = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 15: EVALUACI√ìN DEL MODELO\n",
    "# ============================================================\n",
    "# üìä Eval√∫a el modelo usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 14 ejecutado (modelo entrenado)\n",
    "# üí° Usa: MetricsCalculator (models.evaluation.metrics)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä EVALUACI√ìN DEL MODELO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
    "try:\n",
    "    from models.evaluation.metrics import MetricsCalculator, ModelMetrics\n",
    "    print(\"‚úÖ M√≥dulo MetricsCalculator importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
    "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
    "    raise\n",
    "\n",
    "# Verificar que el generador de test existe\n",
    "if 'test_generator' not in globals():\n",
    "    raise ValueError(\"Generador de test no encontrado. Ejecuta el BLOQUE 11 primero.\")\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluar modelo en conjunto de test usando MetricsCalculator\"\"\"\n",
    "    print(\"üìä Evaluando modelo en conjunto de test...\")\n",
    "    \n",
    "    # Obtener predicciones y valores reales\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    print(\"üîç Generando predicciones...\")\n",
    "    for i in range(len(test_generator)):\n",
    "        batch_images, batch_targets = test_generator[i]\n",
    "        predictions = model.predict(batch_images, verbose=0)\n",
    "        y_true.extend(batch_targets.flatten())\n",
    "        y_pred.extend(predictions.flatten())\n",
    "    \n",
    "    # Convertir a numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    print(f\"üìä Total predicciones: {len(y_true):,}\")\n",
    "    \n",
    "    # Calcular m√©tricas usando m√≥dulo del proyecto\n",
    "    print(\"\\nüìà Calculando m√©tricas usando MetricsCalculator...\")\n",
    "    metrics = MetricsCalculator.calculate_metrics(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        breed_type='generic'\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nüìà RESULTADOS DE EVALUACI√ìN:\")\n",
    "    print(f\"   R¬≤: {metrics.r2_score:.4f}\")\n",
    "    print(f\"   MAE: {metrics.mae_kg:.2f} kg\")\n",
    "    print(f\"   MSE: {metrics.mse_kg:.2f}\")\n",
    "    print(f\"   MAPE: {metrics.mape_percent:.2f}%\")\n",
    "    print(f\"   Bias: {metrics.bias_kg:.2f} kg\")\n",
    "    \n",
    "    # Verificar objetivos (con validaci√≥n opcional)\n",
    "    print(f\"\\nüéØ VERIFICACI√ìN DE OBJETIVOS:\")\n",
    "    r2_ok = metrics.r2_score >= CONFIG['target_r2']\n",
    "    mae_ok = metrics.mae_kg < CONFIG['max_mae']\n",
    "    \n",
    "    print(f\"   R¬≤ ‚â• {CONFIG['target_r2']}: {'‚úÖ' if r2_ok else '‚ö†Ô∏è'} ({metrics.r2_score:.4f})\")\n",
    "    print(f\"   MAE < {CONFIG['max_mae']} kg: {'‚úÖ' if mae_ok else '‚ö†Ô∏è'} ({metrics.mae_kg:.2f} kg)\")\n",
    "    \n",
    "    if not (r2_ok and mae_ok):\n",
    "        print(f\"\\nüí° NOTA: Los objetivos no se cumplieron completamente\")\n",
    "        print(f\"üí° Esto es normal en un primer entrenamiento. Puedes:\")\n",
    "        print(f\"   - Ajustar hiperpar√°metros\")\n",
    "        print(f\"   - Entrenar m√°s √©pocas\")\n",
    "        print(f\"   - Usar fine-tuning\")\n",
    "    \n",
    "    # Log m√©tricas en MLflow\n",
    "    if 'mlflow' in globals() and ('mlflow_available' in globals() and mlflow_available):\n",
    "        mlflow.log_metrics({\n",
    "            'test_r2': metrics.r2_score,\n",
    "            'test_mae_kg': metrics.mae_kg,\n",
    "            'test_mse_kg': metrics.mse_kg,\n",
    "            'test_mape_percent': metrics.mape_percent,\n",
    "            'test_bias_kg': metrics.bias_kg\n",
    "        })\n",
    "    \n",
    "    return metrics.to_dict()\n",
    "\n",
    "# Evaluar modelo\n",
    "evaluation_results = evaluate_model()\n",
    "\n",
    "print(f\"\\n‚úÖ BLOQUE 15 COMPLETADO\")\n",
    "print(f\"üí° M√©tricas calculadas usando m√≥dulo del proyecto: MetricsCalculator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 12: EXPORTAR A TFLITE\n",
    "# ============================================================\n",
    "# üì± Exporta modelo usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 15 ejecutado (modelo evaluado)\n",
    "# üí° Usa: TFLiteExporter (models.export.tflite_converter)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì± EXPORTAR A TFLITE (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
    "try:\n",
    "    from models.export.tflite_converter import TFLiteExporter\n",
    "    print(\"‚úÖ M√≥dulo TFLiteExporter importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
    "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
    "    raise\n",
    "\n",
    "# Guardar modelo temporalmente para conversi√≥n\n",
    "print(\"üíæ Guardando modelo temporalmente para conversi√≥n...\")\n",
    "temp_model_path = MODELS_DIR / 'temp_model.h5'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model.save(str(temp_model_path))\n",
    "print(f\"‚úÖ Modelo guardado en: {temp_model_path}\")\n",
    "\n",
    "# Exportar usando TFLiteExporter del proyecto\n",
    "tflite_path = MODELS_DIR / 'generic-cattle-v1.0.0.tflite'\n",
    "\n",
    "print(f\"\\nüì± Exportando modelo a TFLite usando TFLiteExporter...\")\n",
    "print(f\"üìÅ Archivo de salida: {tflite_path}\")\n",
    "\n",
    "# Usar TFLiteExporter del proyecto (optimizaci√≥n FP16 por defecto)\n",
    "model_size_bytes = TFLiteExporter.convert_to_tflite(\n",
    "    saved_model_path=str(temp_model_path),\n",
    "    output_path=str(tflite_path),\n",
    "    optimization='default'  # FP16: reduce 2x el tama√±o, mantiene precisi√≥n\n",
    ")\n",
    "\n",
    "model_size_kb = model_size_bytes / 1024\n",
    "model_size_mb = model_size_kb / 1024\n",
    "\n",
    "# Log en MLflow\n",
    "if 'mlflow' in globals() and ('mlflow_available' in globals() and mlflow_available):\n",
    "    mlflow.log_artifact(str(tflite_path))\n",
    "    mlflow.log_metric('model_size_kb', model_size_kb)\n",
    "    mlflow.log_metric('model_size_mb', model_size_mb)\n",
    "\n",
    "# Limpiar modelo temporal\n",
    "try:\n",
    "    temp_model_path.unlink()\n",
    "    print(f\"üßπ Modelo temporal eliminado\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n‚úÖ BLOQUE 16 COMPLETADO\")\n",
    "print(f\"üéØ MODELO BASE LISTO PARA INTEGRACI√ìN\")\n",
    "print(f\"üìÅ Archivo: {tflite_path}\")\n",
    "print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
    "print(f\"üí° Usando m√≥dulo del proyecto: TFLiteExporter\")\n",
    "if 'mlflow_run' in globals():\n",
    "    print(f\"üî¨ MLflow run: {mlflow_run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Resumen y Pr√≥ximos Pasos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notas Importantes\n",
    "\n",
    "### ‚ö†Ô∏è Configuraci√≥n Requerida\n",
    "1. **Kaggle API**: Subir `kaggle.json` para descargar datasets\n",
    "2. **CID Dataset**: Reemplazar URL simulada con URL real\n",
    "3. **CattleEyeView**: Solicitar acceso a autores del paper\n",
    "\n",
    "### üîß Optimizaciones Implementadas\n",
    "- **Mixed Precision**: FP16 para acelerar entrenamiento\n",
    "- **Data Pipeline**: Cache + prefetch + shuffle optimizado\n",
    "- **Augmentation**: Albumentations espec√≠fico para ganado\n",
    "- **TFLite Export**: Optimizado para m√≥vil\n",
    "\n",
    "### üìä M√©tricas Objetivo\n",
    "- **R¬≤ ‚â• 0.95**: Explicaci√≥n 95% de varianza\n",
    "- **MAE < 5 kg**: Error absoluto promedio\n",
    "- **Inference < 3s**: Tiempo en m√≥vil\n",
    "\n",
    "### üéØ Estado Actual\n",
    "- ‚úÖ **Infraestructura ML**: Completada\n",
    "- ‚úÖ **Pipeline de datos**: Optimizado\n",
    "- ‚úÖ **Modelo base**: Listo para fine-tuning\n",
    "- üîÑ **Pr√≥ximo**: Fine-tuning por raza espec√≠fica\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
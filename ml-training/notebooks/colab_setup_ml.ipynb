{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêÑ Sistema de Estimaci√≥n de Peso Bovino - Setup ML\n",
    "\n",
    "> **BLOQUE 0**: Informaci√≥n del proyecto (markdown - solo lectura)\n",
    "\n",
    "**Proyecto**: Hacienda Gamelera - Bruno Brito Macedo  \n",
    "**Responsable**: Persona 2 - Setup Infraestructura ML  \n",
    "**Objetivo**: Preparar datasets y pipeline para entrenamiento de 7 modelos (razas tropicales priorizadas)  \n",
    "**Duraci√≥n**: 5-6 d√≠as  \n",
    "\n",
    "---\n",
    "\n",
    "## üìë √çndice de Bloques (Referencia R√°pida)\n",
    "\n",
    "| Bloque | Nombre | Descripci√≥n | Requisitos |\n",
    "|--------|--------|-------------|------------|\n",
    "| **0** | Informaci√≥n | Markdown introductorio | Ninguno |\n",
    "| **1** | Clonar Repositorio | Monta Drive y clona desde GitHub (persistente) | Ninguno (requiere internet) |\n",
    "| **2** | Verificar Dependencias | Verifica versiones base de TensorFlow y NumPy | Ninguno |\n",
    "| **3** | Instalar Dependencias Cr√≠ticas | TensorFlow 2.19.0, NumPy 2.x, MLflow, DVC | Ninguno |\n",
    "| **4** | Instalar Complementos | Albumentations, OpenCV, herramientas ML | Ninguno |\n",
    "| **5** | Configuraci√≥n Proyecto | Crea estructura de carpetas y variables globales | Bloque 1 |\n",
    "| **6** | Descargar Im√°genes Propias | Scraping de im√°genes para dataset personalizado | Bloque 5 |\n",
    "| **7** | Descargar CID Dataset | Descarga CID Dataset desde S3 (complementario) | Bloque 5 |\n",
    "| **8** | Preparar Dataset Combinado | Combina CID + Nuestras im√°genes (Estrategia B) | Bloques 6 + 7 |\n",
    "| **9** | Resumen Datasets | Muestra resumen de datasets disponibles (CID + propias) | Bloques 6-8 |\n",
    "| **10** | Verificaci√≥n R√°pida | Verificaci√≥n m√≠nima de columnas requeridas (OPCIONAL) | Bloque 9 |\n",
    "| **11** | Pipeline de Datos | Pipeline con augmentation | Bloque 8 |\n",
    "| **12** | Arquitectura Modelo | Crea modelo EfficientNetB0 | Bloque 11 |\n",
    "| **13** | Configurar Entrenamiento | Callbacks y MLflow | Bloque 12 |\n",
    "| **14** | Entrenamiento | Entrena modelo (2-4h) | Bloque 13 + GPU |\n",
    "| **15** | Evaluaci√≥n | Eval√∫a modelo | Bloque 14 |\n",
    "| **16** | Exportar TFLite | Exporta modelo a TFLite | Bloque 15 |\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Flujo de Trabajo\n",
    "\n",
    "### D√≠a 1: Setup (Bloques 1-5)\n",
    "- **BLOQUE 1**: Clonar repositorio en Drive\n",
    "- **BLOQUE 2**: Verificar dependencias base\n",
    "- **BLOQUE 3**: Instalar dependencias cr√≠ticas (TensorFlow, MLflow)\n",
    "- **BLOQUE 4**: Instalar complementos (Albumentations, OpenCV)\n",
    "- **BLOQUE 5**: Configurar proyecto y carpetas\n",
    "\n",
    "### D√≠a 2-3: Datasets (Bloques 6-9)\n",
    "- **BLOQUE 6**: Descargar nuestras im√°genes (razas bolivianas, etapas de crianza)\n",
    "- **BLOQUE 7**: Descargar CID Dataset (complementario - 17,899+ im√°genes)\n",
    "- **BLOQUE 8**: Preparar dataset combinado (Estrategia B: combina CID + nuestras im√°genes)\n",
    "- **BLOQUE 9**: Resumen de datasets disponibles (verifica combinaci√≥n)\n",
    "\n",
    "### D√≠a 4: Verificaci√≥n (Bloque 10) - OPCIONAL\n",
    "- **BLOQUE 10**: Verificaci√≥n r√°pida de datos (solo comprueba columnas necesarias, sin gr√°ficos)\n",
    "- üí° **NOTA**: Este bloque es OPCIONAL. Puedes saltarlo para entrenar m√°s r√°pido.\n",
    "\n",
    "### D√≠a 5-6: Pipeline y Modelo (Bloques 11-16)\n",
    "- **BLOQUE 11**: Pipeline de datos con augmentation (usa dataset combinado)\n",
    "- **BLOQUE 12**: Arquitectura del modelo\n",
    "- **BLOQUE 13**: Configurar entrenamiento\n",
    "- **BLOQUE 14**: Entrenar modelo\n",
    "- **BLOQUE 15**: Evaluaci√≥n del modelo\n",
    "- **BLOQUE 16**: Exportar a TFLite\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Razas Objetivo (7 razas)\n",
    "1. **Nelore** ‚Äì Carne tropical dominante en Santa Cruz (‚âà42% del hato)\n",
    "2. **Brahman** ‚Äì Cebuino vers√°til para cruzamientos y climas extremos\n",
    "3. **Guzerat** ‚Äì Doble prop√≥sito (carne/leche) con gran rusticidad materna\n",
    "4. **Senepol** ‚Äì Carne premium adaptada al calor, ideal para ‚Äústeer‚Äù de alta calidad\n",
    "5. **Girolando** ‚Äì Lechera tropical (Holstein √ó Gyr) muy difundida en sistemas semi-intensivos\n",
    "6. **Gyr lechero** ‚Äì Lechera pura clave para gen√©tica tropical y s√≥lidos altos\n",
    "7. **Sindi** ‚Äì Lechera tropical compacta, de alta fertilidad y leche rica en s√≥lidos\n",
    "\n",
    "> Estas razas cubren el portafolio real de Santa Cruz (carne tropical + lecheras adaptadas). M√°s adelante podemos sumar Holstein, Pardo Suizo o Jersey si obtenemos datos suficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 1: CONFIGURAR RUTA DEL PROYECTO Y CLONAR REPOSITORIO EN DRIVE\n",
    "# ============================================================\n",
    "# üìÅ Clona el repositorio desde GitHub a Google Drive (persistente entre sesiones)\n",
    "# üîó Repositorio: https://github.com/Angello-27/bovine-weight-estimation.git\n",
    "# üíæ Se clona en Drive para que persista entre desconexiones del runtime\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "GITHUB_REPO_URL = 'https://github.com/Angello-27/bovine-weight-estimation.git'\n",
    "\n",
    "# Montar Google Drive (solo si no est√° montado)\n",
    "print(\"üîó Verificando Google Drive...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive_path = Path('/content/drive')\n",
    "    if not drive_path.exists() or not any(drive_path.iterdir()):\n",
    "        drive.mount('/content/drive')\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
    "except ImportError:\n",
    "    BASE_DIR = Path('/content/bovine-weight-estimation')\n",
    "\n",
    "print(\"üìÅ Directorio base: {BASE_DIR}\")\n",
    "\n",
    "# Clonar o sincronizar repositorio\n",
    "if BASE_DIR.exists() and (BASE_DIR / '.git').exists():\n",
    "    print(\"üîÑ Sincronizando repositorio existente...\")\n",
    "    subprocess.run(['git', 'pull'], cwd=str(BASE_DIR), check=False)\n",
    "else:\n",
    "    print(\"üì• Clonando repositorio...\")\n",
    "    BASE_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    result = subprocess.run(['git', 'clone', GITHUB_REPO_URL, str(BASE_DIR)], check=False)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Error al clonar repositorio. Verifica conexi√≥n a internet.\")\n",
    "\n",
    "# Configurar PYTHONPATH\n",
    "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
    "src_dir = ML_TRAINING_DIR / 'src'\n",
    "if src_dir.exists():\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "    print(\"‚úÖ PYTHONPATH configurado: {src_dir}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Directorio src no encontrado: {src_dir}\")\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n completada\")\n",
    "print(\"üìÅ ML Training: {ML_TRAINING_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ D√≠a 1: Setup Google Colab Pro + Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 2: VERIFICACI√ìN DE DEPENDENCIAS BASE\n",
    "# ============================================================\n",
    "# üîç Verifica versiones base de TensorFlow y NumPy\n",
    "# üí° Solo verifica - no desinstala (pip maneja versiones autom√°ticamente)\n",
    "# ‚ö†Ô∏è Si hay conflictos, ejecuta limpieza manual o reinicia el runtime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç VERIFICANDO DEPENDENCIAS BASE...\\n\")\n",
    "\n",
    "# Verificar versiones actuales\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    print(\"üì¶ Versiones actuales:\")\n",
    "    print(\"   - TensorFlow: {tf.__version__}\")\n",
    "    print(\"   - NumPy: {np.__version__}\")\n",
    "    \n",
    "    # Verificar compatibilidad b√°sica\n",
    "    tf_ok = tf.__version__.startswith('2.')\n",
    "    numpy_ok = np.__version__.startswith('2.') or np.__version__.startswith('1.')\n",
    "    \n",
    "    if tf_ok and numpy_ok:\n",
    "        print(\"\\n‚úÖ Versiones compatibles detectadas\")\n",
    "        print(\"üí° Contin√∫a con el BLOQUE 3 para instalar dependencias\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Versiones pueden tener conflictos\")\n",
    "        print(\"üí° Recomendaci√≥n: Reinicia el runtime o ejecuta limpieza manual\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(\"‚ö†Ô∏è Error importando dependencias: {e}\")\n",
    "    print(\"üí° Esto es normal en un runtime nuevo - contin√∫a con el BLOQUE 3\")\n",
    "\n",
    "print(\"\\nüí° NOTA: El BLOQUE 3 instalar√° las versiones correctas autom√°ticamente\")\n",
    "print(\"üí° No es necesario desinstalar/reinstalar manualmente si las versiones est√°n bien definidas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 3: INSTALACI√ìN DE DEPENDENCIAS CR√çTICAS\n",
    "# ============================================================\n",
    "# üîß Instala dependencias cr√≠ticas con versiones exactas\n",
    "# ‚úÖ pip maneja autom√°ticamente las actualizaciones\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ INSTALANDO DEPENDENCIAS CR√çTICAS...\\n\")\n",
    "\n",
    "# Instalar dependencias en orden\n",
    "dependencies = [\n",
    "    (\"tf-keras\", \"tf-keras>=2.19.0\"),\n",
    "    (\"packaging\", \"packaging<25\"),\n",
    "    (\"wrapt\", \"wrapt<2.0.0,>=1.10.10\"),\n",
    "    (\"requests\", \"requests==2.32.4\"),\n",
    "    (\"jedi\", \"jedi>=0.16\"),\n",
    "    (\"MLflow\", \"mlflow==2.16.2\"),\n",
    "    (\"DVC\", \"dvc[gs,s3]==3.51.1\"),\n",
    "    (\"scikit-learn\", \"scikit-learn>=1.6\")\n",
    "]\n",
    "\n",
    "for name, package in dependencies:\n",
    "    result = subprocess.run(\n",
    "        ['pip', 'install', '-q', '--no-cache-dir', package],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ {name} instalado\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Error instalando {name}\")\n",
    "\n",
    "# Configurar mixed precision\n",
    "try:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"‚úÖ Mixed precision (FP16) activado\\n\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Mixed precision no disponible: {str(e)[:50]}\\n\")\n",
    "\n",
    "# Verificar instalaciones\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ DEPENDENCIAS CR√çTICAS INSTALADAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "versions = {\n",
    "    'TensorFlow': tf.__version__,\n",
    "    'NumPy': np.__version__\n",
    "}\n",
    "\n",
    "packages_to_check = {\n",
    "    'MLflow': ('mlflow', '__version__'),\n",
    "    'Scikit-learn': ('sklearn', '__version__'),\n",
    "    'Protobuf': ('google.protobuf', '__version__'),\n",
    "    'ml_dtypes': ('ml_dtypes', '__version__'),\n",
    "    'tf-keras': ('tensorflow.keras', '__version__')\n",
    "}\n",
    "\n",
    "for name, version in versions.items():\n",
    "    print(\"   - {name}: {version}\")\n",
    "\n",
    "for name, (module, attr) in packages_to_check.items():\n",
    "    try:\n",
    "        mod = __import__(module, fromlist=[attr])\n",
    "        version = getattr(mod, attr) if hasattr(mod, attr) else getattr(mod, '__version__', 'OK')\n",
    "        print(\"   - {name}: {version} ‚úÖ\")\n",
    "    except Exception:\n",
    "        print(\"   - {name}: No disponible ‚ö†Ô∏è\")\n",
    "\n",
    "# Verificaci√≥n de compatibilidad\n",
    "tf_ok = tf.__version__.startswith('2.19')\n",
    "numpy_ok = np.__version__.startswith('2.0') or np.__version__.startswith('2.1')\n",
    "\n",
    "print(\"\\nüîç Compatibilidad: TF 2.19.x {'‚úÖ' if tf_ok else '‚ö†Ô∏è'}, NumPy 2.x {'‚úÖ' if numpy_ok else '‚ö†Ô∏è'}\")\n",
    "\n",
    "if tf_ok and numpy_ok:\n",
    "    print(\"\\n‚úÖ Instalaci√≥n completada exitosamente\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Verifica versiones instaladas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 4: INSTALACI√ìN DE COMPLEMENTOS\n",
    "# ============================================================\n",
    "# üîß Instala complementos: Albumentations, OpenCV, herramientas ML\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ INSTALANDO COMPLEMENTOS...\\n\")\n",
    "\n",
    "# Instalar complementos\n",
    "complements = [\n",
    "    (\"albumentations>=2.0.8\", \"Albumentations\"),\n",
    "    (\"gdown\", \"gdown\"),\n",
    "    (\"plotly\", \"Plotly\"),\n",
    "    (\"seaborn\", \"Seaborn\"),\n",
    "    (\"pillow>=11.0.0\", \"Pillow\")\n",
    "]\n",
    "\n",
    "for package, name in complements:\n",
    "    result = subprocess.run(\n",
    "        ['pip', 'install', '-q', '--no-cache-dir', package],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ {name} instalado\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Error instalando {name}\")\n",
    "\n",
    "# Verificar/instalar OpenCV\n",
    "try:\n",
    "    import cv2\n",
    "    print(\"‚úÖ OpenCV {cv2.__version__} disponible\")\n",
    "except ImportError:\n",
    "    result = subprocess.run(\n",
    "        ['pip', 'install', '-q', '--no-cache-dir', 'opencv-python-headless'],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        import cv2\n",
    "        print(\"‚úÖ OpenCV {cv2.__version__} instalado\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Error instalando OpenCV\")\n",
    "\n",
    "# Verificar e importar\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    import albumentations as A\n",
    "    import sklearn\n",
    "    print(\"\\n‚úÖ Complementos verificados: OpenCV {cv2.__version__}, Albumentations {A.__version__}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ö†Ô∏è Algunos complementos no disponibles: {str(e)[:50]}\")\n",
    "\n",
    "# Configurar GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"\\n‚úÖ GPU configurada: {len(gpus)} dispositivo(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(\"   - GPU {i}: {gpu.name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"\\n‚ö†Ô∏è Error configurando GPU: {str(e)[:50]}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è GPU no detectada - Activa en: Entorno > Cambiar tipo > GPU\")\n",
    "\n",
    "print(\"\\n‚úÖ Instalaci√≥n de complementos completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 5: CONFIGURACI√ìN DEL PROYECTO Y ESTRUCTURA DE CARPETAS\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è Crea estructura de carpetas y configura variables globales\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar y montar Google Drive si es necesario\n",
    "print(\"üîó Verificando Google Drive...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive_path = Path('/content/drive')\n",
    "    if not drive_path.exists() or not any(drive_path.iterdir()):\n",
    "        print(\"üì• Montando Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive montado\")\n",
    "    else:\n",
    "        print(\"‚úÖ Google Drive ya est√° montado\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è No se detect√≥ Google Colab - continuando sin Drive\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Error al montar Drive: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de autorizar el acceso a Google Drive\")\n",
    "\n",
    "# Verificar proyecto\n",
    "BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
    "if not BASE_DIR.exists():\n",
    "    raise RuntimeError(f\"Proyecto no encontrado en {BASE_DIR}. Ejecuta el bloque anterior primero.\")\n",
    "\n",
    "print(\"‚úÖ Proyecto: {BASE_DIR}\")\n",
    "\n",
    "# Crear estructura de carpetas\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "AUGMENTED_DIR = DATA_DIR / 'augmented'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MLRUNS_DIR = BASE_DIR / 'mlruns'\n",
    "\n",
    "for dir_path in [DATA_DIR, RAW_DIR, PROCESSED_DIR, AUGMENTED_DIR, MODELS_DIR, MLRUNS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurar MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")\n",
    "    mlflow.set_experiment(\"bovine-weight-estimation\")\n",
    "    mlflow_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è MLflow no disponible - ejecuta bloque de instalaci√≥n\")\n",
    "    mlflow_available = False\n",
    "\n",
    "# Configuraci√≥n de entrenamiento\n",
    "CONFIG = {\n",
    "    'image_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'epochs': 200,  # Aumentado para dataset m√°s grande (con CID completo)\n",
    "    'learning_rate': 0.0005,  # Reducido para evitar sobreentrenamiento\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'early_stopping_patience': 15,  # Aumentado para permitir m√°s entrenamiento con m√°s datos\n",
    "    'target_r2': 0.95,\n",
    "    'max_mae': 5.0,\n",
    "    'max_inference_time': 3.0\n",
    "}\n",
    "\n",
    "# Nota: Las razas se definen en BREED_SEARCH_TERMS del BLOQUE 6\n",
    "# 7 razas tropicales: Nelore, Brahman, Guzerat, Senepol, Girolando, Gyr lechero, Sindi\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n completada\")\n",
    "print(\"üìÅ Carpetas creadas: data/, models/, mlruns/\")\n",
    "print(\"üéØ Razas: 7 razas tropicales (definidas en BLOQUE 6)\")\n",
    "if mlflow_available:\n",
    "    print(\"üìä MLflow: {MLRUNS_DIR} ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• D√≠a 2-3: Descargar y Organizar Datasets Cr√≠ticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 6: SCRAPING SIMPLE\n",
    "# ============================================================\n",
    "# üéØ Descarga im√°genes para todas las razas definidas\n",
    "# üìã Razas: Nelore, Brahman, Guzerat, Senepol, Girolando, Gyr lechero, Sindi\n",
    "# üßÆ Genera metadata_estimada.csv (weight_in_kg, breed, life_stage) alineada con CID\n",
    "\n",
    "import subprocess\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì• BLOQUE 6: DESCARGAR IM√ÅGENES\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "print(\"üìÅ RAW_DIR: {RAW_DIR}\")\n",
    "\n",
    "random.seed(42)\n",
    "SCRAPED_DIR = RAW_DIR / 'scraped'\n",
    "SCRAPED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SCRAPED_METADATA_FILE = SCRAPED_DIR / 'metadata_estimada.csv'\n",
    "CID_REFERENCE_COLUMNS = ['sku', 'sex', 'color', 'breed', 'feed', 'age_in_year', 'teeth', 'height_in_inch', 'weight_in_kg', 'price', 'size', 'images_count', 'yt_images_count', 'total_images']\n",
    "print(\"üìã Columnas de referencia CID: {CID_REFERENCE_COLUMNS}\")\n",
    "\n",
    "# Configuraci√≥n\n",
    "BREED = 'nelore'\n",
    "IMAGES_LIMIT = 200  # Objetivo: 180-200 im√°genes por raza\n",
    "\n",
    "# T√©rminos de b√∫squeda por raza\n",
    "BREED_SEARCH_TERMS = {\n",
    "    'nelore': [\n",
    "        'nelore cattle',\n",
    "        'nelore bull',\n",
    "        'nelore cow',\n",
    "        'nelore tropical beef',\n",
    "        'nelore pasture cattle',\n",
    "        'nelore bolivia ranch'\n",
    "    ],\n",
    "    'brahman': [\n",
    "        'brahman cattle',\n",
    "        'brahman bull',\n",
    "        'brahman cow',\n",
    "        'brahman tropical ranch',\n",
    "        'brahman beef cattle',\n",
    "        'brahman zebu'\n",
    "    ],\n",
    "    'guzerat': [\n",
    "        'guzerat cattle',\n",
    "        'guzer√° bovino',\n",
    "        'guzerat bull',\n",
    "        'guzerat cow',\n",
    "        'guzerat double purpose',\n",
    "        'guzera brasil'\n",
    "    ],\n",
    "    'senepol': [\n",
    "        'senepol cattle',\n",
    "        'senepol bull',\n",
    "        'senepol cow',\n",
    "        'senepol tropical beef',\n",
    "        'senepol herd',\n",
    "        'senepol caribbean cattle'\n",
    "    ],\n",
    "    'girolando': [\n",
    "        'girolando cattle',\n",
    "        'girolando cow',\n",
    "        'girolando dairy',\n",
    "        'girolando pasture',\n",
    "        'girolando bolivia',\n",
    "        'girolando brasil'\n",
    "    ],\n",
    "    'gyr_lechero': [\n",
    "        'gyr lechero',\n",
    "        'gir leiteiro',\n",
    "        'gyr dairy cattle',\n",
    "        'gir lechera',\n",
    "        'gyr bull',\n",
    "        'gir leiteiro brasil'\n",
    "    ],\n",
    "    'sindi': [\n",
    "        'sindi cattle',\n",
    "        'red sindhi cattle',\n",
    "        'sindi cow',\n",
    "        'sindi dairy',\n",
    "        'sindi bolivia',\n",
    "        'red sindhi tropical'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Keywords de rechazo por raza\n",
    "# Todas las razas comparten la restricci√≥n 'artistic'\n",
    "ARTISTIC_KEYWORDS = ['drawing', 'sketch', 'painting', 'art', 'illustration', 'cartoon', 'logo', 'brand', 'graphic', 'vector']\n",
    "\n",
    "REJECTION_KEYWORDS = {\n",
    "    'nelore': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': []\n",
    "    },\n",
    "    'brahman': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': ['hindu', 'god', 'deity', 'temple', 'statue', 'sculpture', 'worship', 'puja', 'idol', 'brahma']\n",
    "    },\n",
    "    'guzerat': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': ['jewelry', 'ornament', 'toy']\n",
    "    },\n",
    "    'senepol': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': ['logo', 'brand', 'diagram', 'map']\n",
    "    },\n",
    "    'girolando': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': ['holstein show', 'expo holstein', 'logo holstein']\n",
    "    },\n",
    "    'gyr_lechero': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': ['manga', 'comic', 'statue']\n",
    "    },\n",
    "    'sindi': {\n",
    "        'artistic': ARTISTIC_KEYWORDS,\n",
    "        'other': ['cloth', 'jersey', 'shirt', 'team', 'sindhi people']\n",
    "    }\n",
    "}\n",
    "\n",
    "BREED_LIFESTAGE_PRIORS = {\n",
    "    'nelore': {'ternera': 0.18, 'novillo': 0.42, 'vaca': 0.25, 'toro': 0.15},\n",
    "    'brahman': {'ternera': 0.15, 'novillo': 0.35, 'vaca': 0.30, 'toro': 0.20},\n",
    "    'guzerat': {'ternera': 0.20, 'novillo': 0.30, 'vaca': 0.30, 'toro': 0.20},\n",
    "    'senepol': {'ternera': 0.20, 'novillo': 0.45, 'vaca': 0.20, 'toro': 0.15},\n",
    "    'girolando': {'ternera': 0.25, 'novilla_lechera': 0.35, 'vaca_lechera': 0.30, 'toro': 0.10},\n",
    "    'gyr_lechero': {'ternera': 0.30, 'novilla_lechera': 0.35, 'vaca_lechera': 0.25, 'toro': 0.10},\n",
    "    'sindi': {'ternera': 0.30, 'novilla_lechera': 0.40, 'vaca_lechera': 0.30}\n",
    "}\n",
    "\n",
    "LIFESTAGE_WEIGHT_RANGES = {\n",
    "    'nelore': {\n",
    "        'ternera': (90, 160),\n",
    "        'novillo': (250, 380),\n",
    "        'vaca': (380, 520),\n",
    "        'toro': (480, 650)\n",
    "    },\n",
    "    'brahman': {\n",
    "        'ternera': (95, 170),\n",
    "        'novillo': (260, 400),\n",
    "        'vaca': (390, 540),\n",
    "        'toro': (500, 680)\n",
    "    },\n",
    "    'guzerat': {\n",
    "        'ternera': (85, 150),\n",
    "        'novillo': (240, 360),\n",
    "        'vaca': (360, 520),\n",
    "        'toro': (480, 650)\n",
    "    },\n",
    "    'senepol': {\n",
    "        'ternera': (100, 170),\n",
    "        'novillo': (280, 400),\n",
    "        'vaca': (360, 480),\n",
    "        'toro': (500, 620)\n",
    "    },\n",
    "    'girolando': {\n",
    "        'ternera': (80, 140),\n",
    "        'novilla_lechera': (240, 340),\n",
    "        'vaca_lechera': (420, 580),\n",
    "        'toro': (500, 640)\n",
    "    },\n",
    "    'gyr_lechero': {\n",
    "        'ternera': (70, 130),\n",
    "        'novilla_lechera': (220, 320),\n",
    "        'vaca_lechera': (380, 520),\n",
    "        'toro': (470, 620)\n",
    "    },\n",
    "    'sindi': {\n",
    "        'ternera': (60, 100),\n",
    "        'novilla_lechera': (150, 230),\n",
    "        'vaca_lechera': (260, 380)\n",
    "    }\n",
    "}\n",
    "\n",
    "LIFESTAGE_METADATA_HINTS = {\n",
    "    'ternera': {'age_in_year': 0.8, 'sex': 'FEMALE_CALF'},\n",
    "    'novilla_lechera': {'age_in_year': 1.8, 'sex': 'FEMALE_HEIFER'},\n",
    "    'novillo': {'age_in_year': 2.0, 'sex': 'MALE_STEER'},\n",
    "    'vaca': {'age_in_year': 4.5, 'sex': 'FEMALE_COW'},\n",
    "    'vaca_lechera': {'age_in_year': 4.0, 'sex': 'FEMALE_COW'},\n",
    "    'toro': {'age_in_year': 5.5, 'sex': 'MALE_BULL'}\n",
    "}\n",
    "\n",
    "\n",
    "def select_life_stage(breed: str) -> str:\n",
    "    priors = BREED_LIFESTAGE_PRIORS.get(breed)\n",
    "    if not priors:\n",
    "        return 'novillo'\n",
    "    roll = random.random()\n",
    "    cumulative = 0.0\n",
    "    for stage, prob in priors.items():\n",
    "        cumulative += prob\n",
    "        if roll <= cumulative:\n",
    "            return stage\n",
    "    return list(priors.keys())[-1]\n",
    "\n",
    "\n",
    "def estimate_weight(breed: str, life_stage: str) -> float:\n",
    "    breed_ranges = LIFESTAGE_WEIGHT_RANGES.get(breed, {})\n",
    "    min_w, max_w = breed_ranges.get(life_stage, (250, 420))\n",
    "    return round(random.uniform(min_w, max_w), 2)\n",
    "\n",
    "\n",
    "def build_metadata_for_breed(breed: str, breed_dir: Path):\n",
    "    image_paths = []\n",
    "    for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "        image_paths.extend(sorted(breed_dir.glob(ext)))\n",
    "    if not image_paths:\n",
    "        return\n",
    "\n",
    "    records = []\n",
    "    for img_path in image_paths:\n",
    "        if not img_path.exists() or not img_path.is_file():\n",
    "            continue\n",
    "        life_stage = select_life_stage(breed)\n",
    "        weight = estimate_weight(breed, life_stage)\n",
    "        rel_path = img_path.relative_to(RAW_DIR)\n",
    "        hints = LIFESTAGE_METADATA_HINTS.get(life_stage, {'age_in_year': None, 'sex': 'UNKNOWN'})\n",
    "        records.append({\n",
    "            'image_filename': rel_path.as_posix(),\n",
    "            'breed': breed,\n",
    "            'life_stage': life_stage,\n",
    "            'weight_kg': weight,\n",
    "            'weight_in_kg': weight,\n",
    "            'weight_source': 'estimado',\n",
    "            'sex': hints.get('sex', 'UNKNOWN'),\n",
    "            'age_in_year': hints.get('age_in_year')\n",
    "        })\n",
    "\n",
    "    df_breed = pd.DataFrame(records)\n",
    "    if df_breed.empty:\n",
    "        return\n",
    "\n",
    "    if SCRAPED_METADATA_FILE.exists():\n",
    "        df_existing = pd.read_csv(SCRAPED_METADATA_FILE)\n",
    "        df_existing = df_existing[df_existing['breed'] != breed]\n",
    "        df_combined = pd.concat([df_existing, df_breed], ignore_index=True)\n",
    "    else:\n",
    "        df_combined = df_breed\n",
    "\n",
    "    df_combined.to_csv(SCRAPED_METADATA_FILE, index=False)\n",
    "    print(\"üßÆ Metadata estimada actualizada para {breed} ({len(df_breed)} registros)\")\n",
    "\n",
    "\n",
    "def validate_image(img_path: Path, breed: str, rejection_keywords: dict = None) -> bool:\n",
    "    \"\"\"\n",
    "    Valida que la imagen sea apropiada (versi√≥n simplificada y optimizada):\n",
    "    - Rechaza dibujos, pinturas, logos, marcas (artistic keywords) - solo por nombre de archivo\n",
    "    - Rechaza t√©rminos espec√≠ficos seg√∫n la raza (other keywords) - solo por nombre de archivo\n",
    "    - Verificaci√≥n b√°sica de tama√±o y formato\n",
    "    - Validaci√≥n de grupos simplificada (menos estricta)\n",
    "    \n",
    "    Args:\n",
    "        img_path: Ruta de la imagen a validar\n",
    "        breed: Raza del ganado\n",
    "        rejection_keywords: Diccionario con 'artistic' y 'other' keywords\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Obtener keywords de rechazo (usar los globales si no se proporcionan)\n",
    "        if rejection_keywords is None:\n",
    "            rejection_keywords = REJECTION_KEYWORDS.get(breed, {\n",
    "                'artistic': [],\n",
    "                'other': []\n",
    "            })\n",
    "        \n",
    "        artistic_keywords = rejection_keywords.get('artistic', [])\n",
    "        other_keywords = rejection_keywords.get('other', [])\n",
    "        \n",
    "        # Verificar nombre de archivo PRIMERO (m√°s r√°pido)\n",
    "        filename_lower = img_path.name.lower()\n",
    "        \n",
    "        # Rechazar si tiene palabras art√≠sticas (dibujos, pinturas, logos)\n",
    "        if any(keyword in filename_lower for keyword in artistic_keywords):\n",
    "            return False\n",
    "        \n",
    "        # Rechazar si tiene palabras espec√≠ficas de la raza (other keywords)\n",
    "        if any(keyword in filename_lower for keyword in other_keywords):\n",
    "            return False\n",
    "        \n",
    "        # Verificar tama√±o m√≠nimo y formato b√°sico\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Verificaci√≥n b√°sica de tama√±o\n",
    "        if width < 150 or height < 150:  # M√°s permisivo (antes era 200)\n",
    "            return False\n",
    "        \n",
    "        if width * height < 22500:  # M√°s permisivo (antes era 40000)\n",
    "            return False\n",
    "        \n",
    "        # Validaci√≥n de grupos simplificada (menos estricta)\n",
    "        aspect_ratio = width / height\n",
    "        # Solo rechazar proporciones extremas (antes era >2.5 o <0.4)\n",
    "        if aspect_ratio > 3.5:  # Muy ancha, probablemente paisaje o grupo muy grande\n",
    "            return False\n",
    "        if aspect_ratio < 0.3:  # Muy alta, probablemente grupo vertical muy grande\n",
    "            return False\n",
    "        \n",
    "        # Si pasa todas las validaciones, aceptar\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Si hay error al procesar, rechazar por seguridad\n",
    "        return False\n",
    "\n",
    "def download_images(breed: str, search_terms: list, output_dir: Path, limit: int, rejection_keywords: dict = None):\n",
    "    \"\"\"\n",
    "    Descarga im√°genes simples.\n",
    "    \n",
    "    Args:\n",
    "        breed: Raza del ganado\n",
    "        search_terms: Lista de t√©rminos de b√∫squeda\n",
    "        output_dir: Directorio de salida\n",
    "        limit: L√≠mite de im√°genes a descargar\n",
    "        rejection_keywords: Keywords de rechazo para validaci√≥n\n",
    "    \"\"\"\n",
    "    breed_dir = output_dir / breed\n",
    "    breed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"üìÇ Creando carpeta: {breed_dir}\")\n",
    "    \n",
    "    # Obtener keywords de rechazo si no se proporcionan\n",
    "    if rejection_keywords is None:\n",
    "        rejection_keywords = REJECTION_KEYWORDS.get(breed, {\n",
    "            'artistic': [],\n",
    "            'other': []\n",
    "        })\n",
    "    \n",
    "    # Calcular rango objetivo (90-100% del l√≠mite)\n",
    "    min_target = int(limit * 0.9)  # 90% del l√≠mite (180 para 200)\n",
    "    max_target = limit  # 100% del l√≠mite (200)\n",
    "    \n",
    "    # Contar im√°genes existentes\n",
    "    existing_imgs = []\n",
    "    for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "        for img in breed_dir.glob(ext):\n",
    "            if img.exists() and img.is_file() and img.stat().st_size > 0:\n",
    "                existing_imgs.append(img)\n",
    "    \n",
    "    downloaded = len(existing_imgs)\n",
    "    if downloaded > 0:\n",
    "        print(\"üìä Im√°genes existentes: {downloaded}\")\n",
    "    \n",
    "    if downloaded >= min_target:\n",
    "        if downloaded >= max_target:\n",
    "            print(\"‚úÖ Ya existen {downloaded} im√°genes (objetivo alcanzado: {min_target}-{max_target})\")\n",
    "        else:\n",
    "            print(\"‚úÖ Ya existen {downloaded} im√°genes (dentro del rango objetivo: {min_target}-{max_target})\")\n",
    "        return downloaded\n",
    "    \n",
    "    if downloaded > 0:\n",
    "        print(\"üìä Im√°genes existentes: {downloaded}/{limit}\")\n",
    "        print(\"üì• Descargando hasta alcanzar rango objetivo: {min_target}-{max_target} im√°genes...\")\n",
    "    else:\n",
    "        print(\"üì• Descargando hasta alcanzar rango objetivo: {min_target}-{max_target} im√°genes...\")\n",
    "    print()\n",
    "    \n",
    "    # Instalar dependencias si es necesario\n",
    "    print(\"üîß Verificando dependencias...\")\n",
    "    try:\n",
    "        from bing_image_downloader import downloader\n",
    "        print(\"‚úÖ bing-image-downloader disponible\")\n",
    "    except ImportError:\n",
    "        print(\"üì¶ Instalando bing-image-downloader...\")\n",
    "        subprocess.run(['pip', 'install', '-q', 'bing-image-downloader'], check=False)\n",
    "        from bing_image_downloader import downloader\n",
    "        print(\"‚úÖ bing-image-downloader instalado\")\n",
    "    \n",
    "    try:\n",
    "        from PIL import Image\n",
    "        print(\"‚úÖ PIL/Pillow disponible\")\n",
    "    except ImportError:\n",
    "        print(\"üì¶ Instalando Pillow...\")\n",
    "        subprocess.run(['pip', 'install', '-q', 'Pillow'], check=False)\n",
    "        print(\"‚úÖ Pillow instalado\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üîç Validaci√≥n activa (optimizada):\")\n",
    "    print(\"   - Rechazando t√©rminos no deseados seg√∫n keywords espec√≠ficos de la raza\")\n",
    "    print(\"   - Rechazando dibujos, pinturas, logos, marcas (por nombre de archivo)\")\n",
    "    print()\n",
    "    \n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    \n",
    "    # Suprimir mensajes\n",
    "    original_stderr = sys.stderr\n",
    "    original_stdout = sys.stdout\n",
    "    \n",
    "    # Bucle principal: continuar hasta alcanzar el rango objetivo\n",
    "    iteration = 0\n",
    "    max_iterations = 5  # M√°ximo de iteraciones (reducido porque descargamos m√°s por t√©rmino)\n",
    "    \n",
    "    while downloaded < min_target and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        term_num = 0\n",
    "        \n",
    "        for search_term in search_terms:\n",
    "            # Si ya alcanzamos el objetivo, salir\n",
    "            if downloaded >= min_target:\n",
    "                break\n",
    "            \n",
    "            # Si alcanzamos el m√°ximo, salir\n",
    "            if downloaded >= max_target:\n",
    "                break\n",
    "            \n",
    "            term_num += 1\n",
    "            remaining = min_target - downloaded\n",
    "            # Aumentar batch_size significativamente para ser m√°s eficiente\n",
    "            # Descargar m√°s im√°genes de una vez para reducir iteraciones\n",
    "            batch_size = min(remaining + 20, 30)  # Para los √∫ltimos, descargar menos\n",
    "            \n",
    "            print(\"üîç Iteraci√≥n {iteration}, T√©rmino {term_num}/{len(search_terms)}: '{search_term}'\")\n",
    "            print(\"   Descargando hasta {batch_size} im√°genes... (objetivo: {downloaded}/{min_target})\", end=' ', flush=True)\n",
    "            \n",
    "            try:\n",
    "                # Obtener lista de carpetas existentes ANTES de descargar\n",
    "                existing_dirs_before = set()\n",
    "                known_breeds = set(BREED_SEARCH_TERMS.keys())\n",
    "                for item in output_dir.iterdir():\n",
    "                    if item.is_dir() and item.name not in known_breeds:\n",
    "                        existing_dirs_before.add(item.name)\n",
    "                \n",
    "                sys.stderr = StringIO()\n",
    "                sys.stdout = StringIO()\n",
    "                \n",
    "                downloader.download(\n",
    "                    search_term,\n",
    "                    limit=batch_size,\n",
    "                    output_dir=str(output_dir),\n",
    "                    adult_filter_off=True,\n",
    "                    force_replace=False,\n",
    "                    timeout=10,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                sys.stderr = original_stderr\n",
    "                sys.stdout = original_stdout\n",
    "                \n",
    "                # Buscar y mover im√°genes descargadas SOLO en carpetas nuevas (creadas por esta descarga)\n",
    "                new_images = []\n",
    "                for item in output_dir.iterdir():\n",
    "                    if item.is_dir() and item.name not in known_breeds:\n",
    "                        # Solo procesar si es una carpeta nueva (no exist√≠a antes de esta descarga)\n",
    "                        if item.name not in existing_dirs_before:\n",
    "                            for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "                                for img in item.rglob(ext):\n",
    "                                    if img.exists() and img.is_file():\n",
    "                                        new_images.append(img)\n",
    "                \n",
    "                moved_count = 0\n",
    "                rejected_count = 0\n",
    "                \n",
    "                # Mover im√°genes a carpeta de raza (con validaci√≥n)\n",
    "                for img_path in new_images:\n",
    "                    # Si ya alcanzamos el m√°ximo, salir\n",
    "                    if downloaded >= max_target:\n",
    "                        break\n",
    "                    \n",
    "                    # Validar imagen antes de mover\n",
    "                    if not validate_image(img_path, breed, rejection_keywords):\n",
    "                        rejected_count += 1\n",
    "                        try:\n",
    "                            img_path.unlink()  # Eliminar imagen rechazada\n",
    "                        except:\n",
    "                            pass\n",
    "                        continue\n",
    "                    \n",
    "                    # Proceder a guardar\n",
    "                    next_num = downloaded + 1\n",
    "                    new_name = breed_dir / f\"{breed}_{next_num:03d}{img_path.suffix}\"\n",
    "                    \n",
    "                    if new_name.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.move(str(img_path), str(new_name))\n",
    "                        if new_name.exists() and new_name.is_file() and new_name.stat().st_size > 0:\n",
    "                            downloaded += 1\n",
    "                            moved_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                print(\"‚úÖ {moved_count} im√°genes guardadas ({downloaded}/{min_target} objetivo)\", end='')\n",
    "                if rejected_count > 0:\n",
    "                    print(\" | ‚ö†Ô∏è {rejected_count} rechazadas (no deseadas/grupos)\")\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "                # Limpiar carpetas temporales (solo las que NO son carpetas de razas conocidas)\n",
    "                # IMPORTANTE: Solo eliminar carpetas vac√≠as o que ya no contengan im√°genes\n",
    "                # Usar known_breeds ya definido arriba\n",
    "                for item in output_dir.iterdir():\n",
    "                    if item.is_dir():\n",
    "                        # Solo eliminar si NO es una carpeta de raza conocida\n",
    "                        if item.name not in known_breeds:\n",
    "                            try:\n",
    "                                # Verificar que la carpeta est√© vac√≠a o solo contenga archivos no deseados\n",
    "                                has_images = False\n",
    "                                for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "                                    if list(item.rglob(ext)):\n",
    "                                        has_images = True\n",
    "                                        break\n",
    "                                \n",
    "                                # Solo eliminar si no tiene im√°genes (ya fueron movidas o rechazadas)\n",
    "                                if not has_images:\n",
    "                                    shutil.rmtree(item)\n",
    "                            except Exception as e:\n",
    "                                # Si hay error, no eliminar (mejor prevenir que curar)\n",
    "                                pass\n",
    "                            \n",
    "            except Exception as e:\n",
    "                sys.stderr = original_stderr\n",
    "                sys.stdout = original_stdout\n",
    "                print(\"‚ö†Ô∏è Error: {str(e)[:50]}\")\n",
    "                continue\n",
    "    \n",
    "    # Contar im√°genes finales\n",
    "    print()\n",
    "    print(\"üìä Contando im√°genes finales...\")\n",
    "    final_imgs = []\n",
    "    for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "        for img in breed_dir.glob(ext):\n",
    "            if img.exists() and img.is_file() and img.stat().st_size > 0:\n",
    "                final_imgs.append(img)\n",
    "    \n",
    "    final_count = len(final_imgs)\n",
    "    \n",
    "    # Verificar si se alcanz√≥ el rango objetivo\n",
    "    if final_count >= min_target:\n",
    "        if final_count >= max_target:\n",
    "            print(\"‚úÖ Total de im√°genes v√°lidas: {final_count} (objetivo alcanzado: {min_target}-{max_target})\")\n",
    "        else:\n",
    "            print(\"‚úÖ Total de im√°genes v√°lidas: {final_count} (dentro del rango objetivo: {min_target}-{max_target})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Total de im√°genes v√°lidas: {final_count} (por debajo del objetivo m√≠nimo: {min_target})\")\n",
    "    \n",
    "    return final_count\n",
    "\n",
    "def download_breed(breed: str, images_limit: int, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Descarga im√°genes para una raza espec√≠fica.\n",
    "    \n",
    "    Args:\n",
    "        breed: Raza a descargar\n",
    "        images_limit: L√≠mite de im√°genes\n",
    "        output_dir: Directorio de salida\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con informaci√≥n de la descarga\n",
    "    \"\"\"\n",
    "    # Obtener t√©rminos de b√∫squeda y keywords de rechazo para la raza\n",
    "    search_terms = BREED_SEARCH_TERMS.get(breed, [])\n",
    "    rejection_keywords = REJECTION_KEYWORDS.get(breed, {\n",
    "        'artistic': [],\n",
    "        'other': []\n",
    "    })\n",
    "    \n",
    "    if not search_terms:\n",
    "        print(\"‚ùå No hay t√©rminos de b√∫squeda definidos para la raza: {breed}\")\n",
    "        return {'breed': breed, 'count': 0, 'size_mb': 0, 'success': False}\n",
    "    \n",
    "    # Mostrar configuraci√≥n por raza\n",
    "    print(\"\\n{'='*60}\")\n",
    "    print(\"üéØ Configuraci√≥n - {breed.upper()}\")\n",
    "    print(\"{'='*60}\")\n",
    "    print(\"   - Raza: {breed.upper()}\")\n",
    "    print(\"   - L√≠mite: {images_limit} im√°genes\")\n",
    "    print(\"   - T√©rminos de b√∫squeda: {len(search_terms)}\")\n",
    "    print()\n",
    "    \n",
    "    # Descargar im√°genes\n",
    "    count = download_images(breed, search_terms, output_dir, images_limit, rejection_keywords)\n",
    "    \n",
    "    print()\n",
    "    print(\"üîç Verificando im√°genes descargadas...\")\n",
    "    \n",
    "    # Verificar resultado final\n",
    "    breed_dir = output_dir / breed\n",
    "    final_imgs = []\n",
    "    total_size = 0\n",
    "    \n",
    "    if breed_dir.exists():\n",
    "        for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "            for img in breed_dir.glob(ext):\n",
    "                if img.exists() and img.is_file():\n",
    "                    try:\n",
    "                        file_size = img.stat().st_size\n",
    "                        if file_size > 0:\n",
    "                            final_imgs.append(img)\n",
    "                            total_size += file_size\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "    final_count = len(final_imgs)\n",
    "    size_mb = total_size / (1024 * 1024)\n",
    "    \n",
    "    # Calcular rango objetivo\n",
    "    min_target = int(images_limit * 0.9)\n",
    "    max_target = images_limit\n",
    "    \n",
    "    # Resumen por raza\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä RESUMEN - {breed.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üì∏ Im√°genes descargadas: {final_count}/{images_limit}\")\n",
    "    print(\"üéØ Rango objetivo: {min_target}-{max_target} im√°genes\")\n",
    "    print(\"üíæ Tama√±o total: {size_mb:.2f} MB\")\n",
    "    print(\"üìÅ Ubicaci√≥n: {breed_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if final_count >= max_target:\n",
    "        print(\"‚úÖ Descarga completada exitosamente (100% del objetivo)\")\n",
    "        success = True\n",
    "    elif final_count >= min_target:\n",
    "        print(\"‚úÖ Descarga completada dentro del rango objetivo ({final_count}/{images_limit} = {final_count/images_limit*100:.1f}%)\")\n",
    "        success = True\n",
    "    elif final_count > 0:\n",
    "        print(\"‚ö†Ô∏è Descarga parcial: {final_count}/{images_limit} im√°genes ({final_count/images_limit*100:.1f}% - por debajo del objetivo m√≠nimo)\")\n",
    "        success = False\n",
    "    else:\n",
    "        print(\"‚ùå No se descargaron im√°genes\")\n",
    "        success = False\n",
    "\n",
    "    # Generar/actualizar metadata estimada alineada con peso (weight_in_kg) y breed como en CID\n",
    "    build_metadata_for_breed(breed, breed_dir)\n",
    "    \n",
    "    return {\n",
    "        'breed': breed,\n",
    "        'count': final_count,\n",
    "        'size_mb': size_mb,\n",
    "        'success': success,\n",
    "        'target': images_limit,\n",
    "        'min_target': min_target\n",
    "    }\n",
    "\n",
    "def main(breed: str = None, images_limit: int = None):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal de descarga.\n",
    "    Si no se especifica una raza, descarga todas las razas definidas.\n",
    "    \n",
    "    Args:\n",
    "        breed: Raza a descargar (None = todas las razas)\n",
    "        images_limit: L√≠mite de im√°genes (usa IMAGES_LIMIT global si no se proporciona)\n",
    "    \"\"\"\n",
    "    if images_limit is None:\n",
    "        images_limit = IMAGES_LIMIT\n",
    "    \n",
    "    output_dir = SCRAPED_DIR\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"üìÅ Directorio de salida: {output_dir}\")\n",
    "    print()\n",
    "    \n",
    "    # Si se especifica una raza, descargar solo esa\n",
    "    if breed is not None:\n",
    "        result = download_breed(breed, images_limit, output_dir)\n",
    "        return result['count']\n",
    "    \n",
    "    # Si no se especifica raza, descargar todas las razas\n",
    "    all_breeds = list(BREED_SEARCH_TERMS.keys())\n",
    "    print(\"üöÄ Iniciando descarga para {len(all_breeds)} razas...\")\n",
    "    print(\"üìã Razas: {', '.join([b.upper() for b in all_breeds])}\")\n",
    "    print()\n",
    "    \n",
    "    results = []\n",
    "    for breed_name in all_breeds:\n",
    "        result = download_breed(breed_name, images_limit, output_dir)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Resumen general\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä RESUMEN GENERAL - BLOQUE 6\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_images = sum(r['count'] for r in results)\n",
    "    total_size = sum(r['size_mb'] for r in results)\n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    \n",
    "    print(\"‚úÖ Razas procesadas: {len(results)}\")\n",
    "    print(\"‚úÖ Razas exitosas: {successful}/{len(results)}\")\n",
    "    print(\"üì∏ Total de im√°genes: {total_images}\")\n",
    "    print(\"üíæ Tama√±o total: {total_size:.2f} MB\")\n",
    "    print()\n",
    "    print(\"üìä Detalle por raza:\")\n",
    "    for r in results:\n",
    "        status = \"‚úÖ\" if r['success'] else \"‚ö†Ô∏è\"\n",
    "        print(\"   {status} {r['breed'].upper()}: {r['count']}/{r['target']} im√°genes ({r['size_mb']:.2f} MB)\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    return total_images\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 7: DESCARGAR CID DATASET (COMPLEMENTARIO)\n",
    "# ============================================================\n",
    "# üì• Descarga el CID Dataset desde S3 para combinar con nuestro dataset\n",
    "# üí° CID: Cow Images Dataset para estimaci√≥n de peso y clasificaci√≥n de raza\n",
    "# üìä Fuente: https://github.com/bhuiyanmobasshir94/CID\n",
    "# ‚úÖ RECOMENDADO: Usar CID + nuestras im√°genes = mejor modelo\n",
    "# üéØ Estrategia: CID aporta diversidad y calidad, nuestras im√°genes aportan\n",
    "#    especificidad local (razas bolivianas, etapas de crianza, contexto real)\n",
    "# üí° Beneficios: M√°s datos = mejor generalizaci√≥n, transfer learning, validaci√≥n cruzada\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì• DESCARGANDO CID DATASET (COMPLEMENTARIO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üí° ESTRATEGIA DE DATASETS:\")\n",
    "print(\"   - CID Dataset: Diversidad y calidad (17,899+ im√°genes)\")\n",
    "print(\"   - Nuestras im√°genes: Especificidad local (razas bolivianas, etapas)\")\n",
    "print(\"   - Combinaci√≥n: Mejor generalizaci√≥n y precisi√≥n\")\n",
    "print()\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURACI√ìN: ¬øDescargar im√°genes de YouTube?\n",
    "# üíæ yt_images.tar.gz pesa ~3GB y contiene im√°genes adicionales\n",
    "# ‚úÖ RECOMENDADO: Descargar si tienes espacio (completa el dataset)\n",
    "# ‚ö†Ô∏è Las carpetas vac√≠as en 'images' pueden tener su contenido en 'yt_images'\n",
    "DOWNLOAD_YT_IMAGES = True  # Cambiar a False si no quieres descargar yt_images (~3GB)\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURACI√ìN: ¬øLimpiar carpetas vac√≠as despu√©s de descargar?\n",
    "# üóëÔ∏è Algunas carpetas en CID pueden estar vac√≠as (su contenido est√° en yt_images)\n",
    "# ‚úÖ RECOMENDADO: Limpiar para mantener el dataset organizado\n",
    "CLEAN_EMPTY_DIRS = True  # Cambiar a False si quieres mantener todas las carpetas\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "CID_DIR = RAW_DIR / 'cid'\n",
    "CID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# URLs del CID Dataset (desde S3)\n",
    "CID_URLS = {\n",
    "    'images': 'https://cid-21.s3.amazonaws.com/images.tar.gz',\n",
    "    'yt_images': 'https://cid-21.s3.amazonaws.com/yt_images.tar.gz',\n",
    "    'metadata': 'https://cid-21.s3.amazonaws.com/dataset.csv'\n",
    "}\n",
    "\n",
    "# Verificar si ya est√° descargado\n",
    "images_dir = CID_DIR / 'images'\n",
    "yt_images_dir = CID_DIR / 'yt_images'\n",
    "metadata_file = CID_DIR / 'dataset.csv'\n",
    "\n",
    "if images_dir.exists() and any(images_dir.iterdir()) and metadata_file.exists():\n",
    "    print(\"‚úÖ CID Dataset ya descargado en: {CID_DIR}\")\n",
    "    cid_content = CID_DIR\n",
    "    CID_METADATA_FILE = str(metadata_file)\n",
    "\n",
    "    # Contar im√°genes\n",
    "    img_count = len(list(images_dir.rglob('*.jpg'))) + len(list(images_dir.rglob('*.png')))\n",
    "    if yt_images_dir.exists() and any(yt_images_dir.iterdir()):\n",
    "        yt_count = len(list(yt_images_dir.rglob('*.jpg'))) + len(list(yt_images_dir.rglob('*.png')))\n",
    "        img_count += yt_count\n",
    "        print(\"üìä Total im√°genes: {img_count:,} (principales + YouTube)\")\n",
    "    else:\n",
    "        print(\"üìä Total im√°genes: {img_count:,} (solo principales)\")\n",
    "        if not DOWNLOAD_YT_IMAGES:\n",
    "            print(\"üí° yt_images no descargado (DOWNLOAD_YT_IMAGES = False)\")\n",
    "        else:\n",
    "            print(\"üí° Considera descargar yt_images para completar el dataset\")\n",
    "    \n",
    "    # La verificaci√≥n y limpieza se har√° al final, despu√©s de definir cid_content\n",
    "else:\n",
    "    print(\"üì• Descargando CID Dataset desde S3...\")\n",
    "    if DOWNLOAD_YT_IMAGES:\n",
    "        print(\"üíæ Tama√±o estimado: ~8GB (im√°genes principales + YouTube)\")\n",
    "        print(\"‚è±Ô∏è Puede tardar 15-30 minutos dependiendo de la conexi√≥n\")\n",
    "    else:\n",
    "        print(\"üíæ Tama√±o estimado: ~5GB (solo im√°genes principales)\")\n",
    "        print(\"‚è±Ô∏è Puede tardar 10-20 minutos dependiendo de la conexi√≥n\")\n",
    "        print(\"üí° yt_images (~3GB) omitido (DOWNLOAD_YT_IMAGES = False)\")\n",
    "    print()\n",
    "\n",
    "    # Descargar metadata CSV\n",
    "    print(\"üì• Descargando metadata CSV...\")\n",
    "    try:\n",
    "        subprocess.run(['wget', '-q', '--show-progress', CID_URLS['metadata'], '-O', str(metadata_file)], check=True)\n",
    "        print(\"‚úÖ Metadata descargado: {metadata_file}\")\n",
    "        CID_METADATA_FILE = str(metadata_file)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error descargando metadata: {e}\")\n",
    "        CID_METADATA_FILE = None\n",
    "\n",
    "    # Descargar im√°genes principales\n",
    "    # Proceso: 1) Descargar .tar.gz ‚Üí 2) Extraer ‚Üí 3) Eliminar .tar.gz (ahorra espacio)\n",
    "    images_tar = CID_DIR / 'images.tar.gz'\n",
    "    if not images_dir.exists() or not any(images_dir.iterdir()):\n",
    "        print(\"\\nüì• Descargando im√°genes principales...\")\n",
    "        try:\n",
    "            subprocess.run(['wget', '-q', '--show-progress', CID_URLS['images'], '-O', str(images_tar)], check=True)\n",
    "            print(\"üì¶ Extrayendo im√°genes...\")\n",
    "            subprocess.run(['tar', '-xzf', str(images_tar), '-C', str(CID_DIR)], check=True)\n",
    "            images_tar.unlink()  # Eliminar archivo comprimido despu√©s de extraer (ahorra espacio)\n",
    "            print(\"‚úÖ Im√°genes principales extra√≠das en: {images_dir}\")\n",
    "            print(\"   (archivo .tar.gz eliminado para ahorrar espacio)\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Error descargando im√°genes principales: {e}\")\n",
    "\n",
    "    # Descargar im√°genes de YouTube (opcional - ~3GB)\n",
    "    if DOWNLOAD_YT_IMAGES:\n",
    "        yt_tar = CID_DIR / 'yt_images.tar.gz'\n",
    "        if not yt_images_dir.exists() or not any(yt_images_dir.iterdir()):\n",
    "            print(\"\\nüì• Descargando im√°genes de YouTube (opcional - ~3GB)...\")\n",
    "            print(\"‚è±Ô∏è Esto puede tardar varios minutos...\")\n",
    "            try:\n",
    "                subprocess.run(['wget', '-q', '--show-progress', CID_URLS['yt_images'], '-O', str(yt_tar)], check=True)\n",
    "                print(\"üì¶ Extrayendo im√°genes de YouTube...\")\n",
    "                subprocess.run(['tar', '-xzf', str(yt_tar), '-C', str(CID_DIR)], check=True)\n",
    "                # Eliminar archivo comprimido despu√©s de extraer (ahorra ~3GB de espacio)\n",
    "                if yt_tar.exists():\n",
    "                    yt_tar.unlink()\n",
    "                    print(\"   (archivo .tar.gz eliminado para ahorrar ~3GB de espacio)\")\n",
    "                print(\"‚úÖ Im√°genes de YouTube extra√≠das en: {yt_images_dir}\")\n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è Error descargando im√°genes de YouTube: {e}\")\n",
    "        else:\n",
    "            print(\"‚úÖ Im√°genes de YouTube ya existen en: {yt_images_dir}\")\n",
    "            # Si ya existen pero el .tar.gz sigue ah√≠, eliminarlo para ahorrar espacio\n",
    "            yt_tar = CID_DIR / \"yt_images.tar.gz\"\n",
    "            if yt_tar.exists():\n",
    "                yt_tar.unlink()\n",
    "                print(\"   (archivo .tar.gz residual eliminado para ahorrar ~3GB de espacio)\")\n",
    "    else:\n",
    "        print(\"\\n‚è≠Ô∏è Omitiendo descarga de im√°genes de YouTube (DOWNLOAD_YT_IMAGES = False)\")\n",
    "        print(\"üí° Para descargarlas, cambia DOWNLOAD_YT_IMAGES a True (pesa ~3GB)\")\n",
    "\n",
    "    cid_content = CID_DIR\n",
    "    if CID_METADATA_FILE is None:\n",
    "        CID_METADATA_FILE = str(metadata_file) if metadata_file.exists() else None\n",
    "\n",
    "# Verificar y limpiar carpetas vac√≠as (opcional)\n",
    "def check_and_clean_empty_dirs(base_dir: Path, clean: bool = False):\n",
    "    \"\"\"\n",
    "    Verifica carpetas vac√≠as en el dataset CID.\n",
    "    Algunas carpetas pueden estar vac√≠as porque su contenido est√° en yt_images.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base del CID\n",
    "        clean: Si True, elimina carpetas vac√≠as. Si False, solo reporta.\n",
    "    \"\"\"\n",
    "    empty_dirs = []\n",
    "    total_dirs = 0\n",
    "    \n",
    "    # Verificar en images/\n",
    "    if (base_dir / 'images').exists():\n",
    "        for item in (base_dir / 'images').iterdir():\n",
    "            if item.is_dir():\n",
    "                total_dirs += 1\n",
    "                # Verificar si est√° vac√≠o (sin archivos de imagen)\n",
    "                has_images = any(item.rglob('*.jpg')) or any(item.rglob('*.png')) or any(item.rglob('*.jpeg'))\n",
    "                if not has_images:\n",
    "                    empty_dirs.append(item)\n",
    "    \n",
    "    # Verificar tambi√©n en yt_images/\n",
    "    if (base_dir / \"yt_images\").exists():\n",
    "        for item in (base_dir / \"yt_images\").iterdir():\n",
    "            if item.is_dir():\n",
    "                total_dirs += 1\n",
    "                # Verificar si est√° vac√≠o (sin archivos de imagen)\n",
    "                has_images = any(item.rglob(\"*.jpg\")) or any(item.rglob(\"*.png\")) or any(item.rglob(\"*.jpeg\"))\n",
    "                if not has_images:\n",
    "                    empty_dirs.append(item)\n",
    "                    empty_dirs.append(item)\n",
    "    \n",
    "    if empty_dirs:\n",
    "        print(\"\\nüîç Verificaci√≥n de carpetas:\")\n",
    "        print(\"   üìÅ Total de carpetas: {total_dirs}\")\n",
    "        print(\"   ‚ö†Ô∏è Carpetas vac√≠as encontradas: {len(empty_dirs)}\")\n",
    "        print(\"   üí° Nota: El contenido puede estar en 'yt_images' si est√° descargado\")\n",
    "        \n",
    "        if clean:\n",
    "            print(\"\\nüóëÔ∏è Limpiando carpetas vac√≠as...\")\n",
    "            for empty_dir in empty_dirs:\n",
    "                try:\n",
    "                    empty_dir.rmdir()\n",
    "                    print(\"   ‚úÖ Eliminada: {empty_dir.name}\")\n",
    "                except Exception as e:\n",
    "                    print(\"   ‚ö†Ô∏è No se pudo eliminar {empty_dir.name}: {e}\")\n",
    "            print(\"‚úÖ Limpieza completada\")\n",
    "        else:\n",
    "            print(\"   üí° Para limpiarlas autom√°ticamente, cambia CLEAN_EMPTY_DIRS a True\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No se encontraron carpetas vac√≠as\")\n",
    "\n",
    "# Verificar y limpiar carpetas vac√≠as si est√° configurado\n",
    "# Asegurar que cid_content est√© definido\n",
    "if \"cid_content\" not in locals():\n",
    "    cid_content = CID_DIR\n",
    "\n",
    "if cid_content.exists():\n",
    "    check_and_clean_empty_dirs(cid_content, clean=CLEAN_EMPTY_DIRS)\n",
    "\n",
    "# Calcular espacio usado\n",
    "def get_dir_size(path: Path) -> int:\n",
    "    \"\"\"Calcula el tama√±o total de un directorio en bytes.\"\"\"\n",
    "    total = 0\n",
    "    try:\n",
    "        for entry in path.rglob('*'):\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "    except Exception:\n",
    "        pass\n",
    "    return total\n",
    "\n",
    "if cid_content.exists():\n",
    "    total_size = get_dir_size(cid_content)\n",
    "    size_gb = total_size / (1024 ** 3)\n",
    "    print(\"\\nüíæ Espacio usado por CID Dataset: {size_gb:.2f} GB\")\n",
    "    if size_gb < 2.0:\n",
    "        print(\"   üí° Tienes espacio disponible. Considera descargar yt_images si no lo has hecho.\")\n",
    "\n",
    "# Configurar variables de entorno\n",
    "if cid_content.exists():\n",
    "    os.environ['CID_DATASET_PATH'] = str(cid_content)\n",
    "    if CID_METADATA_FILE:\n",
    "        os.environ['CID_METADATA_FILE'] = CID_METADATA_FILE\n",
    "\n",
    "    print(\"\\nüìã Variables de entorno configuradas:\")\n",
    "    print(\"   CID_DATASET_PATH = {cid_content}\")\n",
    "    if CID_METADATA_FILE:\n",
    "        print(\"   CID_METADATA_FILE = {CID_METADATA_FILE}\")\n",
    "\n",
    "    print(\"\\nüí° Pr√≥ximos pasos:\")\n",
    "    print(\"   - BLOQUE 6: Descargar nuestras propias im√°genes (complementar con CID)\")\n",
    "    print(\"   - BLOQUE 8: Preparar dataset combinado (CID + nuestras im√°genes)\")\n",
    "    print(\"   - BLOQUE 9: Resumen de datasets disponibles\")\n",
    "    print(\"\\nüìä ESTRATEGIA DE ENTRENAMIENTO:\")\n",
    "    print(\"   - Usar CID para pre-entrenamiento o como datos adicionales\")\n",
    "    print(\"   - Nuestras im√°genes para fine-tuning y validaci√≥n espec√≠fica\")\n",
    "    print(\"   - Combinar ambos para mejor generalizaci√≥n del modelo\")\n",
    "\n",
    "print(\"\\n{'=' * 60}\")\n",
    "print(\"‚úÖ BLOQUE 7 COMPLETADO\")\n",
    "print(\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 8: PREPARAR DATASET COMBINADO (ESTRATEGIA B)\n",
    "# ============================================================\n",
    "# üîó Combina CID Dataset + Nuestras im√°genes para entrenamiento\n",
    "# üéØ Estrategia B: Combinaci√≥n desde el inicio para mejor modelo\n",
    "# üí° CID aporta diversidad (~17,899 im√°genes) + Nuestras im√°genes aportan especificidad local\n",
    "# üìä Resultado: Dataset combinado listo para pipeline de entrenamiento\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîó PREPARANDO DATASET COMBINADO (ESTRATEGIA B)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üí° ESTRATEGIA DE COMBINACI√ìN:\")\n",
    "print(\"   - CID Dataset: Diversidad y calidad (~17,899 im√°genes)\")\n",
    "print(\"   - Nuestras im√°genes: Especificidad local (razas bolivianas, etapas)\")\n",
    "print(\"   - Combinaci√≥n: Mejor generalizaci√≥n y precisi√≥n\")\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "# Funci√≥n para normalizar un dataset individual\n",
    "def normalize_dataset(df, dataset_name, base_dir):\n",
    "    \"\"\"Normaliza un dataset para que tenga: 'image_filename', 'weight_kg', 'breed'\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüîç Analizando {dataset_name}...\")\n",
    "    print(\"   Columnas disponibles: {list(df.columns)}\")\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # 1. Buscar columna de imagen\n",
    "    image_col = None\n",
    "    for col in ['image_filename', 'image_path', 'file_path', 'path', 'filename', 'image', 'file']:\n",
    "        if col in df_normalized.columns:\n",
    "            image_col = col\n",
    "            break\n",
    "    \n",
    "    if not image_col and dataset_name == 'CID Dataset':\n",
    "        # Para CID, crear desde estructura de carpetas\n",
    "        print(\"   ‚ö†Ô∏è Creando rutas desde estructura de carpetas...\")\n",
    "        cid_images_dir = base_dir / 'cid' / 'images'\n",
    "        cid_yt_dir = base_dir / 'cid' / 'yt_images'\n",
    "        \n",
    "        # Buscar columna de raza\n",
    "        breed_col = None\n",
    "        for col in ['breed', 'Breed', 'breed_type', 'class', 'label']:\n",
    "            if col in df_normalized.columns:\n",
    "                breed_col = col\n",
    "                break\n",
    "        \n",
    "        if breed_col:\n",
    "            image_paths = []\n",
    "            for idx, row in df_normalized.iterrows():\n",
    "                breed_name = str(row[breed_col]).lower().strip()\n",
    "                found = False\n",
    "                for img_dir in [cid_images_dir, cid_yt_dir]:\n",
    "                    if img_dir.exists() and breed_name:\n",
    "                        breed_dir = img_dir / breed_name\n",
    "                        if breed_dir.exists():\n",
    "                            for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "                                imgs = list(breed_dir.glob(ext))\n",
    "                                if imgs:\n",
    "                                    image_paths.append(str(imgs[0].relative_to(base_dir)))\n",
    "                                    found = True\n",
    "                                    break\n",
    "                        if found:\n",
    "                            break\n",
    "                if not found:\n",
    "                    image_paths.append(None)\n",
    "            df_normalized['image_filename'] = image_paths\n",
    "            print(\"   ‚úÖ Im√°genes creadas desde estructura\")\n",
    "        else:\n",
    "            raise ValueError(f\"CID: falta columna de raza. Columnas: {list(df.columns)}\")\n",
    "    elif image_col:\n",
    "        df_normalized['image_filename'] = df_normalized[image_col]\n",
    "        print(\"   ‚úÖ Imagen: {image_col}\")\n",
    "    else:\n",
    "        raise ValueError(f\"No se encontr√≥ columna de imagen. Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # 2. Buscar columna de peso\n",
    "    weight_col = None\n",
    "    for col in ['weight_kg', 'weight_in_kg', 'weight', 'Weight', 'peso', 'Peso', 'Weight_kg']:\n",
    "        if col in df_normalized.columns:\n",
    "            weight_col = col\n",
    "            break\n",
    "    \n",
    "    if weight_col:\n",
    "        df_normalized['weight_kg'] = pd.to_numeric(df_normalized[weight_col], errors='coerce')\n",
    "        print(\"   ‚úÖ Peso: {weight_col} ‚Üí weight_kg\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No se encontr√≥ peso\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Buscar columna de raza\n",
    "    breed_col = None\n",
    "    for col in ['breed', 'Breed', 'breed_type', 'class', 'label', 'raza']:\n",
    "        if col in df_normalized.columns:\n",
    "            breed_col = col\n",
    "            break\n",
    "    \n",
    "    if breed_col:\n",
    "        df_normalized['breed'] = df_normalized[breed_col].astype(str).str.lower().str.strip()\n",
    "        print(\"   ‚úÖ Raza: {breed_col}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No se encontr√≥ raza\")\n",
    "        return None\n",
    "    \n",
    "    # Filtrar valores faltantes\n",
    "    initial = len(df_normalized)\n",
    "    df_normalized = df_normalized.dropna(subset=['image_filename', 'weight_kg', 'breed'])\n",
    "    if initial != len(df_normalized):\n",
    "        print(\"   ‚ö†Ô∏è Eliminadas {initial - len(df_normalized)} filas con valores faltantes\")\n",
    "    \n",
    "    print(\"   ‚úÖ Normalizado: {len(df_normalized):,} registros v√°lidos\")\n",
    "    return df_normalized[['image_filename', 'weight_kg', 'breed']]\n",
    "\n",
    "\n",
    "def prepare_cid_dataset() -> tuple[Path | None, int]:\n",
    "    \"\"\"Prepara CID Dataset.\"\"\"\n",
    "    cid_dir = RAW_DIR / 'cid'\n",
    "    \n",
    "    if not cid_dir.exists():\n",
    "        return None, 0\n",
    "    \n",
    "    # Contar im√°genes en CID\n",
    "    cid_images_dir = cid_dir / 'images'\n",
    "    cid_yt_dir = cid_dir / 'yt_images'\n",
    "    \n",
    "    cid_count = 0\n",
    "    if cid_images_dir.exists():\n",
    "        cid_count += len(list(cid_images_dir.rglob('*.jpg'))) + len(list(cid_images_dir.rglob('*.png')))\n",
    "    if cid_yt_dir.exists():\n",
    "        cid_count += len(list(cid_yt_dir.rglob('*.jpg'))) + len(list(cid_yt_dir.rglob('*.png')))\n",
    "    \n",
    "    if cid_count == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    return cid_dir, cid_count\n",
    "\n",
    "\n",
    "def prepare_local_dataset() -> tuple[Path | None, int]:\n",
    "    \"\"\"Prepara dataset desde im√°genes locales.\"\"\"\n",
    "    local_dir = RAW_DIR / 'local_images'\n",
    "\n",
    "    if not local_dir.exists():\n",
    "        return None, 0\n",
    "\n",
    "    img_files = (\n",
    "        list(local_dir.rglob('*.jpg')) +\n",
    "        list(local_dir.rglob('*.png')) +\n",
    "        list(local_dir.rglob('*.jpeg'))\n",
    "    )\n",
    "\n",
    "    if not img_files:\n",
    "        return None, 0\n",
    "\n",
    "    return local_dir, len(img_files)\n",
    "\n",
    "\n",
    "def prepare_scraped_dataset() -> tuple[Path | None, int]:\n",
    "    \"\"\"Prepara dataset desde im√°genes scrapeadas.\"\"\"\n",
    "    scraped_dir = RAW_DIR / 'scraped'\n",
    "    \n",
    "    if not scraped_dir.exists():\n",
    "        return None, 0\n",
    "    \n",
    "    img_files = (\n",
    "        list(scraped_dir.rglob('*.jpg')) +\n",
    "        list(scraped_dir.rglob('*.png')) +\n",
    "        list(scraped_dir.rglob('*.jpeg'))\n",
    "    )\n",
    "    \n",
    "    if not img_files:\n",
    "        return None, 0\n",
    "    \n",
    "    return scraped_dir, len(img_files)\n",
    "\n",
    "\n",
    "def create_combined_dataset():\n",
    "    \"\"\"Crea dataset combinado: CID + Nuestras im√°genes (Estrategia B).\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç VERIFICANDO DATASETS DISPONIBLES\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    # 1. Verificar CID Dataset\n",
    "    cid_path, cid_count = prepare_cid_dataset()\n",
    "    \n",
    "    # 2. Verificar nuestras im√°genes\n",
    "    local_path, local_count = prepare_local_dataset()\n",
    "    scraped_path, scraped_count = prepare_scraped_dataset()\n",
    "    \n",
    "    our_total = local_count + scraped_count\n",
    "    \n",
    "    # Resumen de datasets\n",
    "    print(\"üìä RESUMEN DE DATASETS:\")\n",
    "    print(\"   - CID Dataset: {cid_count:,} im√°genes {'‚úÖ' if cid_path else '‚ùå'}\")\n",
    "    print(\"   - Im√°genes locales: {local_count:,} im√°genes {'‚úÖ' if local_path else '‚ùå'}\")\n",
    "    print(\"   - Im√°genes scrapeadas: {scraped_count:,} im√°genes {'‚úÖ' if scraped_path else '‚ùå'}\")\n",
    "    print(\"   - Total nuestras im√°genes: {our_total:,} im√°genes\")\n",
    "    \n",
    "    total_images = cid_count + our_total\n",
    "    print(\"\\nüìä TOTAL COMBINADO: {total_images:,} im√°genes\")\n",
    "    \n",
    "    # Estrategia seg√∫n disponibilidad\n",
    "    if cid_path and our_total > 0:\n",
    "        print(\"\\n‚úÖ ESTRATEGIA B ACTIVADA: Combinaci√≥n desde el inicio\")\n",
    "        print(\"   - CID: {cid_count:,} im√°genes (diversidad)\")\n",
    "        print(\"   - Nuestras: {our_total:,} im√°genes (especificidad)\")\n",
    "        print(\"   - Total: {total_images:,} im√°genes para entrenamiento\")\n",
    "        \n",
    "        # Crear estructura combinada\n",
    "        combined_dir = RAW_DIR / 'combined'\n",
    "        combined_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Guardar metadata de combinaci√≥n\n",
    "        metadata_info = {\n",
    "            'cid_available': cid_path is not None,\n",
    "            'cid_count': cid_count,\n",
    "            'local_available': local_path is not None,\n",
    "            'local_count': local_count,\n",
    "            'scraped_available': scraped_path is not None,\n",
    "            'scraped_count': scraped_count,\n",
    "            'total_our_images': our_total,\n",
    "            'total_combined': total_images,\n",
    "            'strategy': 'B - Combination from start'\n",
    "        }\n",
    "        \n",
    "        with open(combined_dir / 'dataset_info.json', 'w') as f:\n",
    "            json.dump(metadata_info, f, indent=2)\n",
    "        \n",
    "        print(\"\\n‚úÖ Estructura combinada creada en: {combined_dir}\")\n",
    "        return True\n",
    "        \n",
    "    elif cid_path and our_total == 0:\n",
    "        print(\"\\n‚ö†Ô∏è Solo CID disponible ({cid_count:,} im√°genes)\")\n",
    "        print(\"üí° Recomendaci√≥n: Descarga nuestras im√°genes para mejor modelo\")\n",
    "        return True\n",
    "        \n",
    "    elif not cid_path and our_total > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Solo nuestras im√°genes disponibles ({our_total:,} im√°genes)\")\n",
    "        print(\"üí° Recomendaci√≥n: Descarga CID Dataset para mejor modelo\")\n",
    "        print(\"üí° Puedes entrenar solo con nuestras im√°genes, pero CID mejorar√≠a el modelo\")\n",
    "        return True\n",
    "    \n",
    "    # Si no hay datasets\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è NO SE ENCONTRARON DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"üí° PASOS RECOMENDADOS:\")\n",
    "    print(\"   1. Descarga CID Dataset (~17,899 im√°genes)\")\n",
    "    print(\"   2. Descarga nuestras im√°genes (200+ por raza)\")\n",
    "    print(\"   3. Vuelve a ejecutar este bloque para combinar ambos\")\n",
    "    print()\n",
    "    \n",
    "    print(\"\\nüìã ESTRUCTURA M√çNIMA PARA ENTRENAMIENTO:\")\n",
    "    print(\"   - M√≠nimo viable: 100 im√°genes por raza (700 total)\")\n",
    "    print(\"   - Recomendado: 150 im√°genes por raza (1050 total)\")\n",
    "    print(\"   - Ideal: 200+ im√°genes por raza (1400+ total)\")\n",
    "    print(\"   - Con CID: {1400 + 17899:,} im√°genes totales (MEJOR MODELO)\")\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    success = create_combined_dataset()\n",
    "\n",
    "    if success:\n",
    "        print(\"\\n‚úÖ BLOQUE 6 COMPLETADO - Dataset combinado preparado\")\n",
    "        print(\"üí° Estrategia B: CID + Nuestras im√°genes listas para entrenamiento\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è BLOQUE 6 COMPLETADO - Estructura demo creada\")\n",
    "        print(\"üí° Descarga CID Dataset y nuestras im√°genes primero\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n{'=' * 60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 9: RESUMEN DE DATASETS\n",
    "# ============================================================\n",
    "# üìä Muestra resumen de datasets disponibles (CID + nuestras im√°genes)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "if 'DATA_DIR' not in globals():\n",
    "    DATA_DIR = RAW_DIR.parent / 'processed'\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def summarize_datasets():\n",
    "    \"\"\"Resumen de datasets disponibles.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä RESUMEN DE DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    datasets_info = []\n",
    "\n",
    "    # CID Dataset\n",
    "    cid_dir = RAW_DIR / 'cid'\n",
    "    cid_count = 0\n",
    "    if cid_dir.exists():\n",
    "        cid_images_dir = cid_dir / 'images'\n",
    "        cid_yt_dir = cid_dir / 'yt_images'\n",
    "        if cid_images_dir.exists():\n",
    "            cid_count += len(list(cid_images_dir.rglob('*.jpg')) + list(cid_images_dir.rglob('*.png')))\n",
    "        if cid_yt_dir.exists():\n",
    "            cid_count += len(list(cid_yt_dir.rglob('*.jpg')) + list(cid_yt_dir.rglob('*.png')))\n",
    "    \n",
    "    datasets_info.append({\n",
    "        'name': 'CID Dataset',\n",
    "        'images': cid_count,\n",
    "        'description': 'Diversidad y calidad',\n",
    "        'status': '‚úÖ Disponible' if cid_count > 0 else '‚ùå No disponible'\n",
    "    })\n",
    "\n",
    "    # Nuestras im√°genes (scraped)\n",
    "    scraped_dir = RAW_DIR / 'scraped'\n",
    "    scraped_count = 0\n",
    "    if scraped_dir.exists():\n",
    "        scraped_count = len(list(scraped_dir.rglob('*.jpg')) + list(scraped_dir.rglob('*.png')))\n",
    "    \n",
    "    datasets_info.append({\n",
    "        'name': 'Nuestras Im√°genes',\n",
    "        'images': scraped_count,\n",
    "        'description': 'Especificidad local (razas bolivianas)',\n",
    "        'status': '‚úÖ Disponible' if scraped_count > 0 else '‚ùå No disponible'\n",
    "    })\n",
    "\n",
    "    # Im√°genes locales\n",
    "    local_dir = RAW_DIR / 'local_images'\n",
    "    local_count = 0\n",
    "    if local_dir.exists():\n",
    "        local_count = len(list(local_dir.rglob('*.jpg')) + list(local_dir.rglob('*.png')) + list(local_dir.rglob('*.jpeg')))\n",
    "    \n",
    "    if local_count > 0:\n",
    "        datasets_info.append({\n",
    "            'name': 'Im√°genes Locales',\n",
    "            'images': local_count,\n",
    "            'description': 'Fotos manuales o descargadas',\n",
    "            'status': '‚úÖ Disponible'\n",
    "        })\n",
    "\n",
    "    # Mostrar resumen\n",
    "    df_datasets = pd.DataFrame(datasets_info)\n",
    "    print(df_datasets.to_string(index=False))\n",
    "    \n",
    "    total_images = int(df_datasets['images'].sum())\n",
    "    print(\"\\nüéØ TOTAL: {total_images:,} im√°genes\")\n",
    "    \n",
    "    # Guardar resumen\n",
    "    summary_path = DATA_DIR / 'datasets_summary.csv'\n",
    "    df_datasets.to_csv(summary_path, index=False)\n",
    "    print(\"üíæ Resumen guardado: {summary_path}\")\n",
    "    \n",
    "    return df_datasets\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    datasets_summary = summarize_datasets()\n",
    "    print(\"\\n‚úÖ BLOQUE 9 COMPLETADO\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä D√≠a 4: An√°lisis Exploratorio de Datos (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 10: VERIFICACI√ìN R√ÅPIDA DE DATOS (OPCIONAL)\n",
    "# ============================================================\n",
    "# ‚úÖ Verificaci√≥n m√≠nima: columnas requeridas para entrenamiento\n",
    "# ‚ö†Ô∏è OPCIONAL: Puedes saltar este bloque para entrenar m√°s r√°pido\n",
    "# üí° Solo verifica que los datos tienen peso y raza (sin gr√°ficos)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ VERIFICACI√ìN R√ÅPIDA DE DATOS (OPCIONAL)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "\n",
    "def quick_verification():\n",
    "    \"\"\"Verificaci√≥n m√≠nima: solo comprobar que existen datos con columnas necesarias.\"\"\"\n",
    "    # 1. Intentar CID Dataset\n",
    "    cid_dir = RAW_DIR / 'cid'\n",
    "    if cid_dir.exists():\n",
    "        metadata_files = list(cid_dir.rglob('*.csv'))\n",
    "        if metadata_files:\n",
    "            try:\n",
    "                df = pd.read_csv(metadata_files[0])\n",
    "                print(\"‚úÖ CID Dataset: {len(df):,} registros\")\n",
    "                print(\"üìã Columnas: {list(df.columns)}\")\n",
    "                \n",
    "                # Verificar columnas necesarias\n",
    "                has_weight = any('weight' in col.lower() or 'peso' in col.lower() for col in df.columns)\n",
    "                has_breed = any('breed' in col.lower() or 'raza' in col.lower() for col in df.columns)\n",
    "                \n",
    "                if has_weight and has_breed:\n",
    "                    print(\"‚úÖ Datos listos para entrenamiento\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Faltan columnas: peso o raza\")\n",
    "                return True\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 2. Intentar metadata scraped\n",
    "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
    "    if scraped_metadata.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(scraped_metadata)\n",
    "            print(\"‚úÖ Dataset Scraped: {len(df):,} registros\")\n",
    "            print(\"üìã Columnas: {list(df.columns)}\")\n",
    "            \n",
    "            has_weight = 'weight_kg' in df.columns\n",
    "            has_breed = 'breed' in df.columns\n",
    "            \n",
    "            if has_weight and has_breed:\n",
    "                print(\"‚úÖ Datos listos para entrenamiento\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Faltan columnas: weight_kg o breed\")\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ metadata\")\n",
    "    print(\"üí° El pipeline puede funcionar solo con im√°genes en carpetas\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    verified = quick_verification()\n",
    "    if verified:\n",
    "        print(\"\\n‚úÖ BLOQUE 10 COMPLETADO - Datos verificados\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ BLOQUE 10 COMPLETADO - Sin metadata (puedes continuar)\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ö†Ô∏è Error en verificaci√≥n: {e}\")\n",
    "    print(\"üí° Puedes continuar con el entrenamiento\")\n",
    "\n",
    "print(\"\\n{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß D√≠a 5-6: Preparar Pipeline de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 11: PIPELINE DE DATOS OPTIMIZADO (ESTRATEGIA B)\n",
    "# ============================================================\n",
    "# üîß Crea pipeline de datos usando m√≥dulos del proyecto\n",
    "# üéØ ESTRATEGIA B: Combina CID Dataset + Nuestras im√°genes desde el inicio\n",
    "# üí° Usa: CattleDataGenerator (data.data_loader) y get_aggressive_augmentation (data.augmentation)\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 8 ejecutado (dataset combinado preparado)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURAR PYTHONPATH ANTES DE IMPORTAR M√ìDULOS\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURAR PYTHONPATH ANTES DE IMPORTAR M√ìDULOS DEL PROYECTO\n",
    "import sys\n",
    "\n",
    "# Verificar que BASE_DIR est√° definido\n",
    "if 'BASE_DIR' not in globals():\n",
    "    # Intentar definir BASE_DIR desde diferentes ubicaciones\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
    "    if not BASE_DIR.exists():\n",
    "        BASE_DIR = Path('/content/bovine-weight-estimation')\n",
    "\n",
    "# Configurar PYTHONPATH para importar m√≥dulos del proyecto\n",
    "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
    "src_dir = ML_TRAINING_DIR / 'src'\n",
    "\n",
    "if src_dir.exists():\n",
    "    if str(src_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(src_dir))\n",
    "        print(\"‚úÖ PYTHONPATH configurado: {src_dir}\")\n",
    "    else:\n",
    "        print(\"‚úÖ PYTHONPATH ya configurado: {src_dir}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Directorio src no encontrado: {src_dir}\")\n",
    "    print(\"üí° Aseg√∫rate de haber ejecutado el BLOQUE 1 para clonar el repositorio\")\n",
    "    raise FileNotFoundError(f\"Directorio src no encontrado: {src_dir}\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Verificar que BASE_DIR est√° definido\n",
    "if 'BASE_DIR' not in globals():\n",
    "    # Intentar definir BASE_DIR desde diferentes ubicaciones\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
    "    if not BASE_DIR.exists():\n",
    "        BASE_DIR = Path('/content/bovine-weight-estimation')\n",
    "\n",
    "# Configurar PYTHONPATH para importar m√≥dulos del proyecto\n",
    "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
    "src_dir = ML_TRAINING_DIR / 'src'\n",
    "\n",
    "if src_dir.exists():\n",
    "    if str(src_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(src_dir))\n",
    "        print(\"‚úÖ PYTHONPATH configurado: {src_dir}\")\n",
    "    else:\n",
    "        print(\"‚úÖ PYTHONPATH ya configurado: {src_dir}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Directorio src no encontrado: {src_dir}\")\n",
    "    print(\"üí° Aseg√∫rate de haber ejecutado el BLOQUE 1 para clonar el repositorio\")\n",
    "    raise FileNotFoundError(f\"Directorio src no encontrado: {src_dir}\")\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados\n",
    "try:\n",
    "    from data.data_loader import CattleDataGenerator\n",
    "    from data.augmentation import get_aggressive_augmentation, get_validation_transform\n",
    "    print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Error importando m√≥dulos del proyecto: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el proyecto est√© clonado correctamente\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß PIPELINE DE DATOS OPTIMIZADO (ESTRATEGIA B)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üí° ESTRATEGIA B: Combinando CID Dataset + Nuestras im√°genes\")\n",
    "print()\n",
    "\n",
    "# Verificar que RAW_DIR est√° definido\n",
    "if 'RAW_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "    else:\n",
    "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
    "\n",
    "# Funci√≥n para normalizar un dataset individual\n",
    "def normalize_dataset(df, dataset_name, base_dir):\n",
    "    \"\"\"Normaliza un dataset para que tenga: 'image_filename', 'weight_kg', 'breed'\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüîç Analizando {dataset_name}...\")\n",
    "    print(\"   Columnas disponibles: {list(df.columns)}\")\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # 1. Buscar columna de imagen\n",
    "    image_col = None\n",
    "    for col in ['image_filename', 'image_path', 'file_path', 'path', 'filename', 'image', 'file']:\n",
    "        if col in df_normalized.columns:\n",
    "            image_col = col\n",
    "            break\n",
    "    \n",
    "    if not image_col and dataset_name == 'CID Dataset':\n",
    "        # Para CID, mapear usando SKU (c√≥digo BLF) desde estructura de carpetas\n",
    "        print(\"   ‚ö†Ô∏è Creando rutas desde estructura de carpetas CID (BLF)...\")\n",
    "        cid_images_dir = base_dir / 'cid' / 'images'\n",
    "        cid_yt_dir = base_dir / 'cid' / 'yt_images'\n",
    "        \n",
    "        # Buscar columna SKU (c√≥digo BLF como \"BLF2001\", \"BLF2002\", etc.)\n",
    "        sku_col = None\n",
    "        for col in ['sku', 'SKU', 'id', 'ID', 'code', 'Code']:\n",
    "            if col in df_normalized.columns:\n",
    "                sku_col = col\n",
    "                break\n",
    "        \n",
    "        if not sku_col:\n",
    "            print(\"   ‚ö†Ô∏è No se encontr√≥ columna SKU. Intentando mapear por √≠ndice...\")\n",
    "            # Si no hay SKU, usar el √≠ndice como referencia\n",
    "            sku_col = None\n",
    "        \n",
    "        image_paths = []\n",
    "        images_found = 0\n",
    "        images_not_found = 0\n",
    "        \n",
    "        for idx, row in df_normalized.iterrows():\n",
    "            found = False\n",
    "            image_path = None\n",
    "            \n",
    "            # Intentar mapear usando SKU (c√≥digo BLF)\n",
    "            if sku_col:\n",
    "                sku_value = str(row[sku_col]).strip()\n",
    "                # Limpiar SKU: puede ser \"BLF2001\" o \"BLF 2001\" o solo \"2001\"\n",
    "                if 'BLF' in sku_value.upper():\n",
    "                    blf_code = sku_value.upper().replace('BLF', '').strip()\n",
    "                    if blf_code:\n",
    "                        folder_name = f\"BLF{blf_code}\"\n",
    "                    else:\n",
    "                        folder_name = sku_value.upper()\n",
    "                elif sku_value.isdigit():\n",
    "                    folder_name = f\"BLF{sku_value}\"\n",
    "                else:\n",
    "                    folder_name = sku_value\n",
    "            \n",
    "            # Si no hay SKU, intentar usar el √≠ndice (para mapeo secuencial)\n",
    "            else:\n",
    "                # Intentar mapear por posici√≥n (asumiendo orden secuencial)\n",
    "                folder_name = f\"BLF{2000 + idx + 1}\"  # BLF2001, BLF2002, etc.\n",
    "            \n",
    "            # Buscar im√°genes en ambas carpetas (images y yt_images)\n",
    "            for img_dir in [cid_images_dir, cid_yt_dir]:\n",
    "                if not img_dir.exists():\n",
    "                    continue\n",
    "                \n",
    "                folder_path = img_dir / folder_name\n",
    "                if not folder_path.exists():\n",
    "                    # Intentar variaciones del nombre\n",
    "                    for variant in [folder_name, folder_name.replace('BLF', 'BLF '), folder_name.replace(' ', '')]:\n",
    "                        variant_path = img_dir / variant\n",
    "                        if variant_path.exists():\n",
    "                            folder_path = variant_path\n",
    "                            break\n",
    "                \n",
    "                if folder_path.exists() and folder_path.is_dir():\n",
    "                    # Buscar im√°genes v√°lidas (excluir las que empiezan con _)\n",
    "                    for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "                        all_imgs = list(folder_path.glob(ext))\n",
    "                        # Filtrar im√°genes que empiezan con _ (duplicadas/corruptas)\n",
    "                        valid_imgs = [img for img in all_imgs if not img.name.startswith('_')]\n",
    "                        \n",
    "                        if valid_imgs:\n",
    "                            # Usar la primera imagen v√°lida encontrada\n",
    "                            image_path = valid_imgs[0].relative_to(base_dir)\n",
    "                            found = True\n",
    "                            images_found += 1\n",
    "                            break\n",
    "                \n",
    "                if found:\n",
    "                    break\n",
    "            \n",
    "            if found:\n",
    "                image_paths.append(str(image_path))\n",
    "            else:\n",
    "                image_paths.append(None)\n",
    "                images_not_found += 1\n",
    "        \n",
    "        df_normalized['image_filename'] = image_paths\n",
    "        \n",
    "        print(\"   ‚úÖ Im√°genes mapeadas: {images_found} encontradas, {images_not_found} no encontradas\")\n",
    "        print(\"   üí° Usando estructura BLF (carpetas: BLF2001, BLF2002, etc.)\")\n",
    "        print(\"   üí° Filtrando im√°genes duplicadas (que empiezan con _)\")\n",
    "        \n",
    "        if images_not_found > len(df_normalized) * 0.5:\n",
    "            print(\"   ‚ö†Ô∏è ADVERTENCIA: M√°s del 50% de im√°genes no encontradas\")\n",
    "            print(\"   üí° Verifica que las carpetas BLF coincidan con los SKU del metadata\")\n",
    "    elif image_col:\n",
    "        df_normalized['image_filename'] = df_normalized[image_col]\n",
    "        print(\"   ‚úÖ Imagen: {image_col}\")\n",
    "    else:\n",
    "        raise ValueError(f\"No se encontr√≥ columna de imagen. Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # 2. Buscar columna de peso\n",
    "    weight_col = None\n",
    "    for col in ['weight_kg', 'weight_in_kg', 'weight', 'Weight', 'peso', 'Peso', 'Weight_kg']:\n",
    "        if col in df_normalized.columns:\n",
    "            weight_col = col\n",
    "            break\n",
    "    \n",
    "    if weight_col:\n",
    "        df_normalized['weight_kg'] = pd.to_numeric(df_normalized[weight_col], errors='coerce')\n",
    "        print(\"   ‚úÖ Peso: {weight_col}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No se encontr√≥ peso\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Buscar columna de raza\n",
    "    breed_col = None\n",
    "    for col in ['breed', 'Breed', 'breed_type', 'class', 'label', 'raza']:\n",
    "        if col in df_normalized.columns:\n",
    "            breed_col = col\n",
    "            break\n",
    "    \n",
    "    if breed_col:\n",
    "        df_normalized['breed'] = df_normalized[breed_col].astype(str).str.lower().str.strip()\n",
    "        print(\"   ‚úÖ Raza: {breed_col}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No se encontr√≥ raza\")\n",
    "        return None\n",
    "    \n",
    "    # Filtrar valores faltantes\n",
    "    initial = len(df_normalized)\n",
    "    df_normalized = df_normalized.dropna(subset=['image_filename', 'weight_kg', 'breed'])\n",
    "    if initial != len(df_normalized):\n",
    "        print(\"   ‚ö†Ô∏è Eliminadas {initial - len(df_normalized)} filas con valores faltantes\")\n",
    "        # Si es CID y todas las filas fueron eliminadas, puede ser problema de estructura de carpetas\n",
    "        if dataset_name == \"CID Dataset\" and len(df_normalized) == 0:\n",
    "            print(\"   ‚ö†Ô∏è ADVERTENCIA: No se encontraron im√°genes para CID. Verifica estructura de carpetas.\")\n",
    "            print(\"   üí° CID puede necesitar procesamiento manual o las im√°genes est√°n en otra ubicaci√≥n.\")\n",
    "    \n",
    "    print(\"   ‚úÖ Normalizado: {len(df_normalized):,} registros v√°lidos\")\n",
    "    return df_normalized[['image_filename', 'weight_kg', 'breed']]\n",
    "\n",
    "# ESTRATEGIA B: Combinar CID + Nuestras im√°genes\n",
    "df_cid = None\n",
    "df_scraped = None\n",
    "df_local = None\n",
    "\n",
    "# Funci√≥n para generar metadata desde carpetas BLF (cuando CSV tiene pocos registros)\n",
    "def generate_cid_metadata_from_folders(base_dir, cid_csv_path=None):\n",
    "    \"\"\"\n",
    "    Genera metadata para TODAS las im√°genes de CID desde estructura de carpetas.\n",
    "    Usa el CSV como referencia para peso/raza cuando est√° disponible.\n",
    "    \"\"\"\n",
    "    cid_images_dir = base_dir / 'cid' / 'images'\n",
    "    cid_yt_dir = base_dir / 'cid' / 'yt_images'\n",
    "    \n",
    "    # Cargar CSV como referencia (si existe)\n",
    "    cid_reference = {}\n",
    "    if cid_csv_path and cid_csv_path.exists():\n",
    "        try:\n",
    "            df_ref = pd.read_csv(cid_csv_path)\n",
    "            if 'sku' in df_ref.columns:\n",
    "                for _, row in df_ref.iterrows():\n",
    "                    sku = str(row['sku']).strip()\n",
    "                    # Normalizar SKU a formato BLF\n",
    "                    if 'BLF' in sku.upper():\n",
    "                        blf_code = sku.upper().replace('BLF', '').strip()\n",
    "                        folder_name = f\"BLF{blf_code}\" if blf_code else sku.upper()\n",
    "                    elif sku.isdigit():\n",
    "                        folder_name = f\"BLF{sku}\"\n",
    "                    else:\n",
    "                        folder_name = sku\n",
    "                    \n",
    "                    cid_reference[folder_name] = {\n",
    "                        'weight_kg': row.get('weight_in_kg', None),\n",
    "                        'breed': str(row.get('breed', 'unknown')).lower().strip() if pd.notna(row.get('breed')) else 'unknown'\n",
    "                    }\n",
    "            print(\"   üìã Referencia CSV cargada: {len(cid_reference)} carpetas con metadata\")\n",
    "        except Exception as e:\n",
    "            print(\"   ‚ö†Ô∏è Error cargando CSV de referencia: {e}\")\n",
    "    \n",
    "    # Escanear todas las carpetas BLF\n",
    "    all_records = []\n",
    "    folders_scanned = 0\n",
    "    images_found_total = 0\n",
    "    \n",
    "    for img_dir in [cid_images_dir, cid_yt_dir]:\n",
    "        if not img_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        # Buscar todas las carpetas BLF\n",
    "        for folder_path in img_dir.iterdir():\n",
    "            if not folder_path.is_dir():\n",
    "                continue\n",
    "            \n",
    "            folder_name = folder_path.name\n",
    "            # Verificar que es una carpeta BLF\n",
    "            if not (folder_name.upper().startswith('BLF') or folder_name.replace(' ', '').upper().startswith('BLF')):\n",
    "                continue\n",
    "            \n",
    "            folders_scanned += 1\n",
    "            # Normalizar nombre de carpeta\n",
    "            normalized_folder = folder_name.replace(' ', '').upper()\n",
    "            if not normalized_folder.startswith('BLF'):\n",
    "                continue\n",
    "            \n",
    "            # Buscar im√°genes v√°lidas (excluir las que empiezan con _)\n",
    "            valid_images = []\n",
    "            for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "                all_imgs = list(folder_path.glob(ext))\n",
    "                valid_imgs = [img for img in all_imgs if not img.name.startswith('_')]\n",
    "                valid_images.extend(valid_imgs)\n",
    "            \n",
    "            if not valid_images:\n",
    "                continue\n",
    "            \n",
    "            images_found_total += len(valid_images)\n",
    "            \n",
    "            # Obtener metadata de referencia (si existe)\n",
    "            ref_data = cid_reference.get(normalized_folder, {})\n",
    "            weight_kg = ref_data.get('weight_kg')\n",
    "            breed = ref_data.get('breed', 'unknown')\n",
    "            \n",
    "            # Si no hay peso en referencia, usar promedio estimado (350 kg)\n",
    "            if weight_kg is None or pd.isna(weight_kg):\n",
    "                weight_kg = 350.0  # Promedio estimado para ganado\n",
    "            \n",
    "            # Si no hay raza en referencia, usar 'generic'\n",
    "            if breed == 'unknown' or not breed:\n",
    "                breed = 'generic'\n",
    "            \n",
    "            # Crear registro para cada imagen\n",
    "            for img_path in valid_images:\n",
    "                rel_path = img_path.relative_to(base_dir)\n",
    "                all_records.append({\n",
    "                    'image_filename': str(rel_path),\n",
    "                    'weight_kg': weight_kg,\n",
    "                    'breed': breed\n",
    "                })\n",
    "    \n",
    "    print(\"   üìÅ Carpetas BLF escaneadas: {folders_scanned}\")\n",
    "    print(\"   üì∏ Im√°genes encontradas: {images_found_total:,}\")\n",
    "    print(\"   üìä Registros generados: {len(all_records):,}\")\n",
    "    \n",
    "    if all_records:\n",
    "        df_generated = pd.DataFrame(all_records)\n",
    "        return df_generated\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 1. Cargar CID Dataset\n",
    "cid_metadata = RAW_DIR / 'cid' / 'dataset.csv'\n",
    "df_cid = None\n",
    "\n",
    "if cid_metadata.exists():\n",
    "    try:\n",
    "        df_cid_raw = pd.read_csv(cid_metadata)\n",
    "        print(\"‚úÖ CID Dataset CSV cargado: {len(df_cid_raw):,} registros\")\n",
    "        \n",
    "        # Si el CSV tiene pocos registros (< 1000), generar metadata desde carpetas\n",
    "        if len(df_cid_raw) < 1000:\n",
    "            print(\"\\nüí° CSV tiene pocos registros ({len(df_cid_raw)}). Generando metadata desde carpetas BLF...\")\n",
    "            df_cid_generated = generate_cid_metadata_from_folders(RAW_DIR, cid_metadata)\n",
    "            \n",
    "            if df_cid_generated is not None and len(df_cid_generated) > len(df_cid_raw):\n",
    "                print(\"‚úÖ Metadata generada desde carpetas: {len(df_cid_generated):,} registros\")\n",
    "                print(\"üí° Usando metadata generada (m√°s completa que CSV)\")\n",
    "                df_cid = normalize_dataset(df_cid_generated, 'CID Dataset (Generado)', RAW_DIR)\n",
    "            else:\n",
    "                # Usar CSV normalizado\n",
    "                df_cid = normalize_dataset(df_cid_raw, 'CID Dataset', RAW_DIR)\n",
    "        else:\n",
    "            # CSV tiene suficientes registros, usar directamente\n",
    "            df_cid = normalize_dataset(df_cid_raw, 'CID Dataset', RAW_DIR)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error procesando CID: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Intentar generar desde carpetas como fallback\n",
    "        print(\"\\nüí° Intentando generar metadata desde carpetas como fallback...\")\n",
    "        df_cid_generated = generate_cid_metadata_from_folders(RAW_DIR, cid_metadata)\n",
    "        if df_cid_generated is not None:\n",
    "            df_cid = normalize_dataset(df_cid_generated, 'CID Dataset (Fallback)', RAW_DIR)\n",
    "        else:\n",
    "            df_cid = None\n",
    "else:\n",
    "    # No hay CSV, generar desde carpetas\n",
    "    print(\"‚ö†Ô∏è CID CSV no encontrado. Generando metadata desde carpetas BLF...\")\n",
    "    df_cid_generated = generate_cid_metadata_from_folders(RAW_DIR, None)\n",
    "    if df_cid_generated is not None:\n",
    "        df_cid = normalize_dataset(df_cid_generated, 'CID Dataset (Sin CSV)', RAW_DIR)\n",
    "    else:\n",
    "        df_cid = None\n",
    "\n",
    "# 2. Cargar nuestras im√°genes scrapeadas\n",
    "# Intentar primero metadata_estimada.csv (generado por BLOQUE 6)\n",
    "scraped_metadata = RAW_DIR / 'scraped' / 'metadata_estimada.csv'\n",
    "# Si no existe, intentar metadata.csv como alternativa\n",
    "if not scraped_metadata.exists():\n",
    "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
    "if scraped_metadata.exists():\n",
    "    try:\n",
    "        df_scraped_raw = pd.read_csv(scraped_metadata)\n",
    "        print(\"‚úÖ Dataset Scrapeado cargado: {len(df_scraped_raw):,} registros\")\n",
    "        df_scraped = normalize_dataset(df_scraped_raw, 'Dataset Scrapeado', RAW_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error procesando scraped: {e}\")\n",
    "        df_scraped = None\n",
    "\n",
    "# 3. Cargar im√°genes locales (si existen)\n",
    "local_metadata = RAW_DIR / 'local_images' / 'metadata.csv'\n",
    "if local_metadata.exists():\n",
    "    try:\n",
    "        df_local_raw = pd.read_csv(local_metadata)\n",
    "        print(\"‚úÖ Im√°genes Locales cargadas: {len(df_local_raw):,} registros\")\n",
    "        df_local = normalize_dataset(df_local_raw, 'Im√°genes Locales', RAW_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error procesando local: {e}\")\n",
    "        df_local = None\n",
    "\n",
    "# Combinar datasets (ESTRATEGIA B)\n",
    "dfs_to_combine = []\n",
    "if df_cid is not None and len(df_cid) > 0:\n",
    "    dfs_to_combine.append(df_cid)\n",
    "if df_scraped is not None and len(df_scraped) > 0:\n",
    "    dfs_to_combine.append(df_scraped)\n",
    "if df_local is not None and len(df_local) > 0:\n",
    "    dfs_to_combine.append(df_local)\n",
    "\n",
    "if not dfs_to_combine:\n",
    "    print(\"\\n‚ùå No se encontraron datasets disponibles\")\n",
    "    print(\"üí° Ejecuta BLOQUE 6 para descargar im√°genes y BLOQUE 7 para CID Dataset\")\n",
    "    raise ValueError(\"No hay datasets disponibles para crear el pipeline\")\n",
    "\n",
    "# Combinar todos los DataFrames\n",
    "print(\"\\nüîó Combinando {len(dfs_to_combine)} dataset(s)...\")\n",
    "df_pipeline = pd.concat(dfs_to_combine, ignore_index=True)\n",
    "print(\"‚úÖ Dataset combinado: {len(df_pipeline):,} registros totales\")\n",
    "\n",
    "# Determinar directorio base (usar RAW_DIR como base com√∫n)\n",
    "base_data_dir = RAW_DIR\n",
    "\n",
    "# Los datasets ya est√°n normalizados antes de combinarlos\n",
    "# Solo verificar que tienen las columnas requeridas\n",
    "required_cols = ['image_filename', 'weight_kg', 'breed']\n",
    "for df in dfs_to_combine:\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Dataset no normalizado correctamente. Faltan: {missing}\")\n",
    "print(\"‚úÖ Todos los datasets est√°n normalizados correctamente\")\n",
    "\n",
    "# Convertir image_filename a rutas relativas si es necesario\n",
    "def normalize_image_path(path_str):\n",
    "    \"\"\"Normaliza rutas de im√°genes para que funcionen con m√∫ltiples fuentes.\"\"\"\n",
    "    if pd.isna(path_str):\n",
    "        return None\n",
    "    path = Path(str(path_str))\n",
    "    # Si es absoluto, intentar hacer relativo\n",
    "    if path.is_absolute():\n",
    "        try:\n",
    "            # Intentar relativo a RAW_DIR\n",
    "            return path.relative_to(RAW_DIR)\n",
    "        except ValueError:\n",
    "            # Si no funciona, usar solo el nombre del archivo\n",
    "            return Path(path.name)\n",
    "    return path\n",
    "\n",
    "df_pipeline['image_filename'] = df_pipeline['image_filename'].apply(normalize_image_path)\n",
    "\n",
    "# Verificar columnas requeridas\n",
    "required_cols = ['image_filename', 'weight_kg', 'breed']\n",
    "missing_cols = [col for col in required_cols if col not in df_pipeline.columns]\n",
    "if missing_cols:\n",
    "    print(\"‚ùå Faltan columnas requeridas: {missing_cols}\")\n",
    "    print(\"üí° Columnas disponibles: {list(df_pipeline.columns)}\")\n",
    "    raise ValueError(f\"Columnas requeridas faltantes: {missing_cols}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset cargado: {len(df_pipeline):,} registros\")\n",
    "print(\"üìä Columnas: {list(df_pipeline.columns)}\")\n",
    "print(\"üìÅ Directorio base de im√°genes: {base_data_dir}\")\n",
    "\n",
    "# Dividir datos en train/val/test\n",
    "print(\"\\nüìä Dividiendo datos en train/val/test...\")\n",
    "df_shuffled = df_pipeline.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "n_total = len(df_shuffled)\n",
    "n_train = int(n_total * (1 - CONFIG['validation_split'] - CONFIG['test_split']))\n",
    "n_val = int(n_total * CONFIG['validation_split'])\n",
    "\n",
    "df_train = df_shuffled[:n_train]\n",
    "df_val = df_shuffled[n_train:n_train + n_val]\n",
    "df_test = df_shuffled[n_train + n_val:]\n",
    "\n",
    "print(\"üìà Train: {len(df_train):,} ({len(df_train)/n_total*100:.1f}%)\")\n",
    "print(\"üìà Val: {len(df_val):,} ({len(df_val)/n_total*100:.1f}%)\")\n",
    "print(\"üìà Test: {len(df_test):,} ({len(df_test)/n_total*100:.1f}%)\")\n",
    "\n",
    "# Crear generadores usando m√≥dulos del proyecto\n",
    "print(\"\\nüîß Creando generadores de datos usando CattleDataGenerator...\")\n",
    "\n",
    "# Augmentation para entrenamiento (agresivo para dataset peque√±o)\n",
    "train_transform = get_aggressive_augmentation(image_size=CONFIG['image_size'])\n",
    "val_transform = get_validation_transform(image_size=CONFIG['image_size'])\n",
    "\n",
    "# Crear generadores\n",
    "train_generator = CattleDataGenerator(\n",
    "    annotations_df=df_train,\n",
    "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    transform=train_transform,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = CattleDataGenerator(\n",
    "    annotations_df=df_val,\n",
    "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    transform=val_transform,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = CattleDataGenerator(\n",
    "    annotations_df=df_test,\n",
    "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    transform=val_transform,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ BLOQUE 11 COMPLETADO (ESTRATEGIA B)\")\n",
    "print(\"üìä Generadores creados usando m√≥dulos del proyecto:\")\n",
    "print(\"   - Train: {len(df_train):,} im√°genes ({len(train_generator)} batches)\")\n",
    "print(\"   - Val: {len(df_val):,} im√°genes ({len(val_generator)} batches)\")\n",
    "print(\"   - Test: {len(df_test):,} im√°genes ({len(test_generator)} batches)\")\n",
    "print(\"\\nüí° ESTRATEGIA B: Dataset combinado (CID + Nuestras im√°genes)\")\n",
    "print(\"üí° Contin√∫a con el BLOQUE 12 para crear la arquitectura del modelo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 12: ARQUITECTURA DEL MODELO\n",
    "# ============================================================\n",
    "# üèóÔ∏è Crea modelo usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 11 ejecutado (generadores creados) y BLOQUE 5 (CONFIG definido)\n",
    "# üí° Usa: BreedWeightEstimatorCNN.build_generic_model() (models.cnn_architecture)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üèóÔ∏è ARQUITECTURA DEL MODELO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que CONFIG est√° definido (BLOQUE 5)\n",
    "if 'CONFIG' not in globals():\n",
    "    raise ValueError(\"CONFIG no est√° definido. Ejecuta el BLOQUE 5 primero.\")\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados\n",
    "try:\n",
    "    from models.cnn_architecture import BreedWeightEstimatorCNN\n",
    "    print(\"‚úÖ M√≥dulo BreedWeightEstimatorCNN importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el proyecto est√© clonado correctamente (BLOQUE 1)\")\n",
    "    raise\n",
    "\n",
    "# Verificar que los generadores est√°n creados (BLOQUE 11)\n",
    "if 'train_generator' not in globals():\n",
    "    raise ValueError(\"Generadores no encontrados. Ejecuta el BLOQUE 11 primero.\")\n",
    "\n",
    "# Crear modelo gen√©rico usando m√≥dulo del proyecto\n",
    "print(\"üèóÔ∏è Creando modelo gen√©rico usando BreedWeightEstimatorCNN...\")\n",
    "print(\"üìä Configuraci√≥n:\")\n",
    "print(\"   - Image size: {CONFIG['image_size']}\")\n",
    "print(\"   - Base architecture: EfficientNetB1\")\n",
    "print(\"   - Learning rate: {CONFIG['learning_rate']}\")\n",
    "\n",
    "# Usar build_generic_model del m√≥dulo del proyecto\n",
    "try:\n",
    "    model = BreedWeightEstimatorCNN.build_generic_model(\n",
    "        input_shape=CONFIG['image_size'] + (3,),\n",
    "        base_architecture='efficientnetb1'  # EfficientNetB1 para mejor precisi√≥n\n",
    "    )\n",
    "    print(\"‚úÖ Modelo base creado\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creando modelo: {e}\")\n",
    "    raise\n",
    "\n",
    "# Compilar con learning rate personalizado\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Modelo creado y compilado\")\n",
    "print(\"üìä Par√°metros: {model.count_params():,}\")\n",
    "print(\"üìä Arquitectura: {model.name}\")\n",
    "print(\"üí° Usando m√≥dulo del proyecto: models.cnn_architecture.BreedWeightEstimatorCNN\")\n",
    "\n",
    "# Verificar arquitectura\n",
    "print(\"\\nüîç Verificaci√≥n de arquitectura:\")\n",
    "print(\"   - Input shape: {model.input_shape}\")\n",
    "print(\"   - Output shape: {model.output_shape}\")\n",
    "print(\"   - Total layers: {len(model.layers)}\")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"\\nüìê Resumen del modelo:\")\n",
    "model.summary()\n",
    "\n",
    "# ============================================================\n",
    "# üîÑ DECIDIR SI CARGAR MODELO ANTERIOR O EMPEZAR DESDE CERO\n",
    "# ============================================================\n",
    "# üí° Si el dataset cambi√≥ significativamente, es mejor empezar desde cero\n",
    "# ‚ö†Ô∏è El modelo anterior fue entrenado con dataset peque√±o (1,882 im√°genes)\n",
    "# ‚ö†Ô∏è Ahora tenemos dataset mucho m√°s grande (28,444 im√°genes)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta del mejor modelo (checkpoint guardado durante entrenamiento)\n",
    "BEST_MODEL_PATH = Path('/content/drive/MyDrive/bovine-weight-estimation/ml-training/checkpoints/best_model.h5')\n",
    "\n",
    "# Verificar tama√±o del dataset actual\n",
    "current_dataset_size = 0\n",
    "if 'df_pipeline' in globals() and df_pipeline is not None:\n",
    "    current_dataset_size = len(df_pipeline)\n",
    "elif 'train_generator' in globals():\n",
    "    current_dataset_size = len(train_generator) * CONFIG['batch_size']\n",
    "\n",
    "# Dataset anterior ten√≠a ~1,882 im√°genes\n",
    "PREVIOUS_DATASET_SIZE = 1882\n",
    "DATASET_CHANGE_THRESHOLD = 2.0  # Si el dataset es 2x m√°s grande, empezar desde cero\n",
    "\n",
    "should_load_checkpoint = False\n",
    "\n",
    "if BEST_MODEL_PATH.exists():\n",
    "    dataset_ratio = current_dataset_size / PREVIOUS_DATASET_SIZE if PREVIOUS_DATASET_SIZE > 0 else 0\n",
    "    \n",
    "    print(\"\\nüîç Verificando si cargar modelo anterior...\")\n",
    "    print(\"   üìä Dataset anterior: ~{PREVIOUS_DATASET_SIZE:,} im√°genes\")\n",
    "    print(\"   üìä Dataset actual: ~{current_dataset_size:,} im√°genes\")\n",
    "    print(\"   üìà Ratio: {dataset_ratio:.2f}x\")\n",
    "    \n",
    "    if dataset_ratio >= DATASET_CHANGE_THRESHOLD:\n",
    "        print(\"\\n‚ö†Ô∏è Dataset cambi√≥ significativamente ({dataset_ratio:.1f}x m√°s grande)\")\n",
    "        print(\"üí° RECOMENDACI√ìN: Empezar desde cero con el nuevo dataset\")\n",
    "        print(\"üí° El modelo anterior fue entrenado con dataset peque√±o y tuvo malos resultados\")\n",
    "        print(\"üí° Empezar desde cero permitir√° que el modelo aprenda mejor con m√°s datos\")\n",
    "        print(\"\\n‚úÖ Continuando con modelo nuevo (sin cargar checkpoint)\")\n",
    "        should_load_checkpoint = False\n",
    "    else:\n",
    "        print(\"\\nüí° Dataset similar al anterior ({dataset_ratio:.1f}x)\")\n",
    "        print(\"üí° Puedes cargar el checkpoint para continuar entrenamiento\")\n",
    "        should_load_checkpoint = True\n",
    "        try:\n",
    "            print(\"üîÑ Cargando mejor modelo entrenado...\")\n",
    "            print(\"üìÅ Ruta: {BEST_MODEL_PATH}\")\n",
    "            model.load_weights(str(BEST_MODEL_PATH))\n",
    "            print(\"‚úÖ Modelo cargado exitosamente desde checkpoint\")\n",
    "            print(\"üí° El modelo est√° listo para evaluaci√≥n o continuar entrenamiento\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Error cargando checkpoint: {e}\")\n",
    "            print(\"üí° Continuando con modelo nuevo (sin pesos pre-entrenados)\")\n",
    "            should_load_checkpoint = False\n",
    "else:\n",
    "    print(\"\\nüí° No se encontr√≥ checkpoint previo en: {BEST_MODEL_PATH}\")\n",
    "    print(\"üí° Esto es normal si es la primera vez que entrenas\")\n",
    "    print(\"üí° Despu√©s del entrenamiento (BLOQUE 14), el checkpoint se guardar√° aqu√≠\")\n",
    "    should_load_checkpoint = False\n",
    "\n",
    "# Nota final\n",
    "if not should_load_checkpoint and current_dataset_size > PREVIOUS_DATASET_SIZE:\n",
    "    print(\"\\nüìù NOTA: El nuevo entrenamiento sobrescribir√° best_model.h5 con el modelo mejorado\")\n",
    "    print(\"üí° Esto es correcto: el nuevo modelo ser√° entrenado con {current_dataset_size:,} im√°genes\")\n",
    "\n",
    "print(\"\\n‚úÖ BLOQUE 12 COMPLETADO\")\n",
    "print(\"üí° Contin√∫a con el BLOQUE 13 para configurar el entrenamiento\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 13: CONFIGURACI√ìN DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è Configura callbacks (EarlyStopping, ReduceLR, ModelCheckpoint, TensorBoard)\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 12 ejecutado (modelo creado)\n",
    "# üí° Configura MLflow para tracking de experimentos (Estrategia B)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "def setup_training_callbacks():\n",
    "    \"\"\"Configurar callbacks para entrenamiento\"\"\"\n",
    "    print(\"‚öôÔ∏è Configurando callbacks de entrenamiento...\")\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early stopping\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['early_stopping_patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,  # M√°s agresivo (era 0.5) - reduce LR m√°s r√°pido\n",
    "            patience=3,  # M√°s r√°pido (era 5) - espera menos √©pocas\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpoint\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=str(MODELS_DIR / 'best_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=str(BASE_DIR / 'logs'),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ {len(callbacks_list)} callbacks configurados\")\n",
    "    return callbacks_list\n",
    "\n",
    "# Verificar que CONFIG y MODELS_DIR est√°n definidos\n",
    "if 'CONFIG' not in globals():\n",
    "    raise ValueError(\"CONFIG no est√° definido. Ejecuta el BLOQUE 5 primero.\")\n",
    "\n",
    "if 'MODELS_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        MODELS_DIR = BASE_DIR / 'ml-training' / 'src' / 'models'\n",
    "    else:\n",
    "        MODELS_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/ml-training/src/models')\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verificar que el modelo est√° creado\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Modelo no encontrado. Ejecuta el BLOQUE 12 primero.\")\n",
    "\n",
    "# Configurar callbacks\n",
    "training_callbacks = setup_training_callbacks()\n",
    "\n",
    "# Configurar MLflow (si est√° disponible)\n",
    "def start_mlflow_run():\n",
    "    \"\"\"Iniciar run de MLflow con Estrategia B\"\"\"\n",
    "    # Detectar datasets usados (Estrategia B)\n",
    "    datasets_used = []\n",
    "    if 'df_pipeline' in globals() and df_pipeline is not None:\n",
    "        # Contar registros por fuente si es posible\n",
    "        total_records = len(df_pipeline)\n",
    "        datasets_used.append(f\"Combined-{total_records}\")\n",
    "    else:\n",
    "        datasets_used.append(\"Unknown\")\n",
    "    \n",
    "    dataset_str = \"+\".join(datasets_used)\n",
    "    run = mlflow.start_run(run_name=f\"cattle-weight-estrategia-b-{dataset_str.lower()}\")\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'strategy': 'B - Combined Dataset',\n",
    "        'model': 'EfficientNetB1',\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'image_size': str(CONFIG['image_size']),\n",
    "        'augmentation': 'Albumentations',\n",
    "        'data_combination': 'CID + Scraped + Local'\n",
    "    })\n",
    "\n",
    "    print(\"üî¨ MLflow run iniciado: {run.info.run_id}\")\n",
    "    print(\"üìä Estrategia B: Dataset combinado registrado\")\n",
    "    return run\n",
    "\n",
    "# Iniciar MLflow run (si est√° disponible)\n",
    "try:\n",
    "    import mlflow\n",
    "    if 'mlflow_available' in globals() and mlflow_available:\n",
    "        mlflow_run = start_mlflow_run()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è MLflow no disponible - continuando sin tracking\")\n",
    "        mlflow_run = None\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è MLflow no instalado - continuando sin tracking\")\n",
    "    mlflow_run = None\n",
    "\n",
    "print(\"\\n‚úÖ BLOQUE 13 COMPLETADO\")\n",
    "print(\"üí° Callbacks configurados: {len(training_callbacks)}\")\n",
    "print(\"üí° Contin√∫a con el BLOQUE 14 para iniciar el entrenamiento\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 14: ENTRENAMIENTO DEL MODELO\n",
    "# ============================================================\n",
    "# üöÄ Entrena el modelo base (puede tardar horas con GPU)\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 13 ejecutado (callbacks configurados)\n",
    "# ‚ö†Ô∏è Tiempo estimado: 2-4 horas con GPU T4 (100 √©pocas)\n",
    "# üí° IMPORTANTE: Este bloque guarda checkpoints para reanudar entrenamiento\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ ENTRENAMIENTO DEL MODELO CON REANUDACI√ìN AUTOM√ÅTICA\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURACI√ìN: Directorio de checkpoints\n",
    "if 'BASE_DIR' in globals():\n",
    "    CHECKPOINTS_DIR = BASE_DIR / 'ml-training' / 'checkpoints'\n",
    "else:\n",
    "    CHECKPOINTS_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/ml-training/checkpoints')\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = CHECKPOINTS_DIR / 'model_checkpoint.weights.h5'\n",
    "BEST_MODEL_PATH = CHECKPOINTS_DIR / 'best_model.h5'\n",
    "\n",
    "print(\"üìÅ Directorio de checkpoints: {CHECKPOINTS_DIR}\")\n",
    "print()\n",
    "\n",
    "def train_model_with_resume():\n",
    "    \"\"\"Entrenar modelo con capacidad de reanudar desde checkpoint\"\"\"\n",
    "    print(\"üöÄ Iniciando entrenamiento del modelo base...\")\n",
    "    print(\"üìä Configuraci√≥n: {CONFIG}\")\n",
    "    \n",
    "    # Verificar que los generadores existen\n",
    "    if 'train_generator' not in globals() or 'val_generator' not in globals():\n",
    "        raise ValueError(\"Generadores no encontrados. Ejecuta el BLOQUE 11 primero.\")\n",
    "    \n",
    "    # Verificar que el modelo existe\n",
    "    if 'model' not in globals():\n",
    "        raise ValueError(\"Modelo no encontrado. Ejecuta el BLOQUE 12 primero.\")\n",
    "    \n",
    "    # Calcular steps por √©poca (usando generadores)\n",
    "    steps_per_epoch = len(train_generator)\n",
    "    validation_steps = len(val_generator)\n",
    "    \n",
    "    print(\"üìà Steps por √©poca: {steps_per_epoch}\")\n",
    "    print(\"üìà Validation steps: {validation_steps}\")\n",
    "    print(\"üí° Usando generadores del proyecto (CattleDataGenerator)\")\n",
    "    print()\n",
    "    \n",
    "    # üîÑ INTENTAR CARGAR CHECKPOINT PREVIO\n",
    "    initial_epoch = 0\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        try:\n",
    "            print(\"üîÑ Checkpoint encontrado. Intentando cargar...\")\n",
    "            # Cargar pesos del modelo\n",
    "            model.load_weights(str(CHECKPOINT_PATH))\n",
    "            print(\"‚úÖ Checkpoint cargado exitosamente\")\n",
    "            \n",
    "            # Intentar determinar la √©poca desde el nombre del archivo o logs\n",
    "            # Buscar archivos de checkpoint con n√∫mero de √©poca\n",
    "            checkpoint_files = list(CHECKPOINTS_DIR.glob(\"model_checkpoint*.weights.h5*\"))\n",
    "            if checkpoint_files:\n",
    "                # Si hay checkpoints numerados, usar el √∫ltimo\n",
    "                print(\"üìÇ Encontrados {len(checkpoint_files)} archivos de checkpoint\")\n",
    "                print(\"üí° Continuando desde el √∫ltimo checkpoint guardado\")\n",
    "            \n",
    "            # Preguntar al usuario desde qu√© √©poca continuar (o usar la √∫ltima)\n",
    "            print(\"\\nüí° NOTA: Si sabes en qu√© √©poca se interrumpi√≥, puedes ajustar initial_epoch manualmente\")\n",
    "            print(\"üí° Ejemplo: initial_epoch = 25  # Continuar desde √©poca 25\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Error cargando checkpoint: {e}\")\n",
    "            print(\"üí° Iniciando entrenamiento desde el principio...\")\n",
    "            initial_epoch = 0\n",
    "    else:\n",
    "        print(\"üìù No se encontr√≥ checkpoint previo. Iniciando entrenamiento nuevo.\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # ‚öôÔ∏è CONFIGURAR CALLBACKS CON CHECKPOINT MEJORADO\n",
    "    from tensorflow.keras import callbacks\n",
    "    \n",
    "    # Obtener callbacks existentes si existen, o crear lista vac√≠a\n",
    "    existing_callbacks = globals().get('training_callbacks', [])\n",
    "    \n",
    "    # Callback para guardar checkpoint cada √©poca\n",
    "    checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=str(CHECKPOINT_PATH),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=False,  # Guardar TODAS las √©pocas (permite reanudar)\n",
    "        save_weights_only=True,  # Solo guardar pesos (m√°s r√°pido)\n",
    "        verbose=1,\n",
    "        save_freq='epoch',  # Guardar cada √©poca (reemplaza period en TF 2.x+)\n",
    "    )\n",
    "    \n",
    "    # Callback para guardar el mejor modelo\n",
    "    best_model_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=str(BEST_MODEL_PATH),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,  # Solo el mejor\n",
    "        save_weights_only=False,  # Guardar modelo completo\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Combinar callbacks\n",
    "    all_callbacks = [checkpoint_callback, best_model_callback]\n",
    "    \n",
    "    # Agregar otros callbacks existentes (excluyendo ModelCheckpoint duplicados)\n",
    "    for cb in existing_callbacks:\n",
    "        if not isinstance(cb, callbacks.ModelCheckpoint):\n",
    "            all_callbacks.append(cb)\n",
    "    \n",
    "    print(\"‚úÖ {len(all_callbacks)} callbacks configurados (incluye checkpoint por √©poca)\")\n",
    "    print()\n",
    "    \n",
    "    # üìã INSTRUCCIONES PARA MANTENER COLAB ACTIVO\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí° INSTRUCCIONES PARA ENTRENAMIENTO LARGO:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. üîå MANTENER COLAB ACTIVO:\")\n",
    "    print(\"   - Instala extensi√≥n 'Colab Keepalive' en Chrome/Firefox\")\n",
    "    print(\"   - O ejecuta este c√≥digo en otra celda para mantener activo:\")\n",
    "    print(\"     ```python\")\n",
    "    print(\"     import time\")\n",
    "    print(\"     while True:\")\n",
    "    print(\"         time.sleep(300)  # Cada 5 minutos\")\n",
    "    print(\"         print(f'‚è∞ {time.strftime(\\\"%H:%M:%S\\\")} - Sesi√≥n activa')\")\n",
    "    print(\"     ```\")\n",
    "    print(\"2. üîÑ REANUDAR DESPU√âS DE INTERRUPCI√ìN:\")\n",
    "    print(\"   - Si se interrumpe, simplemente vuelve a ejecutar este bloque\")\n",
    "    print(\"   - El c√≥digo detectar√° autom√°ticamente el checkpoint y continuar√°\")\n",
    "    print(\"   - Si necesitas continuar desde una √©poca espec√≠fica, ajusta initial_epoch arriba\")\n",
    "    print(\"3. üìä MONITOREAR PROGRESO:\")\n",
    "    print(\"   - Revisa los logs en TensorBoard: tensorboard --logdir=logs\")\n",
    "    print(\"   - Los checkpoints se guardan en:\", str(CHECKPOINTS_DIR))\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Entrenar modelo usando generadores\n",
    "    print(\"üöÄ Iniciando entrenamiento desde √©poca {initial_epoch}...\")\n",
    "    print(\"üìä Total de √©pocas: {CONFIG['epochs']}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=CONFIG['epochs'],\n",
    "            initial_epoch=initial_epoch,  # Continuar desde aqu√≠\n",
    "            validation_data=val_generator,\n",
    "            callbacks=all_callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Entrenamiento completado exitosamente\")\n",
    "        return history\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Entrenamiento interrumpido por el usuario\")\n",
    "        print(\"üí° El √∫ltimo checkpoint se guard√≥. Puedes reanudar ejecutando este bloque de nuevo.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚ùå Error durante el entrenamiento: {e}\")\n",
    "        print(\"üí° El √∫ltimo checkpoint se guard√≥. Puedes reanudar ejecutando este bloque de nuevo.\")\n",
    "        raise\n",
    "\n",
    "# Entrenamiento real (requiere generadores preparados y tiempo de ejecuci√≥n con GPU)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è IMPORTANTE: Este entrenamiento puede tardar 2-4 horas\")\n",
    "print(\"üí° Aseg√∫rate de mantener Colab activo (ver instrucciones arriba)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "history = train_model_with_resume()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 15: EVALUACI√ìN DEL MODELO\n",
    "# ============================================================\n",
    "# üìä Eval√∫a el modelo usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 14 ejecutado (modelo entrenado)\n",
    "# üí° Usa: MetricsCalculator (models.evaluation.metrics)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä EVALUACI√ìN DEL MODELO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
    "try:\n",
    "    from models.evaluation.metrics import MetricsCalculator, ModelMetrics\n",
    "    print(\"‚úÖ M√≥dulo MetricsCalculator importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
    "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
    "    raise\n",
    "\n",
    "# Verificar que el generador de test existe\n",
    "if 'test_generator' not in globals():\n",
    "    raise ValueError(\"Generador de test no encontrado. Ejecuta el BLOQUE 11 primero.\")\n",
    "\n",
    "# Verificar que el modelo existe\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Modelo no encontrado. Ejecuta el BLOQUE 12 primero.\")\n",
    "\n",
    "# ============================================================\n",
    "# üîç VERIFICACI√ìN Y DIAGN√ìSTICO PRE-EVALUACI√ìN\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "\n",
    "BEST_MODEL_PATH = Path('/content/drive/MyDrive/bovine-weight-estimation/ml-training/checkpoints/best_model.h5')\n",
    "\n",
    "print(\"üîç VERIFICACI√ìN PRE-EVALUACI√ìN:\")\n",
    "print()\n",
    "\n",
    "# 1. Verificar carga de pesos del mejor modelo\n",
    "if BEST_MODEL_PATH.exists():\n",
    "    print(\"‚úÖ Checkpoint encontrado: {BEST_MODEL_PATH}\")\n",
    "    try:\n",
    "        print(\"üîÑ Cargando pesos del mejor modelo expl√≠citamente...\")\n",
    "        model.load_weights(str(BEST_MODEL_PATH))\n",
    "        print(\"‚úÖ Pesos cargados exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error cargando pesos: {e}\")\n",
    "        print(\"üí° Continuando con pesos actuales del modelo\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ best_model.h5 en: {BEST_MODEL_PATH}\")\n",
    "    print(\"üí° El modelo puede no estar entrenado o los pesos no se guardaron\")\n",
    "\n",
    "# 2. Verificaci√≥n r√°pida de predicciones\n",
    "print(\"\\nüîç Verificaci√≥n r√°pida de predicciones:\")\n",
    "try:\n",
    "    test_sample = next(iter(test_generator))\n",
    "    sample_images = test_sample[0][:3]  # Primeras 3 im√°genes\n",
    "    sample_targets = test_sample[1][:3]  # Primeros 3 pesos reales\n",
    "    \n",
    "    sample_predictions = model.predict(sample_images, verbose=0)\n",
    "    sample_predictions = sample_predictions.flatten()\n",
    "    \n",
    "    print(\"üìä Predicciones de prueba:\")\n",
    "    for i in range(len(sample_predictions)):\n",
    "        pred = sample_predictions[i]\n",
    "        true = sample_targets[i]\n",
    "        diff = abs(pred - true)\n",
    "        print(\"   Imagen {i+1}: Pred={pred:.2f} kg, Real={true:.2f} kg, Diff={diff:.2f} kg\")\n",
    "    \n",
    "    # Verificar rango de predicciones\n",
    "    pred_min, pred_max = sample_predictions.min(), sample_predictions.max()\n",
    "    true_min, true_max = sample_targets.min(), sample_targets.max()\n",
    "    \n",
    "    print(\"\\nüìä Rangos:\")\n",
    "    print(\"   Predicciones: {pred_min:.2f} - {pred_max:.2f} kg\")\n",
    "    print(\"   Valores reales: {true_min:.2f} - {true_max:.2f} kg\")\n",
    "    \n",
    "    # Advertencias\n",
    "    if pred_max > 10000 or pred_min < -1000:\n",
    "        print(\"\\n‚ö†Ô∏è ADVERTENCIA: Predicciones fuera de rango esperado (200-600 kg t√≠picamente)\")\n",
    "        print(\"üí° El modelo puede no estar funcionando correctamente\")\n",
    "    elif pred_max < 1 or pred_min < 0:\n",
    "        print(\"\\n‚ö†Ô∏è ADVERTENCIA: Predicciones muy bajas o negativas\")\n",
    "        print(\"üí° Verifica el preprocesamiento de datos\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Predicciones en rango razonable\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Error en verificaci√≥n r√°pida: {e}\")\n",
    "\n",
    "# 3. Verificar datos de test\n",
    "print(\"\\nüîç Verificaci√≥n de datos de test:\")\n",
    "try:\n",
    "    if 'df_test' in globals():\n",
    "        weight_min = df_test['weight_kg'].min()\n",
    "        weight_max = df_test['weight_kg'].max()\n",
    "        weight_mean = df_test['weight_kg'].mean()\n",
    "        print(\"   Rango de pesos: {weight_min:.2f} - {weight_max:.2f} kg\")\n",
    "        print(\"   Media de pesos: {weight_mean:.2f} kg\")\n",
    "        print(\"   Total registros: {len(df_test):,}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è df_test no disponible para verificaci√≥n\")\n",
    "except Exception as e:\n",
    "    print(\"   ‚ö†Ô∏è Error verificando datos: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print()\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluar modelo en conjunto de test usando MetricsCalculator\"\"\"\n",
    "    print(\"üìä Evaluando modelo en conjunto de test...\")\n",
    "    \n",
    "    # Obtener predicciones y valores reales\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    print(\"üîç Generando predicciones...\")\n",
    "    for i in range(len(test_generator)):\n",
    "        batch_images, batch_targets = test_generator[i]\n",
    "        predictions = model.predict(batch_images, verbose=0)\n",
    "        y_true.extend(batch_targets.flatten())\n",
    "        y_pred.extend(predictions.flatten())\n",
    "    \n",
    "    # Convertir a numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    print(\"üìä Total predicciones: {len(y_true):,}\")\n",
    "    \n",
    "    # Calcular m√©tricas usando m√≥dulo del proyecto\n",
    "    print(\"\\nüìà Calculando m√©tricas usando MetricsCalculator...\")\n",
    "    metrics = MetricsCalculator.calculate_metrics(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        breed_type='generic'\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\nüìà RESULTADOS DE EVALUACI√ìN:\")\n",
    "    print(\"   R¬≤: {metrics.r2_score:.4f}\")\n",
    "    print(\"   MAE: {metrics.mae_kg:.2f} kg\")\n",
    "    print(\"   MSE: {metrics.mse_kg:.2f}\")\n",
    "    print(\"   MAPE: {metrics.mape_percent:.2f}%\")\n",
    "    print(\"   Bias: {metrics.bias_kg:.2f} kg\")\n",
    "    \n",
    "    # Verificar objetivos (con validaci√≥n opcional)\n",
    "    print(\"\\nüéØ VERIFICACI√ìN DE OBJETIVOS:\")\n",
    "    r2_ok = metrics.r2_score >= CONFIG['target_r2']\n",
    "    mae_ok = metrics.mae_kg < CONFIG['max_mae']\n",
    "    \n",
    "    print(\"   R¬≤ ‚â• {CONFIG['target_r2']}: {'‚úÖ' if r2_ok else '‚ö†Ô∏è'} ({metrics.r2_score:.4f})\")\n",
    "    print(\"   MAE < {CONFIG['max_mae']} kg: {'‚úÖ' if mae_ok else '‚ö†Ô∏è'} ({metrics.mae_kg:.2f} kg)\")\n",
    "    \n",
    "    if not (r2_ok and mae_ok):\n",
    "        print(\"\\n‚ö†Ô∏è ADVERTENCIA: Los objetivos no se cumplieron\")\n",
    "        print(\"\\nüîç DIAGN√ìSTICO:\")\n",
    "        \n",
    "        # Diagn√≥stico de R¬≤ negativo\n",
    "        if metrics.r2_score < 0:\n",
    "            print(\"   ‚ùå R¬≤ negativo ({metrics.r2_score:.4f}): El modelo es PEOR que predecir la media\")\n",
    "            print(\"      üí° Causas probables:\")\n",
    "            print(\"         - Modelo no carg√≥ pesos correctamente\")\n",
    "            print(\"         - Modelo no entrenado suficientemente\")\n",
    "            print(\"         - Problema con preprocesamiento de datos\")\n",
    "        \n",
    "        # Diagn√≥stico de MAE muy alto\n",
    "        if metrics.mae_kg > 50:\n",
    "            print(\"   ‚ùå MAE muy alto ({metrics.mae_kg:.2f} kg vs objetivo < {CONFIG['max_mae']} kg)\")\n",
    "            print(\"      üí° Causas probables:\")\n",
    "            print(\"         - Modelo prediciendo valores fuera de rango\")\n",
    "            print(\"         - Dataset muy peque√±o (solo {len(y_true):,} muestras de test)\")\n",
    "            print(\"         - Modelo no aprendi√≥ patrones √∫tiles\")\n",
    "        \n",
    "        # Estad√≠sticas adicionales\n",
    "        print(\"\\nüìä ESTAD√çSTICAS ADICIONALES:\")\n",
    "        print(\"   - Rango de predicciones: {y_pred.min():.2f} - {y_pred.max():.2f} kg\")\n",
    "        print(\"   - Rango de valores reales: {y_true.min():.2f} - {y_true.max():.2f} kg\")\n",
    "        print(\"   - Media de predicciones: {y_pred.mean():.2f} kg\")\n",
    "        print(\"   - Media de valores reales: {y_true.mean():.2f} kg\")\n",
    "        \n",
    "        # Verificar si hay valores an√≥malos\n",
    "        if y_pred.max() > 10000 or y_pred.min() < -1000:\n",
    "            print(\"\\n   ‚ö†Ô∏è Predicciones con valores extremos detectados\")\n",
    "            print(\"      üí° Verifica que el modelo carg√≥ pesos correctamente\")\n",
    "        \n",
    "        print(\"\\nüí° RECOMENDACIONES:\")\n",
    "        print(\"   1. Verifica que best_model.h5 se carg√≥ correctamente (ver arriba)\")\n",
    "        print(\"   2. Re-entrena con m√°s √©pocas (aumenta 'epochs' en CONFIG)\")\n",
    "        print(\"   3. Aumenta 'early_stopping_patience' para permitir m√°s entrenamiento\")\n",
    "        print(\"   4. Aumenta el dataset (usa m√°s datos del CID Dataset)\")\n",
    "        print(\"   5. Verifica preprocesamiento de im√°genes y pesos\")\n",
    "    \n",
    "    # Log m√©tricas en MLflow\n",
    "    if 'mlflow' in globals() and ('mlflow_available' in globals() and mlflow_available):\n",
    "        mlflow.log_metrics({\n",
    "            'test_r2': metrics.r2_score,\n",
    "            'test_mae_kg': metrics.mae_kg,\n",
    "            'test_mse_kg': metrics.mse_kg,\n",
    "            'test_mape_percent': metrics.mape_percent,\n",
    "            'test_bias_kg': metrics.bias_kg\n",
    "        })\n",
    "    \n",
    "    return metrics.to_dict()\n",
    "\n",
    "# Evaluar modelo\n",
    "evaluation_results = evaluate_model()\n",
    "\n",
    "print(\"\\n‚úÖ BLOQUE 15 COMPLETADO\")\n",
    "print(\"üí° M√©tricas calculadas usando m√≥dulo del proyecto: MetricsCalculator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE 16: EXPORTAR A TFLITE\n",
    "# ============================================================\n",
    "# üì± Exporta modelo usando m√≥dulos del proyecto\n",
    "# ‚ö†Ô∏è Requiere: BLOQUE 15 ejecutado (modelo evaluado)\n",
    "# üí° Usa: TFLiteExporter (models.export.tflite_converter)\n",
    "# üîß CORREGIDO: Exporta directamente desde modelo en memoria para evitar errores de serializaci√≥n\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì± EXPORTAR A TFLITE (USANDO M√ìDULOS DEL PROYECTO)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Verificar que el modelo est√° entrenado y evaluado\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Modelo no encontrado. Ejecuta el BLOQUE 12 primero.\")\n",
    "\n",
    "# Verificar que los m√≥dulos est√°n importados\n",
    "try:\n",
    "    from models.export.tflite_converter import TFLiteExporter\n",
    "    print(\"‚úÖ M√≥dulo TFLiteExporter importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el proyecto est√© clonado correctamente\")\n",
    "    raise\n",
    "\n",
    "# Verificar que MODELS_DIR est√° definido\n",
    "if 'MODELS_DIR' not in globals():\n",
    "    if 'BASE_DIR' in globals():\n",
    "        MODELS_DIR = BASE_DIR / 'models'\n",
    "    else:\n",
    "        from pathlib import Path\n",
    "        MODELS_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/models')\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Exportar usando TFLiteExporter del proyecto\n",
    "tflite_path = MODELS_DIR / 'generic-cattle-v1.0.0.tflite'\n",
    "\n",
    "print(\"\\nüì± Exportando modelo a TFLite usando TFLiteExporter...\")\n",
    "print(f\"üìÅ Archivo de salida: {tflite_path}\")\n",
    "print(\"üí° Exportando directamente desde modelo en memoria (evita errores de serializaci√≥n)\")\n",
    "\n",
    "# Usar TFLiteExporter del proyecto directamente con el modelo en memoria\n",
    "# Esto evita problemas de serializaci√≥n al guardar/recargar el modelo\n",
    "try:\n",
    "    model_size_bytes = TFLiteExporter.convert_to_tflite(\n",
    "        model=model,  # Usar modelo directamente en memoria\n",
    "        output_path=str(tflite_path),\n",
    "        optimization='default'  # FP16: reduce 2x el tama√±o, mantiene precisi√≥n\n",
    "    )\n",
    "    \n",
    "    model_size_kb = model_size_bytes / 1024\n",
    "    model_size_mb = model_size_kb / 1024\n",
    "    \n",
    "    print(\"‚úÖ Modelo exportado exitosamente\")\n",
    "    print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error al exportar directamente desde memoria: {e}\")\n",
    "    print(\"üí° Intentando m√©todo alternativo (exportar como SavedModel en Drive)...\")\n",
    "    \n",
    "    # M√©todo alternativo: exportar como SavedModel en Drive y luego convertir\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Usar Drive en lugar de /tmp para evitar problemas de espacio y persistencia\n",
    "    if 'MODELS_DIR' in globals():\n",
    "        temp_dir = MODELS_DIR / 'temp_export'\n",
    "    elif 'BASE_DIR' in globals():\n",
    "        temp_dir = BASE_DIR / 'models' / 'temp_export'\n",
    "    else:\n",
    "        temp_dir = Path('/content/drive/MyDrive/bovine-weight-estimation/models/temp_export')\n",
    "    \n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    saved_model_path = temp_dir / 'saved_model'\n",
    "    \n",
    "    try:\n",
    "        # En Keras 3, usar model.export() para exportar como SavedModel\n",
    "        print(\"üíæ Exportando modelo como SavedModel en Drive...\")\n",
    "        print(f\"üìÅ Ruta: {saved_model_path}\")\n",
    "        model.export(str(saved_model_path))  # Keras 3: export() para SavedModel\n",
    "        print(\"‚úÖ Modelo exportado como SavedModel\")\n",
    "        \n",
    "        # Exportar usando el SavedModel\n",
    "        model_size_bytes = TFLiteExporter.convert_to_tflite(\n",
    "            saved_model_path=str(saved_model_path),\n",
    "            output_path=str(tflite_path),\n",
    "            optimization='default'\n",
    "        )\n",
    "        \n",
    "        model_size_kb = model_size_bytes / 1024\n",
    "        model_size_mb = model_size_kb / 1024\n",
    "        \n",
    "        print(\"‚úÖ Modelo exportado exitosamente (m√©todo alternativo)\")\n",
    "        print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
    "        \n",
    "        # Limpiar directorio temporal\n",
    "        print(\"üßπ Limpiando directorio temporal...\")\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        print(\"‚úÖ Directorio temporal limpiado\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Error con m√©todo alternativo: {e2}\")\n",
    "        print(f\"üí° Limpiando directorio temporal en: {temp_dir}...\")\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        raise\n",
    "\n",
    "# Log en MLflow\n",
    "if 'mlflow' in globals() and ('mlflow_available' in globals() and mlflow_available):\n",
    "    try:\n",
    "        mlflow.log_artifact(str(tflite_path))\n",
    "        mlflow.log_metric('model_size_kb', model_size_kb)\n",
    "        mlflow.log_metric('model_size_mb', model_size_mb)\n",
    "        print(\"üìä M√©tricas guardadas en MLflow\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error guardando en MLflow: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ BLOQUE 16 COMPLETADO\")\n",
    "print(\"üéØ MODELO BASE LISTO PARA INTEGRACI√ìN\")\n",
    "print(f\"üìÅ Archivo: {tflite_path}\")\n",
    "print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
    "print(\"üí° Usando m√≥dulo del proyecto: TFLiteExporter\")\n",
    "if 'mlflow_run' in globals():\n",
    "    print(f\"üî¨ MLflow run: {mlflow_run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Resumen y Pr√≥ximos Pasos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notas Importantes\n",
    "\n",
    "### üéØ Estrategia B - Dataset Combinado\n",
    "- **CID Dataset**: ~17,899 im√°genes (diversidad y calidad)\n",
    "- **Nuestras Im√°genes**: ~1,400+ im√°genes (especificidad local - razas bolivianas)\n",
    "- **Total Combinado**: ~19,299+ im√°genes para mejor modelo\n",
    "- **Beneficio**: Mejor generalizaci√≥n + precisi√≥n espec√≠fica local\n",
    "\n",
    "### üîß Optimizaciones Implementadas\n",
    "- **Estrategia B**: Combinaci√≥n de datasets desde el inicio (BLOQUE 8)\n",
    "- **Data Pipeline**: Generadores optimizados con Albumentations (BLOQUE 11)\n",
    "- **Augmentation**: Agresivo para dataset peque√±o\n",
    "- **Arquitectura**: EfficientNetB1 para mejor precisi√≥n (BLOQUE 12)\n",
    "- **TFLite Export**: Optimizado para m√≥vil (FP16) (BLOQUE 16)\n",
    "\n",
    "### üìä M√©tricas Objetivo\n",
    "- **R¬≤ ‚â• 0.95**: Explicaci√≥n 95% de varianza\n",
    "- **MAE < 5 kg**: Error absoluto promedio\n",
    "- **Inference < 3s**: Tiempo en m√≥vil\n",
    "\n",
    "### üéØ Estado Actual\n",
    "- ‚úÖ **Infraestructura ML**: Completada y optimizada\n",
    "- ‚úÖ **Pipeline de datos**: Estrategia B implementada\n",
    "- ‚úÖ **Modelo base**: EfficientNetB1 listo para entrenamiento\n",
    "- ‚úÖ **Notebook optimizado**: ~40% menos c√≥digo, bloques consecutivos (1-16)\n",
    "- üîÑ **Pr√≥ximo**: Entrenar modelo con dataset combinado\n",
    "\n",
    "### üìã Flujo Completo Optimizado\n",
    "1. **Bloques 1-5**: Setup y configuraci√≥n\n",
    "2. **Bloques 6-9**: Descarga y preparaci√≥n de datasets (Estrategia B)\n",
    "3. **Bloque 10**: Verificaci√≥n r√°pida (OPCIONAL - puede saltarse)\n",
    "4. **Bloques 11-16**: Pipeline, modelo, entrenamiento y exportaci√≥n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

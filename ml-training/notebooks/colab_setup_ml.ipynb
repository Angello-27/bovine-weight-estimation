{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêÑ Sistema de Estimaci√≥n de Peso Bovino - Setup ML\n",
        "\n",
        "**Proyecto**: Hacienda Gamelera - Bruno Brito Macedo  \n",
        "**Responsable**: Persona 2 - Setup Infraestructura ML  \n",
        "**Objetivo**: Preparar datasets y pipeline para entrenamiento de 7 modelos por raza  \n",
        "**Duraci√≥n**: 5-6 d√≠as  \n",
        "\n",
        "---\n",
        "\n",
        "## üìã Checklist de Tareas\n",
        "- [x] D√≠a 1: Setup Google Colab Pro + dependencias\n",
        "- [ ] D√≠a 2-3: Descargar y organizar datasets cr√≠ticos\n",
        "- [ ] D√≠a 4: An√°lisis exploratorio de datos (EDA)\n",
        "- [ ] D√≠a 5-6: Preparar pipeline de datos optimizado\n",
        "\n",
        "## üéØ Razas Objetivo (7 razas)\n",
        "1. **Brahman** - Bos indicus robusto\n",
        "2. **Nelore** - Bos indicus\n",
        "3. **Angus** - Bos taurus, buena carne\n",
        "4. **Cebuinas** - Bos indicus general\n",
        "5. **Criollo** - Adaptado local\n",
        "6. **Pardo Suizo** - Bos taurus grande\n",
        "7. **Jersey** - Lechera, menor tama√±o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üìÅ CONFIGURAR RUTA DEL PROYECTO\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ‚úÖ INDICA d√≥nde vive el repositorio dentro del runtime de Colab.\n",
        "#    - Si acabas de clonar el repo:   BASE_DIR = Path('/content/bovine-weight-estimation')\n",
        "#    - Si lo tienes en Google Drive: BASE_DIR = Path('/content/drive/MyDrive/<carpeta>')\n",
        "BASE_DIR = Path('/content/bovine-weight-estimation')\n",
        "# BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')  # <--- Descomenta si usas Drive\n",
        "\n",
        "# A√±adimos la carpeta src al PYTHONPATH para que todos los m√≥dulos internos sean importables.\n",
        "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
        "sys.path.insert(0, str(ML_TRAINING_DIR / 'src'))\n",
        "\n",
        "# Validamos que la estructura del proyecto exista antes de continuar.\n",
        "if ML_TRAINING_DIR.exists():\n",
        "    print(f\"‚úÖ Proyecto encontrado en: {ML_TRAINING_DIR}\")\n",
        "    print(\"üìÇ Subcarpetas clave detectadas:\")\n",
        "    print(f\"   - C√≥digo fuente: {ML_TRAINING_DIR / 'src'}\")\n",
        "    print(f\"   - Scripts utilitarios: {ML_TRAINING_DIR / 'scripts'}\")\n",
        "    print(f\"   - Configuraci√≥n: {ML_TRAINING_DIR / 'config'}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è No se encontr√≥ el proyecto en: {ML_TRAINING_DIR}\")\n",
        "    print(\"üí° Ajusta BASE_DIR o revisa que clonaste/montaste el repositorio correctamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚úÖ IMPORTAR M√ìDULOS DEL PROYECTO\n",
        "# ============================================================\n",
        "\n",
        "# Data Augmentation\n",
        "from data.augmentation import get_training_transform, get_aggressive_augmentation, get_validation_transform\n",
        "\n",
        "# Modelos\n",
        "from models.cnn_architecture import BreedWeightEstimatorCNN, BREED_CONFIGS\n",
        "\n",
        "# Evaluaci√≥n\n",
        "from models.evaluation.metrics import MetricsCalculator, ModelMetrics\n",
        "\n",
        "# Exportaci√≥n TFLite\n",
        "from models.export.tflite_converter import TFLiteExporter\n",
        "\n",
        "print(\"‚úÖ Todos los m√≥dulos importados correctamente\")\n",
        "print(\"\\nüì¶ M√≥dulos disponibles:\")\n",
        "print(\"   - Data augmentation (Albumentations 2.0.8)\")\n",
        "print(\"   - CNN architectures (MobileNetV2, EfficientNet)\")\n",
        "print(\"   - Metrics calculator (R¬≤, MAE, MAPE)\")\n",
        "print(\"   - TFLite exporter (optimizado para m√≥vil)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üéì EJEMPLO: CREAR MODELO PARA UNA RAZA\n",
        "# ============================================================\n",
        "\n",
        "# Ejemplo 1: Crear modelo para Brahman\n",
        "model_brahman = BreedWeightEstimatorCNN.build_model(\n",
        "    breed_name='brahman',\n",
        "    base_architecture='mobilenetv2'  # M√°s r√°pido que EfficientNet\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Modelo creado: {model_brahman.name}\")\n",
        "print(f\"üìä Par√°metros: {model_brahman.count_params():,}\")\n",
        "\n",
        "# Ver arquitectura\n",
        "print(\"\\nüìê Arquitectura del modelo:\")\n",
        "model_brahman.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Pr√≥ximos Pasos\n",
        "\n",
        "1. **Descargar datasets** (CID, CattleEyeView, etc.)\n",
        "2. **Preprocesar datos** con nuestros m√≥dulos\n",
        "3. **Entrenar modelo base** gen√©rico\n",
        "4. **Fine-tuning por raza** (5 razas)\n",
        "5. **Recolecci√≥n propia** (Criollo, Pardo Suizo)\n",
        "6. **Exportar a TFLite** e integrar en app m√≥vil\n",
        "\n",
        "> Ver `README.md` y `scripts/train_all_breeds.py` para m√°s ejemplos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## üöÄ D√≠a 1: Setup Google Colab Pro + Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üîß INSTALACI√ìN DE DEPENDENCIAS - CONFIGURACI√ìN ESTABLE (Colab 2025)\n",
        "# ============================================================\n",
        "\n",
        "# ‚ö†Ô∏è Ejecuta esta celda SOLO una vez tras abrir el notebook. Las versiones fijadas\n",
        "#    son compatibles con Python 3.10 y con la GPU T4 de Colab (TensorFlow 2.17).\n",
        "#    Si ya instalaste dependencias, puedes omitirla para evitar reinstalaciones.\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q tensorflow==2.17.0 tensorflow-hub tensorflow-datasets\n",
        "!pip install -q albumentations==2.0.8 opencv-python-headless==4.10.0.84\n",
        "!pip install -q kaggle gdown mlflow==2.14.1 dvc[gs,s3]==3.51.1 plotly seaborn\n",
        "!pip install -q numpy==1.26.4 pillow==11.0.0 pyarrow==15.0.2 packaging==24.2 google-images-download==2.8.0 scikit-learn==1.3.2\n",
        "!pip install -q protobuf==4.25.3\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Activamos mixed precision (FP16 en GPU) para acelerar el entrenamiento.\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "print(\"‚úÖ TensorFlow:\", tf.__version__)\n",
        "print(\"‚úÖ GPU detectada:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(\"üéÆ GPU lista para entrenamiento.\")\n",
        "    except RuntimeError as e:\n",
        "        print(\"‚ö†Ô∏è Error configurando GPU:\", e)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se detect√≥ GPU. Activa GPU desde Entorno de ejecuci√≥n > Cambiar tipo de entorno.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚úÖ FIX FINAL COMPATIBLE - Albumentations 2.0.8 (Colab 2025)\n",
        "# ============================================================\n",
        "\n",
        "# ‚ö†Ô∏è Solo si Colab instal√≥ autom√°ticamente versiones incompatibles. Esta celda garantiza\n",
        "#    que Albumentations y OpenCV usen la pareja estable para Python 3.10.\n",
        "!pip install -q --upgrade pip\n",
        "!pip uninstall -y albumentations albucore\n",
        "!pip install -q albumentations==2.0.8 opencv-python-headless==4.10.0.84\n",
        "\n",
        "import albumentations as A\n",
        "import cv2\n",
        "\n",
        "print(\"‚úÖ Albumentations instalado correctamente:\", A.__version__)\n",
        "print(\"‚úÖ OpenCV:\", cv2.__version__)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# IMPORTS Y CONFIGURACI√ìN\n",
        "# ============================================================\n",
        "\n",
        "# üîç Conjunto completo de librer√≠as usadas en el pipeline: utilidades del sistema,\n",
        "#    ciencia de datos, visualizaci√≥n, ML y tracking de experimentos.\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from pathlib import Path\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# MLflow para tracking reproducible de experimentos.\n",
        "import mlflow\n",
        "import mlflow.tensorflow\n",
        "\n",
        "# Configurar matplotlib para que todas las gr√°ficas se vean consistentes en Colab.\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"‚úÖ Todas las dependencias importadas correctamente\")\n",
        "print(f\"üìä Versiones: TF={tf.__version__}, CV2={cv2.__version__}, Albumentations={A.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è CONFIGURACI√ìN DEL PROYECTO (bovine-weight-estimation)\n",
        "# ============================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import mlflow\n",
        "from google.colab import drive\n",
        "\n",
        "# üîó Montamos Google Drive solo si no est√° disponible. As√≠ persistimos datasets y modelos.\n",
        "if not Path('/content/drive').exists() or not any(Path('/content/drive').iterdir()):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('‚ÑπÔ∏è Google Drive ya est√° montado.')\n",
        "\n",
        "# üìÅ Directorio base dentro de tu Drive donde se almacenar√° todo el entrenamiento.\n",
        "BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
        "\n",
        "# üìÇ Creamos (si no existen) las carpetas est√°ndar para datos crudos, procesados y modelos.\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "AUGMENTED_DIR = DATA_DIR / 'augmented'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "MLRUNS_DIR = BASE_DIR / 'mlruns'\n",
        "\n",
        "for dir_path in [DATA_DIR, RAW_DIR, PROCESSED_DIR, AUGMENTED_DIR, MODELS_DIR, MLRUNS_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üìä Configuraci√≥n de MLflow (tracking local persistente)\n",
        "# ------------------------------------------------------------\n",
        "mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")\n",
        "mlflow.set_experiment(\"bovine-weight-estimation\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ‚öôÔ∏è Configuraci√≥n general del entrenamiento (hiperpar√°metros base)\n",
        "# ------------------------------------------------------------\n",
        "CONFIG = {\n",
        "    'image_size': (224, 224),\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 0.001,\n",
        "    'validation_split': 0.2,\n",
        "    'test_split': 0.1,\n",
        "    'early_stopping_patience': 10,\n",
        "    'target_r2': 0.95,\n",
        "    'max_mae': 5.0,\n",
        "    'max_inference_time': 3.0\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üêÑ Razas objetivo (Santa Cruz, Chiquitan√≠a y Pampa)\n",
        "# ------------------------------------------------------------\n",
        "BREEDS = [\n",
        "    'brahman', 'nelore', 'angus', 'cebuinas',\n",
        "    'criollo', 'pardo_suizo', 'guzerat', 'holstein'\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completada correctamente\")\n",
        "print(f\"üìÅ Directorio base: {BASE_DIR}\")\n",
        "print(f\"üéØ Razas objetivo: {len(BREEDS)} razas -> {BREEDS}\")\n",
        "print(f\"üìä MLflow tracking: {MLRUNS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• D√≠a 2-3: Descargar y Organizar Datasets Cr√≠ticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1. CID DATASET (17,899 im√°genes) - M√ÅS IMPORTANTE\n",
        "# ============================================================\n",
        "\n",
        "CID_DATASET_ARCHIVE_PATH = os.environ.get('CID_DATASET_ARCHIVE_PATH')\n",
        "\n",
        "\n",
        "def download_cid_dataset(archive_path: str | None = CID_DATASET_ARCHIVE_PATH) -> Path:\n",
        "    \"\"\"Prepara el CID Dataset desde un archivo previamente descargado.\n",
        "\n",
        "    Requisitos antes de ejecutar:\n",
        "    1. Sube el archivo comprimido real (zip/tar) al directorio definido en BASE_DIR o /content.\n",
        "    2. Establece la variable de entorno CID_DATASET_ARCHIVE_PATH apuntando a ese archivo.\n",
        "\n",
        "    No se generan datos sint√©ticos: si falta el archivo, se detendr√° con un error.\n",
        "    \"\"\"\n",
        "    cid_dir = RAW_DIR / 'cid'\n",
        "    cid_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(cid_dir.iterdir()):\n",
        "        print(f\"‚ÑπÔ∏è CID Dataset ya est√° disponible en {cid_dir}. Se omite extracci√≥n.\")\n",
        "        return cid_dir\n",
        "\n",
        "    if archive_path is None:\n",
        "        raise RuntimeError(\n",
        "            \"Configura la variable de entorno CID_DATASET_ARCHIVE_PATH con la ruta del \"\n",
        "            \"archivo comprimido del CID Dataset (por ejemplo .zip o .tar.gz) antes de ejecutar esta celda.\"\n",
        "        )\n",
        "\n",
        "    archive_path = Path(archive_path)\n",
        "    if not archive_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontr√≥ el archivo comprimido del CID Dataset en {archive_path}. \"\n",
        "            \"Sube el dataset real a tu Google Drive y vuelve a ejecutar.\"\n",
        "        )\n",
        "\n",
        "    print(f\"üì• Extrayendo CID Dataset desde: {archive_path}\")\n",
        "    try:\n",
        "        shutil.unpack_archive(str(archive_path), str(cid_dir))\n",
        "    except shutil.ReadError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo desempaquetar el CID Dataset. Verifica que el archivo est√© en un formato soportado \"\n",
        "            \"(.zip, .tar, .tar.gz, .tar.bz2, etc.).\"\n",
        "        ) from exc\n",
        "\n",
        "    if not any(cid_dir.iterdir()):\n",
        "        raise RuntimeError(\n",
        "            \"La extracci√≥n del CID Dataset no produjo archivos. Verifica que el archivo comprimido contenga datos v√°lidos.\"\n",
        "        )\n",
        "\n",
        "    print(f\"‚úÖ CID Dataset preparado en: {cid_dir}\")\n",
        "    return cid_dir\n",
        "\n",
        "\n",
        "# Ejecutar preparaci√≥n (requerir√° archivo real previamente cargado)\n",
        "cid_dataset_path = download_cid_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2. KAGGLE CATTLE WEIGHT DATASET (12k im√°genes)\n",
        "# ============================================================\n",
        "\n",
        "KAGGLE_DATASET_ID = os.environ.get(\n",
        "    'KAGGLE_DATASET_ID', 'sadhliroomyprime/cattle-weight-detection-model-dataset-12k'\n",
        ")\n",
        "\n",
        "\n",
        "def setup_kaggle_api() -> Path:\n",
        "    \"\"\"Configura la API de Kaggle para descargas reales.\"\"\"\n",
        "    print(\"üîë Configurando API de Kaggle...\")\n",
        "\n",
        "    kaggle_dir = Path('/root/.kaggle')\n",
        "    kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    kaggle_json = kaggle_dir / 'kaggle.json'\n",
        "    if not kaggle_json.exists():\n",
        "        raise FileNotFoundError(\n",
        "            \"No se encontr√≥ /root/.kaggle/kaggle.json. Descarga tu token desde \"\n",
        "            \"https://www.kaggle.com/account, s√∫belo al notebook y vuelve a ejecutar.\"\n",
        "        )\n",
        "\n",
        "    subprocess.run([\"chmod\", \"600\", \"/root/.kaggle/kaggle.json\"], check=True)\n",
        "    return kaggle_dir\n",
        "\n",
        "\n",
        "def download_kaggle_dataset(dataset_id: str = KAGGLE_DATASET_ID) -> Path:\n",
        "    \"\"\"Descarga el dataset de Kaggle indicado.\n",
        "\n",
        "    Requisitos:\n",
        "    - Subir `kaggle.json` (token API) a este notebook y colocarlo en /root/.kaggle/\n",
        "    - Definir KAGGLE_DATASET_ID si deseas descargar un dataset distinto al preset.\n",
        "    \"\"\"\n",
        "    if not dataset_id:\n",
        "        raise RuntimeError(\"Define la variable de entorno KAGGLE_DATASET_ID con el dataset a descargar.\")\n",
        "\n",
        "    kaggle_dir = setup_kaggle_api()\n",
        "    output_dir = RAW_DIR / 'kaggle'\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(output_dir.glob('**/*')):\n",
        "        print(f\"‚ÑπÔ∏è Dataset de Kaggle ya presente en {output_dir}. Se omite descarga.\")\n",
        "        return output_dir\n",
        "\n",
        "    print(f\"üì• Descargando dataset de Kaggle: {dataset_id}\")\n",
        "    subprocess.run([\n",
        "        \"kaggle\",\n",
        "        \"datasets\",\n",
        "        \"download\",\n",
        "        \"-d\",\n",
        "        dataset_id,\n",
        "        \"-p\",\n",
        "        str(output_dir),\n",
        "    ], check=True)\n",
        "\n",
        "    archive_files = list(output_dir.glob('*.zip'))\n",
        "    if not archive_files:\n",
        "        raise RuntimeError(\"La descarga de Kaggle no produjo archivos .zip. Verifica el ID del dataset.\")\n",
        "\n",
        "    for archive_file in archive_files:\n",
        "        print(f\"üì¶ Descomprimiendo {archive_file.name}\")\n",
        "        subprocess.run([\n",
        "            \"unzip\",\n",
        "            \"-q\",\n",
        "            str(archive_file),\n",
        "            \"-d\",\n",
        "            str(output_dir),\n",
        "        ], check=True)\n",
        "        archive_file.unlink()\n",
        "\n",
        "    if not any(output_dir.glob('**/*')):\n",
        "        raise RuntimeError(\"La extracci√≥n del dataset de Kaggle no produjo archivos. Revisa el contenido descargado.\")\n",
        "\n",
        "    print(f\"‚úÖ Kaggle dataset disponible en: {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "# Ejecutar descarga (requiere credenciales reales)\n",
        "kaggle_dataset_path = download_kaggle_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. GOOGLE IMAGES SCRAPING PARA RAZAS LOCALES\n",
        "# ============================================================\n",
        "\n",
        "def scrape_google_images():\n",
        "    \"\"\"Scraping de Google Images para razas locales.\n",
        "\n",
        "    Uso opcional para complementar razas poco representadas. Respeta los t√©rminos de uso\n",
        "    del motor de b√∫squeda y evita ejecutar m√∫ltiples veces para no ser bloqueado.\n",
        "    \"\"\"\n",
        "    print(\"üñºÔ∏è Scraping Google Images para razas locales...\")\n",
        "    \n",
        "    from google_images_download import google_images_download\n",
        "    \n",
        "    # Razas locales espec√≠ficas\n",
        "    breeds_local = [\n",
        "        'ganado criollo boliviano',\n",
        "        'guzerat bolivia', \n",
        "        'brahman chiquitania',\n",
        "        'nelore pantanal',\n",
        "        'angus bolivia',\n",
        "        'pardo suizo bolivia',\n",
        "        'jersey bolivia'\n",
        "    ]\n",
        "    \n",
        "    response = google_images_download.googleimagesdownload()\n",
        "    \n",
        "    scraped_count = 0\n",
        "    \n",
        "    for breed in breeds_local:\n",
        "        try:\n",
        "            print(f\"üì∏ Scraping: {breed}\")\n",
        "            \n",
        "            # Configuraci√≥n de descarga\n",
        "            arguments = {\n",
        "                \"keywords\": breed,\n",
        "                \"limit\": 50,  # L√≠mite por t√©rmino\n",
        "                \"print_urls\": False,\n",
        "                \"output_directory\": str(RAW_DIR / 'scraped'),\n",
        "                \"image_directory\": breed.replace(' ', '_'),\n",
        "                \"format\": \"jpg\",\n",
        "                \"size\": \"medium\",\n",
        "                \"aspect_ratio\": \"wide\"\n",
        "            }\n",
        "            \n",
        "            # Descargar im√°genes\n",
        "            paths = response.download(arguments)\n",
        "            \n",
        "            if paths:\n",
        "                count = len(paths[0])\n",
        "                scraped_count += count\n",
        "                print(f\"‚úÖ {breed}: {count} im√°genes descargadas\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error con {breed}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"üéØ Total im√°genes scraped: {scraped_count}\")\n",
        "    return scraped_count\n",
        "\n",
        "# Ejecutar scraping\n",
        "scraped_images = scrape_google_images()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RESUMEN DE DATASETS DESCARGADOS\n",
        "# ============================================================\n",
        "\n",
        "def summarize_datasets(cid_df: pd.DataFrame | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Resumen de todos los datasets disponibles (solo datos reales).\"\"\"\n",
        "    print(\"üìä RESUMEN DE DATASETS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    datasets_info = []\n",
        "\n",
        "    if cid_df is not None:\n",
        "        datasets_info.append({\n",
        "            'name': 'CID Dataset',\n",
        "            'images': len(cid_df),\n",
        "            'description': 'Computer Vision Research - Cattle Image Database',\n",
        "            'status': '‚úÖ Disponible',\n",
        "        })\n",
        "    else:\n",
        "        datasets_info.append({\n",
        "            'name': 'CID Dataset',\n",
        "            'images': 0,\n",
        "            'description': 'CID sin metadata cargada',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    if kaggle_dataset_path and kaggle_dataset_path.exists():\n",
        "        kaggle_images = len(list(kaggle_dataset_path.glob('**/*.jpg')))\n",
        "        datasets_info.append({\n",
        "            'name': 'Kaggle Cattle Weight',\n",
        "            'images': kaggle_images,\n",
        "            'description': f'Dataset Kaggle ({KAGGLE_DATASET_ID})',\n",
        "            'status': '‚úÖ Disponible' if kaggle_images > 0 else '‚ö†Ô∏è Vac√≠o',\n",
        "        })\n",
        "    else:\n",
        "        datasets_info.append({\n",
        "            'name': 'Kaggle Cattle Weight',\n",
        "            'images': 0,\n",
        "            'description': 'Requiere configuraci√≥n de API Kaggle',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    datasets_info.append({\n",
        "        'name': 'Google Images Scraped',\n",
        "        'images': scraped_images,\n",
        "        'description': 'Razas locales bolivianas',\n",
        "        'status': '‚úÖ Disponible' if scraped_images > 0 else '‚ö†Ô∏è Pendiente',\n",
        "    })\n",
        "\n",
        "    df_datasets = pd.DataFrame(datasets_info)\n",
        "    print(df_datasets.to_string(index=False))\n",
        "\n",
        "    total_images = int(df_datasets['images'].sum())\n",
        "    print(f\"\\nüéØ TOTAL IM√ÅGENES DISPONIBLES: {total_images:,}\")\n",
        "\n",
        "    summary_path = DATA_DIR / 'datasets_summary.csv'\n",
        "    df_datasets.to_csv(summary_path, index=False)\n",
        "    print(f\"\\nüíæ Resumen guardado en: {summary_path}\")\n",
        "\n",
        "    return df_datasets\n",
        "\n",
        "# Ejecutar resumen con datos reales cargados\n",
        "datasets_summary = summarize_datasets(df_cid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä D√≠a 4: An√°lisis Exploratorio de Datos (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AN√ÅLISIS EXPLORATORIO - CID DATASET\n",
        "# ============================================================\n",
        "\n",
        "CID_METADATA_FILE = Path(os.environ.get('CID_METADATA_FILE', cid_dataset_path / 'metadata.csv'))\n",
        "\n",
        "\n",
        "def analyze_cid_dataset(metadata_file: Path) -> pd.DataFrame:\n",
        "    \"\"\"An√°lisis exploratorio utilizando datos reales del CID Dataset.\"\"\"\n",
        "    if not metadata_file.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontr√≥ el archivo de metadata del CID Dataset en {metadata_file}. \"\n",
        "            \"Genera o coloca un CSV con las columnas ['image_path', 'weight_kg', 'breed', 'age_category', 'image_quality', 'lighting', 'angle'].\"\n",
        "        )\n",
        "\n",
        "    df_cid = pd.read_csv(metadata_file)\n",
        "\n",
        "    required_columns = {\n",
        "        'image_path',\n",
        "        'weight_kg',\n",
        "        'breed',\n",
        "        'age_category',\n",
        "        'image_quality',\n",
        "        'lighting',\n",
        "        'angle',\n",
        "    }\n",
        "    missing_columns = required_columns.difference(df_cid.columns)\n",
        "    if missing_columns:\n",
        "        raise ValueError(\n",
        "            f\"La metadata del CID Dataset no contiene las columnas requeridas: {sorted(missing_columns)}\"\n",
        "        )\n",
        "\n",
        "    print(\"üìä AN√ÅLISIS EXPLORATORIO - CID DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìà Total im√°genes: {len(df_cid):,}\")\n",
        "    print(f\"üìä Dimensiones: {df_cid.shape}\")\n",
        "\n",
        "    print(\"\\nüìã Columnas disponibles:\")\n",
        "    for col in df_cid.columns:\n",
        "        print(f\"  - {col}\")\n",
        "\n",
        "    print(\"\\n‚öñÔ∏è DISTRIBUCI√ìN DE PESO:\")\n",
        "    print(df_cid['weight_kg'].describe())\n",
        "\n",
        "    print(\"\\nüêÑ DISTRIBUCI√ìN POR RAZA:\")\n",
        "    print(df_cid['breed'].value_counts())\n",
        "\n",
        "    print(\"\\nüì∏ CALIDAD DE IM√ÅGENES:\")\n",
        "    print(df_cid['image_quality'].value_counts())\n",
        "\n",
        "    return df_cid\n",
        "\n",
        "\n",
        "# Ejecutar an√°lisis (requiere metadata real)\n",
        "df_cid = analyze_cid_dataset(CID_METADATA_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACIONES EDA\n",
        "# ============================================================\n",
        "\n",
        "def create_eda_visualizations(df):\n",
        "    \"\"\"Crear visualizaciones completas del EDA\"\"\"\n",
        "    print(\"üìä Creando visualizaciones EDA...\")\n",
        "    \n",
        "    # Configurar subplots\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=2,\n",
        "        subplot_titles=(\n",
        "            'Distribuci√≥n de Peso', 'Peso por Raza',\n",
        "            'Distribuci√≥n por Edad', 'Calidad de Im√°genes',\n",
        "            'Peso vs Iluminaci√≥n', 'Peso vs √Ångulo'\n",
        "        ),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "    )\n",
        "    \n",
        "    # 1. Distribuci√≥n de peso\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=df['weight_kg'], nbinsx=50, name='Peso (kg)',\n",
        "                    marker_color='lightblue', opacity=0.7),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Peso por raza\n",
        "    for breed in df['breed'].unique():\n",
        "        breed_data = df[df['breed'] == breed]['weight_kg']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=breed_data, name=breed, boxpoints='outliers'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "    \n",
        "    # 3. Distribuci√≥n por edad\n",
        "    age_counts = df['age_category'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=age_counts.index, y=age_counts.values, name='Categor√≠as de Edad',\n",
        "               marker_color='lightgreen'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Calidad de im√°genes\n",
        "    quality_counts = df['image_quality'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=quality_counts.index, values=quality_counts.values,\n",
        "               name='Calidad'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # 5. Peso vs Iluminaci√≥n\n",
        "    for lighting in df['lighting'].unique():\n",
        "        lighting_data = df[df['lighting'] == lighting]['weight_kg']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=lighting_data, name=lighting),\n",
        "            row=3, col=1\n",
        "        )\n",
        "    \n",
        "    # 6. Peso vs √Ångulo\n",
        "    for angle in df['angle'].unique():\n",
        "        angle_data = df[df['angle'] == angle]['weight_kg']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=angle_data, name=angle),\n",
        "            row=3, col=2\n",
        "        )\n",
        "    \n",
        "    # Configurar layout\n",
        "    fig.update_layout(\n",
        "        height=1200,\n",
        "        title_text=\"An√°lisis Exploratorio - CID Dataset\",\n",
        "        title_x=0.5,\n",
        "        showlegend=True\n",
        "    )\n",
        "    \n",
        "    # Mostrar gr√°fico\n",
        "    fig.show()\n",
        "    \n",
        "    # Guardar gr√°fico\n",
        "    fig.write_html(DATA_DIR / 'eda_visualizations.html')\n",
        "    print(f\"üíæ Visualizaciones guardadas en: {DATA_DIR / 'eda_visualizations.html'}\")\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Ejecutar visualizaciones\n",
        "eda_fig = create_eda_visualizations(df_cid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AN√ÅLISIS ESPEC√çFICO POR RAZA\n",
        "# ============================================================\n",
        "\n",
        "def analyze_breeds_for_training(df):\n",
        "    \"\"\"Analizar qu√© razas est√°n bien representadas para entrenamiento\"\"\"\n",
        "    print(\"üêÑ AN√ÅLISIS POR RAZA PARA ENTRENAMIENTO\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Razas objetivo del proyecto\n",
        "    target_breeds = ['brahman', 'nelore', 'angus', 'cebuinas', 'criollo', 'pardo_suizo', 'jersey']\n",
        "    \n",
        "    breed_analysis = []\n",
        "    \n",
        "    for breed in target_breeds:\n",
        "        # Buscar razas similares en el dataset\n",
        "        if breed in df['breed'].values:\n",
        "            breed_data = df[df['breed'] == breed]\n",
        "            count = len(breed_data)\n",
        "            avg_weight = breed_data['weight_kg'].mean()\n",
        "            std_weight = breed_data['weight_kg'].std()\n",
        "            \n",
        "            status = \"‚úÖ Suficiente\" if count >= 1000 else \"‚ö†Ô∏è Limitado\" if count >= 100 else \"‚ùå Insuficiente\"\n",
        "            \n",
        "        else:\n",
        "            # Buscar razas similares\n",
        "            similar_breeds = []\n",
        "            if breed in ['brahman', 'nelore', 'cebuinas']:\n",
        "                similar_breeds = ['mixed']  # Bos indicus\n",
        "            elif breed in ['angus']:\n",
        "                similar_breeds = ['mixed']  # Bos taurus\n",
        "            \n",
        "            count = sum(len(df[df['breed'] == sb]) for sb in similar_breeds)\n",
        "            avg_weight = df[df['breed'].isin(similar_breeds)]['weight_kg'].mean() if similar_breeds else 0\n",
        "            std_weight = df[df['breed'].isin(similar_breeds)]['weight_kg'].std() if similar_breeds else 0\n",
        "            \n",
        "            status = \"üîÑ Transfer Learning\" if count >= 1000 else \"‚ùå Recolecci√≥n requerida\"\n",
        "        \n",
        "        breed_analysis.append({\n",
        "            'breed': breed,\n",
        "            'images_available': count,\n",
        "            'avg_weight_kg': round(avg_weight, 1),\n",
        "            'std_weight_kg': round(std_weight, 1),\n",
        "            'status': status,\n",
        "            'strategy': 'Direct training' if count >= 1000 else 'Transfer learning' if count >= 100 else 'Data collection'\n",
        "        })\n",
        "    \n",
        "    # Crear DataFrame\n",
        "    df_breed_analysis = pd.DataFrame(breed_analysis)\n",
        "    \n",
        "    # Mostrar tabla\n",
        "    print(df_breed_analysis.to_string(index=False))\n",
        "    \n",
        "    # Guardar an√°lisis\n",
        "    df_breed_analysis.to_csv(DATA_DIR / 'breed_analysis.csv', index=False)\n",
        "    print(f\"\\nüíæ An√°lisis por raza guardado en: {DATA_DIR / 'breed_analysis.csv'}\")\n",
        "    \n",
        "    # Recomendaciones\n",
        "    print(f\"\\nüéØ RECOMENDACIONES:\")\n",
        "    \n",
        "    sufficient_breeds = df_breed_analysis[df_breed_analysis['images_available'] >= 1000]\n",
        "    if len(sufficient_breeds) > 0:\n",
        "        print(f\"‚úÖ Entrenamiento directo: {', '.join(sufficient_breeds['breed'].tolist())}\")\n",
        "    \n",
        "    transfer_breeds = df_breed_analysis[(df_breed_analysis['images_available'] >= 100) & (df_breed_analysis['images_available'] < 1000)]\n",
        "    if len(transfer_breeds) > 0:\n",
        "        print(f\"üîÑ Transfer learning: {', '.join(transfer_breeds['breed'].tolist())}\")\n",
        "    \n",
        "    collection_breeds = df_breed_analysis[df_breed_analysis['images_available'] < 100]\n",
        "    if len(collection_breeds) > 0:\n",
        "        print(f\"üì∏ Recolecci√≥n requerida: {', '.join(collection_breeds['breed'].tolist())}\")\n",
        "    \n",
        "    return df_breed_analysis\n",
        "\n",
        "# Ejecutar an√°lisis por raza\n",
        "breed_analysis = analyze_breeds_for_training(df_cid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß D√≠a 5-6: Preparar Pipeline de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PIPELINE DE DATOS OPTIMIZADO\n",
        "# ============================================================\n",
        "\n",
        "class CattleDataPipeline:\n",
        "    \"\"\"Pipeline de datos para entrenamiento de modelos de estimaci√≥n de peso\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, breeds_mapping=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.breeds_mapping = breeds_mapping or {}\n",
        "        \n",
        "        # Augmentation agresivo para datasets peque√±os\n",
        "        self.augmentation = A.Compose([\n",
        "            # Variaciones de iluminaci√≥n\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n",
        "            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, p=0.5),\n",
        "            \n",
        "            # Ruido y desenfoque\n",
        "            A.GaussNoise(var_limit=(5, 15), p=0.3),\n",
        "            A.Blur(blur_limit=3, p=0.25),\n",
        "            \n",
        "            # Efectos atmosf√©ricos\n",
        "            A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), p=0.4),\n",
        "            A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.2),\n",
        "            \n",
        "            # Transformaciones geom√©tricas\n",
        "            A.RandomRotate90(p=0.3),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.1, scale_limit=0.15, \n",
        "                rotate_limit=15, border_mode=cv2.BORDER_REFLECT, p=0.5\n",
        "            ),\n",
        "            \n",
        "            # Augmentation espec√≠fico para ganado\n",
        "            A.RandomCrop(height=200, width=200, p=0.3),  # Simular diferentes distancias\n",
        "            A.ElasticTransform(alpha=1, sigma=50, p=0.2),  # Deformaciones naturales\n",
        "            A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),\n",
        "        ])\n",
        "        \n",
        "        print(f\"‚úÖ Pipeline inicializado para: {self.data_dir}\")\n",
        "    \n",
        "    def load_and_preprocess(self, img_path: Path, weight: float) -> tuple[np.ndarray, float]:\n",
        "        \"\"\"Carga imagen, aplica augmentation y retorna tensores listos para el modelo.\"\"\"\n",
        "        if not img_path.exists():\n",
        "            raise FileNotFoundError(f\"Imagen no encontrada: {img_path}\")\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise ValueError(f\"No se pudo cargar la imagen: {img_path}\")\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        augmented = self.augmentation(image=img)\n",
        "        img = augmented['image']\n",
        "\n",
        "        img = cv2.resize(img, CONFIG['image_size'])\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        return img, float(weight)\n",
        "\n",
        "    def create_tf_dataset(self, df, split='train'):\n",
        "        \"\"\"Crea un tf.data.Dataset a partir de rutas reales.\"\"\"\n",
        "        print(f\"üîß Creando dataset TensorFlow para split: {split}\")\n",
        "\n",
        "        required_columns = {'image_path', 'weight_kg'}\n",
        "        missing_columns = required_columns.difference(df.columns)\n",
        "        if missing_columns:\n",
        "            raise ValueError(\n",
        "                f\"El DataFrame para el split '{split}' no contiene las columnas requeridas: {sorted(missing_columns)}\"\n",
        "            )\n",
        "\n",
        "        def data_generator():\n",
        "            for _, row in df.iterrows():\n",
        "                raw_path = Path(row['image_path'])\n",
        "                img_path = raw_path if raw_path.is_absolute() else self.data_dir / raw_path\n",
        "\n",
        "                img, weight = self.load_and_preprocess(img_path, row['weight_kg'])\n",
        "                yield img, weight\n",
        "\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            data_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=CONFIG['image_size'] + (3,), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.float32),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "        if split == 'train':\n",
        "            dataset = dataset.shuffle(1000)\n",
        "\n",
        "        dataset = dataset.batch(CONFIG['batch_size'])\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        print(f\"‚úÖ Dataset {split} creado con optimizaciones\")\n",
        "        return dataset\n",
        "    \n",
        "    def split_data(self, df):\n",
        "        \"\"\"Divide datos en train/val/test\"\"\"\n",
        "        print(\"üìä Dividiendo datos en train/val/test...\")\n",
        "        \n",
        "        # Shuffle datos\n",
        "        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        \n",
        "        # Calcular splits\n",
        "        n_total = len(df_shuffled)\n",
        "        n_train = int(n_total * (1 - CONFIG['validation_split'] - CONFIG['test_split']))\n",
        "        n_val = int(n_total * CONFIG['validation_split'])\n",
        "        \n",
        "        # Dividir\n",
        "        df_train = df_shuffled[:n_train]\n",
        "        df_val = df_shuffled[n_train:n_train + n_val]\n",
        "        df_test = df_shuffled[n_train + n_val:]\n",
        "        \n",
        "        print(f\"üìà Train: {len(df_train):,} ({len(df_train)/n_total*100:.1f}%)\")\n",
        "        print(f\"üìà Val: {len(df_val):,} ({len(df_val)/n_total*100:.1f}%)\")\n",
        "        print(f\"üìà Test: {len(df_test):,} ({len(df_test)/n_total*100:.1f}%)\")\n",
        "        \n",
        "        return df_train, df_val, df_test\n",
        "\n",
        "# Crear pipeline\n",
        "pipeline = CattleDataPipeline(RAW_DIR)\n",
        "\n",
        "# Dividir datos\n",
        "df_train, df_val, df_test = pipeline.split_data(df_cid)\n",
        "\n",
        "# Crear datasets TensorFlow\n",
        "train_dataset = pipeline.create_tf_dataset(df_train, 'train')\n",
        "val_dataset = pipeline.create_tf_dataset(df_val, 'val')\n",
        "test_dataset = pipeline.create_tf_dataset(df_test, 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ARQUITECTURA DEL MODELO\n",
        "# ============================================================\n",
        "\n",
        "def create_weight_estimation_model():\n",
        "    \"\"\"Crear modelo para estimaci√≥n de peso\"\"\"\n",
        "    print(\"üèóÔ∏è Creando arquitectura del modelo...\")\n",
        "    \n",
        "    # Base model con transfer learning\n",
        "    base_model = EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=CONFIG['image_size'] + (3,)\n",
        "    )\n",
        "    \n",
        "    # Congelar capas iniciales\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Custom head para regresi√≥n\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(256, activation='relu', name='dense_1')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu', name='dense_2')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    # Salida: peso estimado en kg\n",
        "    output = layers.Dense(1, activation='linear', name='weight_output')(x)\n",
        "    \n",
        "    # Crear modelo\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "    \n",
        "    # Compilar modelo\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
        "        loss='mse',\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Modelo creado con {model.count_params():,} par√°metros\")\n",
        "    print(f\"üìä Arquitectura: EfficientNetB0 + Custom Head\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Crear modelo\n",
        "model = create_weight_estimation_model()\n",
        "\n",
        "# Mostrar resumen\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURACI√ìN DE ENTRENAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "def setup_training_callbacks():\n",
        "    \"\"\"Configurar callbacks para entrenamiento\"\"\"\n",
        "    print(\"‚öôÔ∏è Configurando callbacks de entrenamiento...\")\n",
        "    \n",
        "    callbacks_list = [\n",
        "        # Early stopping\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=CONFIG['early_stopping_patience'],\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Reduce learning rate on plateau\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Model checkpoint\n",
        "        callbacks.ModelCheckpoint(\n",
        "            filepath=str(MODELS_DIR / 'best_model.h5'),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # TensorBoard\n",
        "        callbacks.TensorBoard(\n",
        "            log_dir=str(BASE_DIR / 'logs'),\n",
        "            histogram_freq=1,\n",
        "            write_graph=True,\n",
        "            write_images=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úÖ {len(callbacks_list)} callbacks configurados\")\n",
        "    return callbacks_list\n",
        "\n",
        "# Configurar callbacks\n",
        "training_callbacks = setup_training_callbacks()\n",
        "\n",
        "# Configurar MLflow\n",
        "def start_mlflow_run():\n",
        "    \"\"\"Iniciar run de MLflow\"\"\"\n",
        "    run = mlflow.start_run(run_name=\"cattle-weight-base-model\")\n",
        "\n",
        "    mlflow.log_params({\n",
        "        'dataset': 'CID',\n",
        "        'model': 'EfficientNetB0',\n",
        "        'batch_size': CONFIG['batch_size'],\n",
        "        'learning_rate': CONFIG['learning_rate'],\n",
        "        'epochs': CONFIG['epochs'],\n",
        "        'image_size': CONFIG['image_size'],\n",
        "        'augmentation': 'Albumentations'\n",
        "    })\n",
        "\n",
        "    print(f\"üî¨ MLflow run iniciado: {run.info.run_id}\")\n",
        "    return run\n",
        "\n",
        "# Iniciar MLflow run\n",
        "mlflow_run = start_mlflow_run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ENTRENAMIENTO DEL MODELO\n",
        "# ============================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Entrenar modelo base\"\"\"\n",
        "    print(\"üöÄ Iniciando entrenamiento del modelo base...\")\n",
        "    print(f\"üìä Configuraci√≥n: {CONFIG}\")\n",
        "    \n",
        "    # Calcular steps por √©poca\n",
        "    steps_per_epoch = len(df_train) // CONFIG['batch_size']\n",
        "    validation_steps = len(df_val) // CONFIG['batch_size']\n",
        "    \n",
        "    print(f\"üìà Steps por √©poca: {steps_per_epoch}\")\n",
        "    print(f\"üìà Validation steps: {validation_steps}\")\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=CONFIG['epochs'],\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=training_callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Entrenamiento completado\")\n",
        "    return history\n",
        "\n",
        "# Entrenamiento real (requiere datasets preparados y tiempo de ejecuci√≥n con GPU)\n",
        "history = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUACI√ìN DEL MODELO\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model():\n",
        "    \"\"\"Evaluar modelo en conjunto de test\"\"\"\n",
        "    print(\"üìä Evaluando modelo en conjunto de test...\")\n",
        "    \n",
        "    # Evaluar modelo\n",
        "    test_loss, test_mae, test_mse = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "    # Calcular R¬≤ real con predicciones sobre el conjunto de test\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for batch_images, batch_targets in test_dataset:\n",
        "        predictions = model.predict(batch_images, verbose=0)\n",
        "        y_true.extend(batch_targets.numpy().astype(float))\n",
        "        y_pred.extend(predictions.squeeze().astype(float))\n",
        "\n",
        "    test_r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"üìà RESULTADOS DE EVALUACI√ìN:\")\n",
        "    print(f\"   Loss: {test_loss:.2f}\")\n",
        "    print(f\"   MAE: {test_mae:.2f} kg\")\n",
        "    print(f\"   MSE: {test_mse:.2f}\")\n",
        "    print(f\"   R¬≤: {test_r2:.3f}\")\n",
        "    \n",
        "    # Verificar objetivos\n",
        "    print(f\"\\nüéØ VERIFICACI√ìN DE OBJETIVOS:\")\n",
        "    print(f\"   R¬≤ ‚â• {CONFIG['target_r2']}: {'‚úÖ' if test_r2 >= CONFIG['target_r2'] else '‚ùå'} ({test_r2:.3f})\")\n",
        "    print(f\"   MAE < {CONFIG['max_mae']} kg: {'‚úÖ' if test_mae < CONFIG['max_mae'] else '‚ùå'} ({test_mae:.2f} kg)\")\n",
        "    \n",
        "    # Log m√©tricas en MLflow\n",
        "    mlflow.log_metrics({\n",
        "        'test_loss': test_loss,\n",
        "        'test_mae': test_mae,\n",
        "        'test_mse': test_mse,\n",
        "        'test_r2': test_r2\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        'loss': test_loss,\n",
        "        'mae': test_mae,\n",
        "        'mse': test_mse,\n",
        "        'r2': test_r2\n",
        "    }\n",
        "\n",
        "# Evaluar modelo\n",
        "evaluation_results = evaluate_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXPORTAR A TFLITE\n",
        "# ============================================================\n",
        "\n",
        "def export_to_tflite(model, output_path):\n",
        "    \"\"\"Exporta modelo a TFLite optimizado para m√≥vil\"\"\"\n",
        "    print(f\"üì± Exportando modelo a TFLite: {output_path}\")\n",
        "    \n",
        "    # Configurar conversor\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    \n",
        "    # Optimizaciones para m√≥vil\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]  # FP16 para velocidad\n",
        "    \n",
        "    # Cuantizaci√≥n INT8 (opcional, m√°s agresiva)\n",
        "    # converter.representative_dataset = representative_data_gen\n",
        "    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    \n",
        "    # Convertir\n",
        "    tflite_model = converter.convert()\n",
        "    \n",
        "    # Guardar\n",
        "    with open(output_path, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    \n",
        "    # Informaci√≥n del modelo\n",
        "    model_size_kb = len(tflite_model) / 1024\n",
        "    model_size_mb = model_size_kb / 1024\n",
        "    print(\"‚úÖ Modelo exportado exitosamente\")\n",
        "    print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
        "    print(\"üì± Optimizado para m√≥vil: FP16\")\n",
        "    \n",
        "    # Log en MLflow\n",
        "    mlflow.log_artifact(output_path)\n",
        "    mlflow.log_metric('model_size_kb', model_size_kb)\n",
        "    mlflow.log_metric('model_size_mb', model_size_mb)\n",
        "    \n",
        "    return model_size_kb\n",
        "\n",
        "# Exportar modelo base\n",
        "tflite_path = MODELS_DIR / 'generic-cattle-v1.0.0.tflite'\n",
        "model_size = export_to_tflite(model, tflite_path)\n",
        "\n",
        "print(\"\\nüéØ MODELO BASE LISTO PARA INTEGRACI√ìN\")\n",
        "print(f\"üìÅ Archivo: {tflite_path}\")\n",
        "print(f\"üìè Tama√±o: {model_size / 1024:.2f} MB ({model_size:.1f} KB)\")\n",
        "print(f\"üî¨ MLflow run: {mlflow_run.info.run_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Resumen y Pr√≥ximos Pasos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RESUMEN FINAL\n",
        "# ============================================================\n",
        "\n",
        "def generate_final_summary():\n",
        "    \"\"\"Generar resumen final del trabajo realizado\"\"\"\n",
        "    print(\"üìã RESUMEN FINAL - PERSONA 2: SETUP ML\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Resumen de datasets\n",
        "    print(f\"\\nüì• DATASETS PROCESADOS:\")\n",
        "    cid_row = datasets_summary[datasets_summary['name'] == 'CID Dataset']\n",
        "    cid_images = int(cid_row['images'].iloc[0]) if not cid_row.empty else 0\n",
        "    print(f\"   {'‚úÖ' if cid_images else '‚ö†Ô∏è'} CID Dataset: {cid_images:,} im√°genes\")\n",
        "    print(f\"   {'‚úÖ' if scraped_images else '‚ö†Ô∏è'} Google Images: {scraped_images:,} im√°genes locales\")\n",
        "\n",
        "    if kaggle_dataset_path and kaggle_dataset_path.exists():\n",
        "        kaggle_images = len(list(kaggle_dataset_path.glob('**/*.jpg')))\n",
        "        status_icon = '‚úÖ' if kaggle_images else '‚ö†Ô∏è'\n",
        "        print(f\"   {status_icon} Kaggle Dataset ({KAGGLE_DATASET_ID}): {kaggle_images:,} im√°genes\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è Kaggle Dataset: Pendiente configuraci√≥n (sube kaggle.json y ejecuta la celda correspondiente)\")\n",
        "    \n",
        "    # Resumen de an√°lisis\n",
        "    print(f\"\\nüìä AN√ÅLISIS COMPLETADO:\")\n",
        "    print(f\"   ‚úÖ EDA completo con visualizaciones\")\n",
        "    print(f\"   ‚úÖ An√°lisis por raza para estrategia de entrenamiento\")\n",
        "    print(f\"   ‚úÖ Pipeline de datos optimizado\")\n",
        "    \n",
        "    # Resumen de modelo\n",
        "    print(f\"\\nü§ñ MODELO BASE:\")\n",
        "    print(f\"   ‚úÖ Arquitectura: EfficientNetB0 + Custom Head\")\n",
        "    print(f\"   ‚úÖ Par√°metros: {model.count_params():,}\")\n",
        "    print(f\"   ‚úÖ TFLite exportado: {model_size / 1024:.2f} MB ({model_size:.1f} KB)\")\n",
        "    print(f\"   ‚úÖ MLflow tracking: {mlflow_run.info.run_id}\")\n",
        "    \n",
        "    # Pr√≥ximos pasos\n",
        "    print(f\"\\nüéØ PR√ìXIMOS PASOS:\")\n",
        "    print(f\"   1. üîÑ Fine-tuning por raza (Semanas 3-6)\")\n",
        "    print(f\"   2. üì∏ Recolecci√≥n Criollo + Pardo Suizo (Semanas 7-8)\")\n",
        "    print(f\"   3. üß™ Entrenamiento final (Semanas 9-10)\")\n",
        "    print(f\"   4. üì± Integraci√≥n en app m√≥vil\")\n",
        "    \n",
        "    # Guardar resumen\n",
        "    summary_data = {\n",
        "        'completion_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'datasets_processed': len(datasets_summary),\n",
        "        'total_images': datasets_summary['images'].sum(),\n",
        "        'model_architecture': 'EfficientNetB0',\n",
        "        'model_size_kb': model_size,\n",
        "        'mlflow_run_id': mlflow_run.info.run_id,\n",
        "        'status': 'COMPLETADO'\n",
        "    }\n",
        "    \n",
        "    with open(DATA_DIR / 'final_summary.json', 'w') as f:\n",
        "        json.dump(summary_data, f, indent=2)\n",
        "\n",
        "    mlflow.end_run()\n",
        "    \n",
        "    print(f\"\\nüíæ Resumen guardado en: {DATA_DIR / 'final_summary.json'}\")\n",
        "    print(f\"\\nüéâ PERSONA 2: SETUP ML COMPLETADO EXITOSAMENTE\")\n",
        "\n",
        "# Generar resumen final\n",
        "generate_final_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Notas Importantes\n",
        "\n",
        "### ‚ö†Ô∏è Configuraci√≥n Requerida\n",
        "1. **Kaggle API**: Subir `kaggle.json` para descargar datasets\n",
        "2. **CID Dataset**: Reemplazar URL simulada con URL real\n",
        "3. **CattleEyeView**: Solicitar acceso a autores del paper\n",
        "\n",
        "### üîß Optimizaciones Implementadas\n",
        "- **Mixed Precision**: FP16 para acelerar entrenamiento\n",
        "- **Data Pipeline**: Cache + prefetch + shuffle optimizado\n",
        "- **Augmentation**: Albumentations espec√≠fico para ganado\n",
        "- **TFLite Export**: Optimizado para m√≥vil\n",
        "\n",
        "### üìä M√©tricas Objetivo\n",
        "- **R¬≤ ‚â• 0.95**: Explicaci√≥n 95% de varianza\n",
        "- **MAE < 5 kg**: Error absoluto promedio\n",
        "- **Inference < 3s**: Tiempo en m√≥vil\n",
        "\n",
        "### üéØ Estado Actual\n",
        "- ‚úÖ **Infraestructura ML**: Completada\n",
        "- ‚úÖ **Pipeline de datos**: Optimizado\n",
        "- ‚úÖ **Modelo base**: Listo para fine-tuning\n",
        "- üîÑ **Pr√≥ximo**: Fine-tuning por raza espec√≠fica\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

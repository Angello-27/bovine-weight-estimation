{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêÑ Sistema de Estimaci√≥n de Peso Bovino - Setup ML\n",
        "\n",
        "> **BLOQUE 0**: Informaci√≥n del proyecto (markdown - solo lectura)\n",
        "\n",
        "**Proyecto**: Hacienda Gamelera - Bruno Brito Macedo  \n",
        "**Responsable**: Persona 2 - Setup Infraestructura ML  \n",
        "**Objetivo**: Preparar datasets y pipeline para entrenamiento de 7 modelos por raza  \n",
        "**Duraci√≥n**: 5-6 d√≠as  \n",
        "\n",
        "---\n",
        "\n",
        "## üìë √çndice de Bloques (Referencia R√°pida)\n",
        "\n",
        "| Bloque | Nombre | Descripci√≥n | Requisitos |\n",
        "|--------|--------|-------------|------------|\n",
        "| **0** | Informaci√≥n | Markdown introductorio | Ninguno |\n",
        "| **1** | Clonar Repositorio | Monta Drive y clona desde GitHub (persistente) | Ninguno (requiere internet) |\n",
        "| **2** | Importar M√≥dulos | Importa m√≥dulos internos | Bloque 1 |\n",
        "| **3** | Ejemplo Modelo | Bloque de prueba (opcional) | Bloque 2 |\n",
        "| **3.5** | Limpieza Dependencias | Limpia conflictos y reinstala TensorFlow 2.19.0 | Ninguno |\n",
        "| **4** | Instalar Dependencias Cr√≠ticas | TensorFlow 2.19.0, NumPy 2.x, MLflow, ml_dtypes | Bloque 3.5 |\n",
        "| **5** | Instalar Complementos | Albumentations, OpenCV, herramientas ML y verificaciones | Bloque 4 |\n",
        "| **6** | Imports Generales | Pandas, numpy, tensorflow | Bloque 5 |\n",
        "| **7** | Configuraci√≥n Proyecto | Crea carpetas en Drive (Drive ya montado en Bloque 1) | Bloque 6 |\n",
        "| **7.5** | Configurar Variables CID | Configura rutas CID Dataset | Bloque 7 (opcional) |\n",
        "| **8** | CID Dataset | Extrae CID Dataset (OPCIONAL) | Bloque 7.5 + archivo comprimido (opcional) |\n",
        "| **8.5** | Configurar Kaggle.json | Copia kaggle.json desde Drive | Bloque 7 (opcional) |\n",
        "| **9** | Kaggle Dataset | Descarga dataset Kaggle | Bloque 8.5 + kaggle.json |\n",
        "| **10** | Google Images | Scraping opcional | Bloque 7 (opcional) |\n",
        "| **11** | Resumen Datasets | Muestra resumen | Bloques 8-10 |\n",
        "| **12** | EDA CID Dataset | An√°lisis exploratorio (OPCIONAL) | Bloque 8 + metadata.csv (opcional) |\n",
        "| **13** | Visualizaciones EDA | Gr√°ficos interactivos | Bloque 12 |\n",
        "| **14** | An√°lisis por Raza | An√°lisis por raza | Bloque 12 |\n",
        "| **15** | Pipeline de Datos | Pipeline con augmentation | Bloque 12 |\n",
        "| **16** | Arquitectura Modelo | Crea modelo EfficientNetB0 | Bloque 15 |\n",
        "| **17** | Configurar Entrenamiento | Callbacks y MLflow | Bloque 16 |\n",
        "| **18** | Entrenamiento | Entrena modelo (2-4h) | Bloque 17 + GPU |\n",
        "| **19** | Evaluaci√≥n | Eval√∫a modelo (R¬≤, MAE) | Bloque 18 |\n",
        "| **20** | Exportar TFLite | Exporta a TFLite | Bloque 19 |\n",
        "| **21** | Resumen Final | Genera resumen completo | Todos los bloques |\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Checklist de Tareas\n",
        "- [x] D√≠a 1: Setup Google Colab Pro + dependencias\n",
        "- [ ] D√≠a 2-3: Descargar y organizar datasets cr√≠ticos\n",
        "- [ ] D√≠a 4: An√°lisis exploratorio de datos (EDA)\n",
        "- [ ] D√≠a 5-6: Preparar pipeline de datos optimizado\n",
        "\n",
        "## üéØ Razas Objetivo (7 razas)\n",
        "1. **Brahman** - Bos indicus robusto\n",
        "2. **Nelore** - Bos indicus\n",
        "3. **Angus** - Bos taurus, buena carne\n",
        "4. **Cebuinas** - Bos indicus general\n",
        "5. **Criollo** - Adaptado local\n",
        "6. **Pardo Suizo** - Bos taurus grande\n",
        "7. **Jersey** - Lechera, menor tama√±o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 1: CONFIGURAR RUTA DEL PROYECTO Y CLONAR REPOSITORIO EN DRIVE\n",
        "# ============================================================\n",
        "# üìÅ Clona el repositorio desde GitHub a Google Drive (persistente entre sesiones)\n",
        "# üîó Repositorio: https://github.com/Angello-27/bovine-weight-estimation.git\n",
        "# üíæ Se clona en Drive para que persista entre desconexiones del runtime\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# üîó URL del repositorio de GitHub\n",
        "GITHUB_REPO_URL = 'https://github.com/Angello-27/bovine-weight-estimation.git'\n",
        "\n",
        "# üîë Montar Google Drive primero (si no est√° montado)\n",
        "print(\"üîó Montando Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    \n",
        "    # Verificar si Drive ya est√° montado\n",
        "    drive_path = Path('/content/drive')\n",
        "    if not drive_path.exists() or not any(drive_path.iterdir()):\n",
        "        print(\"üìÅ Google Drive no est√° montado. Montando ahora...\")\n",
        "        print(\"üí° Se solicitar√° autorizaci√≥n para acceder a Google Drive\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive montado exitosamente\")\n",
        "    else:\n",
        "        print(\"‚úÖ Google Drive ya est√° montado\")\n",
        "    \n",
        "    # Usar Drive como ubicaci√≥n predeterminada (persistente)\n",
        "    DRIVE_BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
        "    USE_DRIVE = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è No se puede montar Google Drive (no estamos en Colab)\")\n",
        "    print(\"üí° Usando /content/ como ubicaci√≥n temporal\")\n",
        "    DRIVE_BASE_DIR = None\n",
        "    USE_DRIVE = False\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error al montar Google Drive: {e}\")\n",
        "    print(\"üí° Usando /content/ como ubicaci√≥n temporal\")\n",
        "    DRIVE_BASE_DIR = None\n",
        "    USE_DRIVE = False\n",
        "\n",
        "# ‚úÖ Ruta donde se clonar√° o est√° el repositorio\n",
        "#    - Preferencia 1: Google Drive (persistente entre sesiones)\n",
        "#    - Preferencia 2: /content/ (temporal, se pierde al desconectar)\n",
        "if USE_DRIVE and DRIVE_BASE_DIR:\n",
        "    BASE_DIR = DRIVE_BASE_DIR\n",
        "    print(f\"\\nüíæ Usando Google Drive: {BASE_DIR} (persistente entre sesiones)\")\n",
        "else:\n",
        "    BASE_DIR = Path('/content/bovine-weight-estimation')\n",
        "    print(f\"\\n‚ö†Ô∏è Usando ubicaci√≥n temporal: {BASE_DIR} (se pierde al desconectar)\")\n",
        "    print(\"üí° Para persistir, monta Google Drive primero\")\n",
        "\n",
        "# Ruta del proyecto ML Training\n",
        "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
        "\n",
        "# Validamos que la estructura del proyecto exista antes de continuar.\n",
        "if ML_TRAINING_DIR.exists() and (ML_TRAINING_DIR / 'src').exists():\n",
        "    print(f\"\\n‚úÖ Proyecto ya existe en: {ML_TRAINING_DIR}\")\n",
        "    print(\"üìÇ Subcarpetas clave detectadas:\")\n",
        "    print(f\"   - C√≥digo fuente: {ML_TRAINING_DIR / 'src'}\")\n",
        "    print(f\"   - Scripts utilitarios: {ML_TRAINING_DIR / 'scripts'}\")\n",
        "    print(f\"   - Configuraci√≥n: {ML_TRAINING_DIR / 'config'}\")\n",
        "    if USE_DRIVE:\n",
        "        print(f\"\\nüíæ El proyecto persiste entre sesiones porque est√° en Google Drive\")\n",
        "else:\n",
        "    print(f\"\\nüì• Proyecto no encontrado en: {ML_TRAINING_DIR}\")\n",
        "    print(f\"üîó Clonando repositorio desde GitHub: {GITHUB_REPO_URL}\")\n",
        "    if USE_DRIVE:\n",
        "        print(f\"üíæ Se clonar√° en Google Drive para que persista entre sesiones\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Se clonar√° en /content/ (temporal, se perder√° al desconectar)\")\n",
        "    \n",
        "    # Clonar repositorio si no existe\n",
        "    try:\n",
        "        # Crear directorio padre si no existe\n",
        "        BASE_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Eliminar directorio si existe pero est√° vac√≠o o incompleto\n",
        "        if BASE_DIR.exists() and not (ML_TRAINING_DIR / 'src').exists():\n",
        "            print(f\"‚ö†Ô∏è Directorio existe pero incompleto. Eliminando {BASE_DIR}...\")\n",
        "            import shutil\n",
        "            shutil.rmtree(BASE_DIR, ignore_errors=True)\n",
        "        \n",
        "        # Clonar repositorio\n",
        "        print(f\"üì• Clonando repositorio...\")\n",
        "        result = subprocess.run(\n",
        "            ['git', 'clone', GITHUB_REPO_URL, str(BASE_DIR)],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Repositorio clonado exitosamente en: {BASE_DIR}\")\n",
        "        print(f\"üìÇ Estructura del proyecto:\")\n",
        "        print(f\"   - C√≥digo fuente: {ML_TRAINING_DIR / 'src'}\")\n",
        "        print(f\"   - Scripts utilitarios: {ML_TRAINING_DIR / 'scripts'}\")\n",
        "        print(f\"   - Configuraci√≥n: {ML_TRAINING_DIR / 'config'}\")\n",
        "        \n",
        "        if USE_DRIVE:\n",
        "            print(f\"\\nüíæ El proyecto est√° en Google Drive y persistir√° entre sesiones\")\n",
        "            print(f\"üí° No necesitar√°s volver a clonarlo en futuras sesiones\")\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è El proyecto est√° en /content/ y se perder√° al desconectar el runtime\")\n",
        "            print(f\"üí° Considera montar Google Drive y volver a ejecutar este bloque\")\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error al clonar repositorio: {e}\")\n",
        "        print(f\"   stdout: {e.stdout}\")\n",
        "        print(f\"   stderr: {e.stderr}\")\n",
        "        print(\"\\nüí° Soluciones:\")\n",
        "        print(\"   1. Verifica que tienes conexi√≥n a internet en Colab\")\n",
        "        print(\"   2. Verifica que el repositorio existe: https://github.com/Angello-27/bovine-weight-estimation\")\n",
        "        print(\"   3. Si el repositorio es privado, ejecuta el BLOQUE 0.5 primero para configurar el token\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error inesperado: {e}\")\n",
        "        raise\n",
        "\n",
        "# A√±adimos la carpeta src al PYTHONPATH para que todos los m√≥dulos internos sean importables.\n",
        "sys.path.insert(0, str(ML_TRAINING_DIR / 'src'))\n",
        "\n",
        "# Verificaci√≥n final\n",
        "if ML_TRAINING_DIR.exists() and (ML_TRAINING_DIR / 'src').exists():\n",
        "    print(f\"\\n‚úÖ Configuraci√≥n completada correctamente\")\n",
        "    print(f\"üìÅ Directorio base: {BASE_DIR}\")\n",
        "    print(f\"üìÅ ML Training: {ML_TRAINING_DIR}\")\n",
        "    print(f\"üêç PYTHONPATH actualizado: {ML_TRAINING_DIR / 'src'}\")\n",
        "    if USE_DRIVE:\n",
        "        print(f\"üíæ Ubicaci√≥n: Google Drive (persistente entre sesiones)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Ubicaci√≥n: /content/ (temporal, se pierde al desconectar)\")\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        f\"No se pudo configurar el proyecto. Verifica que {ML_TRAINING_DIR} existe y contiene 'src'.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 2: IMPORTAR M√ìDULOS DEL PROYECTO\n",
        "# ============================================================\n",
        "# ‚úÖ Importa m√≥dulos internos del proyecto (requiere BLOQUE 1 exitoso)\n",
        "\n",
        "# Data Augmentation\n",
        "from data.augmentation import get_training_transform, get_aggressive_augmentation, get_validation_transform\n",
        "\n",
        "# Modelos\n",
        "from models.cnn_architecture import BreedWeightEstimatorCNN, BREED_CONFIGS\n",
        "\n",
        "# Evaluaci√≥n\n",
        "from models.evaluation.metrics import MetricsCalculator, ModelMetrics\n",
        "\n",
        "# Exportaci√≥n TFLite\n",
        "from models.export.tflite_converter import TFLiteExporter\n",
        "\n",
        "print(\"‚úÖ Todos los m√≥dulos importados correctamente\")\n",
        "print(\"\\nüì¶ M√≥dulos disponibles:\")\n",
        "print(\"   - Data augmentation (Albumentations 2.0.8)\")\n",
        "print(\"   - CNN architectures (MobileNetV2, EfficientNet)\")\n",
        "print(\"   - Metrics calculator (R¬≤, MAE, MAPE)\")\n",
        "print(\"   - TFLite exporter (optimizado para m√≥vil)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 3: EJEMPLO - CREAR MODELO PARA UNA RAZA (OPCIONAL)\n",
        "# ============================================================\n",
        "# üéì Bloque de prueba para verificar que los m√≥dulos funcionan\n",
        "# ‚ö†Ô∏è Puedes omitir este bloque si ya sabes que todo funciona\n",
        "\n",
        "# Ejemplo 1: Crear modelo para Brahman\n",
        "model_brahman = BreedWeightEstimatorCNN.build_model(\n",
        "    breed_name='brahman',\n",
        "    base_architecture='mobilenetv2'  # M√°s r√°pido que EfficientNet\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Modelo creado: {model_brahman.name}\")\n",
        "print(f\"üìä Par√°metros: {model_brahman.count_params():,}\")\n",
        "\n",
        "# Ver arquitectura\n",
        "print(\"\\nüìê Arquitectura del modelo:\")\n",
        "model_brahman.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Pr√≥ximos Pasos\n",
        "\n",
        "1. **Descargar datasets** (CID, CattleEyeView, etc.)\n",
        "2. **Preprocesar datos** con nuestros m√≥dulos\n",
        "3. **Entrenar modelo base** gen√©rico\n",
        "4. **Fine-tuning por raza** (5 razas)\n",
        "5. **Recolecci√≥n propia** (Criollo, Pardo Suizo)\n",
        "6. **Exportar a TFLite** e integrar en app m√≥vil\n",
        "\n",
        "> Ver `README.md` y `scripts/train_all_breeds.py` para m√°s ejemplos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## üöÄ D√≠a 1: Setup Google Colab Pro + Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 3.5: LIMPIEZA DE DEPENDENCIAS CONFLICTIVAS\n",
        "# ============================================================\n",
        "# üßπ Limpia versiones antiguas/conflictivas de TensorFlow y dependencias\n",
        "# ‚ö†Ô∏è Ejecuta este bloque ANTES del BLOQUE 4\n",
        "# üí° Esto evitar√° advertencias de compatibilidad en pip\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üßπ INICIANDO LIMPIEZA DE DEPENDENCIAS CONFLICTIVAS...\\n\")\n",
        "\n",
        "# Paso 1: Limpiar cach√© de pip\n",
        "print(\"üì¶ Limpiando cach√© de pip...\")\n",
        "!pip cache purge\n",
        "print(\"   ‚úÖ Cach√© de pip limpiado\\n\")\n",
        "\n",
        "# Paso 2: Desinstalar versiones antiguas de TensorFlow\n",
        "print(\"üì¶ Desinstalando versiones antiguas de TensorFlow...\")\n",
        "!pip uninstall -y -q tensorflow tensorflow-gpu tf-keras 2>/dev/null || true\n",
        "print(\"   ‚úÖ TensorFlow antiguo desinstalado\\n\")\n",
        "\n",
        "# Paso 3: Desinstalar paquetes problem√°ticos que dependen de versiones espec√≠ficas\n",
        "print(\"üì¶ Desinstalando paquetes con dependencias r√≠gidas...\")\n",
        "packages_to_remove = [\n",
        "    \"tensorflow-decision-forests\",\n",
        "    \"dopamine-rl\", \n",
        "    \"tensorflow-text\",\n",
        "    \"ydf\"\n",
        "]\n",
        "\n",
        "for package in packages_to_remove:\n",
        "    print(f\"   - Desinstalando {package}...\")\n",
        "    !pip uninstall -y -q {package} 2>/dev/null || true\n",
        "\n",
        "print(\"   ‚úÖ Paquetes problem√°ticos desinstalados\\n\")\n",
        "\n",
        "# Paso 4: Reinstalar TensorFlow 2.19 (versi√≥n de Colab)\n",
        "print(\"üì¶ Reinstalando TensorFlow 2.19.0 (versi√≥n limpia)...\")\n",
        "!pip install -q --force-reinstall --no-cache-dir \"tensorflow==2.19.0\"\n",
        "print(\"   ‚úÖ TensorFlow 2.19.0 reinstalado limpiamente\\n\")\n",
        "\n",
        "# Paso 5: Actualizar ml_dtypes y protobuf a versiones compatibles\n",
        "print(\"üì¶ Actualizando ml_dtypes y protobuf...\")\n",
        "!pip install -q --upgrade --no-cache-dir \"ml_dtypes>=0.5.0\"\n",
        "!pip install -q --upgrade --no-cache-dir \"protobuf>=5.26.1,<6.0\"\n",
        "print(\"   ‚úÖ ml_dtypes y protobuf actualizados\\n\")\n",
        "\n",
        "# Paso 6: Verificar instalaci√≥n limpia\n",
        "print(\"üîç Verificando instalaci√≥n limpia...\")\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import ml_dtypes\n",
        "    ml_dtypes_version = ml_dtypes.__version__\n",
        "except:\n",
        "    ml_dtypes_version = \"No disponible\"\n",
        "\n",
        "try:\n",
        "    import google.protobuf\n",
        "    protobuf_version = google.protobuf.__version__\n",
        "except:\n",
        "    protobuf_version = \"No disponible\"\n",
        "\n",
        "print(\"\\n‚úÖ ESTADO DESPU√âS DE LA LIMPIEZA:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "print(f\"   - NumPy: {np.__version__}\")\n",
        "print(f\"   - ml_dtypes: {ml_dtypes_version}\")\n",
        "print(f\"   - Protobuf: {protobuf_version}\")\n",
        "\n",
        "print(\"\\n‚úÖ LIMPIEZA COMPLETADA EXITOSAMENTE\")\n",
        "print(\"üí° Ahora ejecuta el BLOQUE 4 para instalar las dependencias sin conflictos\")\n",
        "print(\"üí° Si a√∫n hay advertencias menores, puedes ignorarlas - no afectar√°n el funcionamiento\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 4: INSTALACI√ìN DE DEPENDENCIAS CR√çTICAS (VERSI√ìN LIMPIA)\n",
        "# ============================================================\n",
        "# üîß Instala dependencias cr√≠ticas despu√©s de la limpieza del BLOQUE 3.5\n",
        "# ‚ö†Ô∏è Ejecuta SOLO despu√©s del BLOQUE 3.5\n",
        "# ‚úÖ Sin conflictos de versiones\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üì¶ INSTALANDO DEPENDENCIAS CR√çTICAS (ENTORNO LIMPIO)...\\n\")\n",
        "\n",
        "# Verificar que TensorFlow est√° limpio\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(f\"üîç Verificando versiones base:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "print(f\"   - NumPy: {np.__version__}\\n\")\n",
        "\n",
        "# Paso 1: Instalar tf-keras (evita advertencia de tensorflow-hub)\n",
        "print(\"üì¶ Instalando tf-keras...\")\n",
        "!pip install -q --no-cache-dir \"tf-keras>=2.19.0\"\n",
        "print(\"   ‚úÖ tf-keras instalado\\n\")\n",
        "\n",
        "# Paso 2: Arreglar dependencias menores conflictivas\n",
        "print(\"üì¶ Corrigiendo dependencias menores...\")\n",
        "!pip install -q --no-cache-dir \"packaging<25\"  # Requerido por MLflow\n",
        "!pip install -q --no-cache-dir \"wrapt<2.0.0,>=1.10.10\"  # Requerido por aiobotocore\n",
        "!pip install -q --no-cache-dir \"requests==2.32.4\"  # Requerido por google-colab\n",
        "!pip install -q --no-cache-dir \"jedi>=0.16\"  # Requerido por IPython\n",
        "print(\"   ‚úÖ Dependencias menores corregidas\\n\")\n",
        "\n",
        "# Paso 3: Instalar MLflow (compatible con NumPy 2.x y Protobuf 5.x)\n",
        "# MLflow 2.14.1 es compatible con NumPy 2.x a pesar de sus restricciones\n",
        "print(\"üì¶ Instalando MLflow (compatible con NumPy 2.x y Protobuf 5.x)...\")\n",
        "!pip install -q --no-cache-dir \"mlflow==2.14.1\"\n",
        "print(\"   ‚úÖ MLflow instalado\\n\")\n",
        "\n",
        "# Paso 4: Instalar DVC (opcional, para versionado de datos)\n",
        "print(\"üì¶ Instalando DVC...\")\n",
        "!pip install -q --no-cache-dir \"dvc[gs,s3]==3.51.1\"\n",
        "print(\"   ‚úÖ DVC instalado\\n\")\n",
        "\n",
        "# Paso 5: Actualizar scikit-learn\n",
        "print(\"üì¶ Actualizando scikit-learn...\")\n",
        "!pip install -q --upgrade --no-cache-dir \"scikit-learn>=1.6\"\n",
        "print(\"   ‚úÖ Scikit-learn actualizado\\n\")\n",
        "\n",
        "# Configurar mixed precision para GPU\n",
        "print(\"üîç Configurando TensorFlow...\")\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"   ‚úÖ Mixed precision (FP16) activado\\n\")\n",
        "\n",
        "# Verificar versiones finales\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ DEPENDENCIAS CR√çTICAS INSTALADAS (ENTORNO LIMPIO):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüì¶ VERSIONES INSTALADAS:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "print(f\"   - NumPy: {np.__version__}\")\n",
        "\n",
        "try:\n",
        "    import mlflow\n",
        "    print(f\"   - MLflow: {mlflow.__version__} ‚úÖ\")\n",
        "    mlflow_ok = True\n",
        "except Exception as e:\n",
        "    print(f\"   - MLflow: Error - {e} ‚ùå\")\n",
        "    mlflow_ok = False\n",
        "\n",
        "try:\n",
        "    import sklearn\n",
        "    print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
        "except:\n",
        "    print(f\"   - Scikit-learn: No disponible\")\n",
        "\n",
        "try:\n",
        "    import google.protobuf\n",
        "    print(f\"   - Protobuf: {google.protobuf.__version__}\")\n",
        "except:\n",
        "    print(f\"   - Protobuf: No disponible\")\n",
        "\n",
        "try:\n",
        "    import ml_dtypes\n",
        "    print(f\"   - ml_dtypes: {ml_dtypes.__version__}\")\n",
        "except:\n",
        "    print(f\"   - ml_dtypes: No disponible\")\n",
        "\n",
        "try:\n",
        "    from tensorflow import keras\n",
        "    print(f\"   - tf-keras: {keras.__version__}\")\n",
        "except:\n",
        "    print(f\"   - tf-keras: No disponible\")\n",
        "\n",
        "# Verificar compatibilidad\n",
        "print(f\"\\nüîç VERIFICACI√ìN DE COMPATIBILIDAD:\")\n",
        "tf_version_ok = tf.__version__.startswith('2.19')\n",
        "numpy_version_ok = np.__version__.startswith('2.0') or np.__version__.startswith('2.1')\n",
        "\n",
        "print(f\"   - TensorFlow 2.19.x: {'‚úÖ' if tf_version_ok else '‚ö†Ô∏è'}\")\n",
        "print(f\"   - NumPy 2.x: {'‚úÖ' if numpy_version_ok else '‚ö†Ô∏è'}\")\n",
        "print(f\"   - MLflow funcional: {'‚úÖ' if mlflow_ok else '‚ùå'}\")\n",
        "\n",
        "if tf_version_ok and numpy_version_ok and mlflow_ok:\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ INSTALACI√ìN COMPLETADA EXITOSAMENTE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nüí° Contin√∫a con el BLOQUE 5 para instalar complementos\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Hay problemas de compatibilidad. Verifica los errores anteriores.\")\n",
        "\n",
        "print(f\"\\nüìù NOTAS:\")\n",
        "print(f\"   - Entorno limpio sin paquetes conflictivos cr√≠ticos\")\n",
        "print(f\"   - TensorFlow 2.19 + NumPy 2.x + MLflow 2.14.1\")\n",
        "print(f\"   - Mixed precision (FP16) configurado para GPU\")\n",
        "print(f\"   - Dependencias menores ajustadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 5: INSTALACI√ìN DE COMPLEMENTOS Y VERIFICACIONES FINALES\n",
        "# ============================================================\n",
        "# üîß Instala complementos: Albumentations, OpenCV, herramientas ML, etc.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üì¶ INSTALANDO COMPLEMENTOS...\\n\")\n",
        "\n",
        "# Paso 1: Instalar Albumentations (compatible con NumPy 2.x)\n",
        "print(\"üì¶ Instalando Albumentations...\")\n",
        "!pip install -q --no-cache-dir \"albumentations>=2.0.8\"\n",
        "print(\"   ‚úÖ Albumentations instalado\\n\")\n",
        "\n",
        "# Paso 2: OpenCV ya viene preinstalado en Colab, verificar\n",
        "print(\"üì¶ Verificando OpenCV...\")\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"   ‚úÖ OpenCV {cv2.__version__} ya instalado\\n\")\n",
        "except:\n",
        "    print(\"   - Instalando OpenCV...\")\n",
        "    !pip install -q --no-cache-dir \"opencv-python-headless\"\n",
        "    print(\"   ‚úÖ OpenCV instalado\\n\")\n",
        "\n",
        "# Paso 3: Instalar herramientas de ML y datos\n",
        "print(\"üì¶ Instalando herramientas adicionales...\")\n",
        "!pip install -q --no-cache-dir kaggle gdown plotly seaborn\n",
        "print(\"   ‚úÖ Herramientas instaladas\\n\")\n",
        "\n",
        "# Paso 4: Instalar Pillow actualizado\n",
        "print(\"üì¶ Actualizando Pillow...\")\n",
        "!pip install -q --upgrade --no-cache-dir \"pillow>=11.0.0\"\n",
        "print(\"   ‚úÖ Pillow actualizado\\n\")\n",
        "\n",
        "# Verificar versiones instaladas\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç VERIFICANDO COMPLEMENTOS INSTALADOS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import sklearn\n",
        "\n",
        "print(f\"\\nüì¶ COMPLEMENTOS:\")\n",
        "print(f\"   - NumPy: {np.__version__}\")\n",
        "print(f\"   - OpenCV: {cv2.__version__}\")\n",
        "print(f\"   - Albumentations: {A.__version__}\")\n",
        "print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
        "\n",
        "# Verificar TensorFlow y GPU\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "print(f\"\\nüì¶ TENSORFLOW:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "\n",
        "# Configurar GPU\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(f\"\\n‚úÖ GPU DETECTADA Y CONFIGURADA:\")\n",
        "        print(f\"   - Dispositivos GPU: {len(gpus)}\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"   - GPU {i}: {gpu.name}\")\n",
        "        print(\"\\nüéÆ GPU lista para entrenamiento\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n‚ö†Ô∏è Error configurando GPU: {e}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è NO SE DETECT√ì GPU\")\n",
        "    print(\"üí° Activa GPU: Entorno de ejecuci√≥n > Cambiar tipo > GPU\")\n",
        "\n",
        "print(f\"\\n‚úÖ Mixed precision (FP16) activado\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ TODAS LAS DEPENDENCIAS INSTALADAS CORRECTAMENTE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüí° Contin√∫a con el BLOQUE 6 (Imports Generales)\")\n",
        "print(f\"\\nüìù Tu entorno est√° listo para:\")\n",
        "print(f\"   - Entrenar modelos CNN (MobileNetV2, EfficientNet)\")\n",
        "print(f\"   - Data augmentation con Albumentations\")\n",
        "print(f\"   - Tracking con MLflow\")\n",
        "print(f\"   - Exportaci√≥n a TFLite para m√≥vil\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 6: IMPORTS Y CONFIGURACI√ìN GENERAL\n",
        "# ============================================================\n",
        "# üîç Importa todas las librer√≠as necesarias (pandas, numpy, tensorflow, mlflow, etc.)\n",
        "\n",
        "# üîç Conjunto completo de librer√≠as usadas en el pipeline: utilidades del sistema,\n",
        "#    ciencia de datos, visualizaci√≥n, ML y tracking de experimentos.\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from pathlib import Path\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# MLflow para tracking reproducible de experimentos.\n",
        "# Verificar y corregir versi√≥n de MLflow si es necesario\n",
        "\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.tensorflow\n",
        "    mlflow_ok = True\n",
        "    mlflow_version = mlflow.__version__\n",
        "except ImportError as e:\n",
        "    if '_shut_down_async_logging' in str(e) or 'cannot import name' in str(e):\n",
        "        print(\"üîß Detectado problema con MLflow. Corrigiendo autom√°ticamente...\\n\")\n",
        "        \n",
        "        # Desinstalar MLflow problem√°tico\n",
        "        print(\"üì¶ Desinstalando MLflow problem√°tico...\")\n",
        "        !pip uninstall -y -q mlflow mlflow-skinny 2>/dev/null || true\n",
        "        print(\"   ‚úÖ MLflow desinstalado\\n\")\n",
        "        \n",
        "        # Instalar MLflow 2.14.1 (versi√≥n estable y probada)\n",
        "        print(\"üì¶ Instalando MLflow 2.14.1 (versi√≥n estable)...\")\n",
        "        !pip install -q --no-cache-dir \"mlflow==2.14.1\"\n",
        "        print(\"   ‚úÖ MLflow 2.14.1 instalado\\n\")\n",
        "        \n",
        "        # Intentar importar nuevamente\n",
        "        try:\n",
        "            import mlflow\n",
        "            import mlflow.tensorflow\n",
        "            mlflow_ok = True\n",
        "            mlflow_version = mlflow.__version__\n",
        "            print(f\"   ‚úÖ MLflow {mlflow_version} funciona correctamente\")\n",
        "            print(f\"   ‚úÖ mlflow.tensorflow importado exitosamente\\n\")\n",
        "        except ImportError as e2:\n",
        "            print(f\"   ‚ùå Error al importar MLflow despu√©s de correcci√≥n: {e2}\")\n",
        "            print(f\"   üí° Intenta reiniciar el runtime: Runtime > Reiniciar sesi√≥n\\n\")\n",
        "            mlflow_ok = False\n",
        "            mlflow_version = \"Error\"\n",
        "            # Continuar sin MLflow para no bloquear el resto del notebook\n",
        "            mlflow = None\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Error al importar MLflow: {e}\")\n",
        "        print(f\"üí° MLflow no est√° disponible, pero puedes continuar sin √©l\\n\")\n",
        "        mlflow_ok = False\n",
        "        mlflow_version = \"Error\"\n",
        "        mlflow = None\n",
        "\n",
        "if mlflow_ok:\n",
        "    print(f\"‚úÖ MLflow {mlflow_version} configurado correctamente\")\n",
        "\n",
        "# Albumentations y OpenCV (requieren BLOQUE 5 ejecutado)\n",
        "try:\n",
        "    import cv2\n",
        "    import albumentations as A\n",
        "    cv2_version = cv2.__version__\n",
        "    albumentations_version = A.__version__\n",
        "    cv2_available = True\n",
        "except ImportError as e:\n",
        "    cv2_version = \"No disponible (ejecuta BLOQUE 5)\"\n",
        "    albumentations_version = \"No disponible (ejecuta BLOQUE 5)\"\n",
        "    cv2_available = False\n",
        "\n",
        "# Configurar matplotlib para que todas las gr√°ficas se vean consistentes en Colab.\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"\\n‚úÖ Todas las dependencias importadas correctamente\")\n",
        "print(f\"\\nüìä Versiones:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "if 'mlflow_ok' in locals() and mlflow_ok:\n",
        "    print(f\"   - MLflow: {mlflow_version} ‚úÖ\")\n",
        "elif 'mlflow_ok' in locals():\n",
        "    print(f\"   - MLflow: {mlflow_version} ‚ö†Ô∏è\")\n",
        "else:\n",
        "    print(f\"   - MLflow: No verificado\")\n",
        "if cv2_available:\n",
        "    print(f\"   - OpenCV: {cv2_version}\")\n",
        "    print(f\"   - Albumentations: {albumentations_version}\")\n",
        "else:\n",
        "    print(f\"   - OpenCV: {cv2_version} ‚ö†Ô∏è (ejecuta BLOQUE 5)\")\n",
        "    print(f\"   - Albumentations: {albumentations_version} ‚ö†Ô∏è (ejecuta BLOQUE 5)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 7: CONFIGURACI√ìN DEL PROYECTO Y ESTRUCTURA DE CARPETAS\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Crea estructura de carpetas para datos y modelos en Drive\n",
        "# üìÅ Usa el mismo BASE_DIR del BLOQUE 1 (proyecto ya clonado en Drive)\n",
        "# üí° Drive ya est√° montado en el BLOQUE 1, as√≠ que solo verificamos\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# üîó Verificar que Drive est√° montado (ya deber√≠a estar montado en el BLOQUE 1)\n",
        "drive_path = Path('/content/drive')\n",
        "if not drive_path.exists() or not any(drive_path.iterdir()):\n",
        "    print(\"‚ö†Ô∏è Google Drive no est√° montado. Montando ahora...\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive montado exitosamente\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al montar Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive debe estar montado. Ejecuta el BLOQUE 1 primero.\")\n",
        "else:\n",
        "    print('‚úÖ Google Drive ya est√° montado (montado en el BLOQUE 1)')\n",
        "\n",
        "# üìÅ Directorio base dentro de tu Drive (mismo que el BLOQUE 1)\n",
        "#    El proyecto ya est√° clonado aqu√≠ desde el BLOQUE 1\n",
        "BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
        "\n",
        "# Verificar que el proyecto existe (deber√≠a existir desde el BLOQUE 1)\n",
        "if not BASE_DIR.exists():\n",
        "    print(f\"‚ö†Ô∏è El proyecto no existe en {BASE_DIR}\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 1 primero para clonar el repositorio en Drive\")\n",
        "    raise RuntimeError(f\"El proyecto debe existir en {BASE_DIR}. Ejecuta el BLOQUE 1 primero.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Proyecto encontrado en: {BASE_DIR}\")\n",
        "\n",
        "# üìÇ Creamos (si no existen) las carpetas est√°ndar para datos crudos, procesados y modelos.\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "AUGMENTED_DIR = DATA_DIR / 'augmented'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "MLRUNS_DIR = BASE_DIR / 'mlruns'\n",
        "\n",
        "for dir_path in [DATA_DIR, RAW_DIR, PROCESSED_DIR, AUGMENTED_DIR, MODELS_DIR, MLRUNS_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üìä Configuraci√≥n de MLflow (tracking local persistente)\n",
        "# ------------------------------------------------------------\n",
        "try:\n",
        "    import mlflow\n",
        "    mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")\n",
        "    mlflow.set_experiment(\"bovine-weight-estimation\")\n",
        "    mlflow_available = True\n",
        "except (ImportError, NameError):\n",
        "    print(\"‚ö†Ô∏è MLflow no est√° disponible. El tracking de experimentos no funcionar√°.\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 6 para instalar/corregir MLflow\")\n",
        "    mlflow_available = False\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ‚öôÔ∏è Configuraci√≥n general del entrenamiento (hiperpar√°metros base)\n",
        "# ------------------------------------------------------------\n",
        "CONFIG = {\n",
        "    'image_size': (224, 224),\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 0.001,\n",
        "    'validation_split': 0.2,\n",
        "    'test_split': 0.1,\n",
        "    'early_stopping_patience': 10,\n",
        "    'target_r2': 0.95,\n",
        "    'max_mae': 5.0,\n",
        "    'max_inference_time': 3.0\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üêÑ Razas objetivo (Santa Cruz, Chiquitan√≠a y Pampa)\n",
        "# ------------------------------------------------------------\n",
        "BREEDS = [\n",
        "    'brahman', 'nelore', 'angus', 'cebuinas',\n",
        "    'criollo', 'pardo_suizo', 'guzerat', 'holstein'\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completada correctamente\")\n",
        "print(f\"üìÅ Directorio base: {BASE_DIR}\")\n",
        "print(f\"üéØ Razas objetivo: {len(BREEDS)} razas -> {BREEDS}\")\n",
        "if mlflow_available:\n",
        "    print(f\"üìä MLflow tracking: {MLRUNS_DIR} ‚úÖ\")\n",
        "else:\n",
        "    print(f\"üìä MLflow tracking: ‚ö†Ô∏è No disponible (ejecuta BLOQUE 6)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• D√≠a 2-3: Descargar y Organizar Datasets Cr√≠ticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 7.5: DESCARGAR CID DATASET DESDE GITHUB\n",
        "# ============================================================\n",
        "# üì• Descarga el CID Dataset desde GitHub usando git clone (con git-lfs)\n",
        "# ‚ö†Ô∏è Ejecuta ANTES del BLOQUE 8 para tener el CID Dataset disponible\n",
        "# üí° El CID Dataset se descarga desde: https://github.com/bhuiyanmobasshir94/CID.git\n",
        "# üìä Total: ~8GB, 17,899 im√°genes con metadata de peso\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì• DESCARGANDO CID DATASET DESDE GITHUB\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido (debe venir del BLOQUE 7)\n",
        "if 'RAW_DIR' not in globals():\n",
        "    # Intentar definir desde BASE_DIR si existe\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è RAW_DIR no est√° definido. Ejecuta el BLOQUE 7 primero.\")\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "# Directorio donde se clonar√° el CID Dataset\n",
        "CID_CLONE_DIR = RAW_DIR / 'cid'\n",
        "CID_REPO_URL = 'https://github.com/bhuiyanmobasshir94/CID.git'\n",
        "\n",
        "# Verificar si el CID Dataset ya est√° clonado\n",
        "if CID_CLONE_DIR.exists() and (CID_CLONE_DIR / 'CID').exists():\n",
        "    print(f\"‚úÖ CID Dataset ya est√° clonado en: {CID_CLONE_DIR / 'CID'}\")\n",
        "    \n",
        "    # Verificar si tiene contenido\n",
        "    cid_content = CID_CLONE_DIR / 'CID'\n",
        "    if any(cid_content.iterdir()):\n",
        "        print(f\"‚úÖ CID Dataset encontrado y listo para usar\")\n",
        "        \n",
        "        # Buscar metadata.csv\n",
        "        metadata_files = list(cid_content.rglob('metadata.csv'))\n",
        "        if metadata_files:\n",
        "            CID_METADATA_FILE = str(metadata_files[0])\n",
        "            print(f\"‚úÖ Metadata CSV encontrado: {CID_METADATA_FILE}\")\n",
        "        else:\n",
        "            CID_METADATA_FILE = str(cid_content / 'metadata.csv')\n",
        "            print(f\"‚ö†Ô∏è Metadata CSV no encontrado, pero el dataset est√° disponible\")\n",
        "        \n",
        "        # Configurar variables de entorno\n",
        "        os.environ['CID_DATASET_PATH'] = str(cid_content)\n",
        "        os.environ['CID_METADATA_FILE'] = CID_METADATA_FILE\n",
        "        \n",
        "        print(f\"\\nüìã Variables de entorno configuradas:\")\n",
        "        print(f\"   CID_DATASET_PATH = {cid_content}\")\n",
        "        print(f\"   CID_METADATA_FILE = {CID_METADATA_FILE}\")\n",
        "        print(f\"\\nüí° Pr√≥ximos pasos:\")\n",
        "        print(f\"   - BLOQUE 8: Configurar Kaggle.json (opcional)\")\n",
        "        print(f\"   - BLOQUE 9: Preparar otros datasets (local, Kaggle, scraped)\")\n",
        "        print(f\"   - BLOQUE 10: Descargar im√°genes desde m√∫ltiples fuentes (ideal: 200+ por raza)\")\n",
        "        print(f\"   - BLOQUE 12: Analizar CID Dataset (opcional)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Directorio CID existe pero est√° vac√≠o. Re-clonando...\")\n",
        "        # Eliminar directorio vac√≠o\n",
        "        import shutil\n",
        "        shutil.rmtree(CID_CLONE_DIR, ignore_errors=True)\n",
        "        CID_CLONE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "else:\n",
        "    # Crear directorio si no existe\n",
        "    CID_CLONE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"üì• Descargando CID Dataset desde GitHub...\")\n",
        "    print(f\"üîó Repositorio: {CID_REPO_URL}\")\n",
        "    print(f\"üìÅ Destino: {CID_CLONE_DIR}\")\n",
        "    print(f\"üíæ Tama√±o estimado: ~8GB (puede tardar varios minutos)\")\n",
        "    print()\n",
        "    \n",
        "    # Instalar git-lfs (requerido para archivos grandes)\n",
        "    print(\"üì¶ Instalando git-lfs...\")\n",
        "    try:\n",
        "        subprocess.run(['git', 'lfs', 'install'], check=True, capture_output=True)\n",
        "        print(\"‚úÖ git-lfs instalado correctamente\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Error instalando git-lfs: {e}\")\n",
        "        print(\"üí° Intentando continuar sin git-lfs...\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è git no est√° disponible. Instalando...\")\n",
        "        subprocess.run(['apt-get', 'update', '-qq'], check=True)\n",
        "        subprocess.run(['apt-get', 'install', '-y', 'git', 'git-lfs'], check=True)\n",
        "        subprocess.run(['git', 'lfs', 'install'], check=True)\n",
        "        print(\"‚úÖ git y git-lfs instalados\")\n",
        "    \n",
        "    # Clonar repositorio CID\n",
        "    print(f\"\\nüì• Clonando repositorio CID (esto puede tardar varios minutos)...\")\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['git', 'clone', CID_REPO_URL, str(CID_CLONE_DIR / 'CID')],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        print(\"‚úÖ CID Dataset clonado exitosamente\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error al clonar CID Dataset: {e}\")\n",
        "        print(f\"   stdout: {e.stdout}\")\n",
        "        print(f\"   stderr: {e.stderr}\")\n",
        "        print(\"\\nüí° Soluciones:\")\n",
        "        print(\"   1. Verifica que tienes conexi√≥n a internet en Colab\")\n",
        "        print(\"   2. Verifica que el repositorio existe: https://github.com/bhuiyanmobasshir94/CID\")\n",
        "        print(\"   3. Si el repositorio es privado, necesitar√°s configurar credenciales\")\n",
        "        print(\"   4. Puedes continuar con otros datasets (Kaggle, Google Images)\")\n",
        "        raise\n",
        "    \n",
        "    # Verificar que se clon√≥ correctamente\n",
        "    cid_content = CID_CLONE_DIR / 'CID'\n",
        "    if cid_content.exists() and any(cid_content.iterdir()):\n",
        "        print(f\"‚úÖ CID Dataset clonado correctamente en: {cid_content}\")\n",
        "        \n",
        "        # Buscar metadata.csv\n",
        "        metadata_files = list(cid_content.rglob('metadata.csv'))\n",
        "        if metadata_files:\n",
        "            CID_METADATA_FILE = str(metadata_files[0])\n",
        "            print(f\"‚úÖ Metadata CSV encontrado: {CID_METADATA_FILE}\")\n",
        "        else:\n",
        "            CID_METADATA_FILE = str(cid_content / 'metadata.csv')\n",
        "            print(f\"‚ö†Ô∏è Metadata CSV no encontrado en la ubicaci√≥n esperada\")\n",
        "            print(f\"üí° Busca manualmente el archivo metadata.csv en: {cid_content}\")\n",
        "        \n",
        "        # Contar im√°genes (aproximado)\n",
        "        img_count = len(list(cid_content.rglob('*.jpg'))) + len(list(cid_content.rglob('*.png'))) + len(list(cid_content.rglob('*.jpeg')))\n",
        "        if img_count > 0:\n",
        "            print(f\"üìä Total im√°genes encontradas: {img_count:,}\")\n",
        "        \n",
        "        # Configurar variables de entorno\n",
        "        os.environ['CID_DATASET_PATH'] = str(cid_content)\n",
        "        os.environ['CID_METADATA_FILE'] = CID_METADATA_FILE\n",
        "        \n",
        "        print(f\"\\nüìã Variables de entorno configuradas:\")\n",
        "        print(f\"   CID_DATASET_PATH = {cid_content}\")\n",
        "        print(f\"   CID_METADATA_FILE = {CID_METADATA_FILE}\")\n",
        "        print(f\"\\nüí° Pr√≥ximos pasos:\")\n",
        "        print(f\"   - BLOQUE 8: Configurar Kaggle.json (opcional)\")\n",
        "        print(f\"   - BLOQUE 9: Preparar otros datasets (local, Kaggle, scraped)\")\n",
        "        print(f\"   - BLOQUE 10: Descargar im√°genes desde m√∫ltiples fuentes (ideal: 200+ por raza)\")\n",
        "        print(f\"   - BLOQUE 12: Analizar CID Dataset (opcional)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è El repositorio se clon√≥ pero no se encontr√≥ contenido\")\n",
        "        print(f\"üí° Verifica manualmente: {cid_content}\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"‚úÖ BLOQUE 7.5 COMPLETADO\")\n",
        "print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 8: CONFIGURAR KAGGLE.JSON DESDE DRIVE (OPCIONAL)\n",
        "# ============================================================\n",
        "# üîë Copia kaggle.json desde Google Drive a /root/.kaggle/\n",
        "# ‚ö†Ô∏è Ejecuta SOLO si tienes kaggle.json en Drive y no lo has configurado a√∫n\n",
        "# üìÅ Ajusta la ruta KAGGLE_JSON_PATH seg√∫n donde est√© tu archivo en Drive\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# üëâ Ruta del kaggle.json en Google Drive (ra√≠z de MyDrive)\n",
        "KAGGLE_JSON_PATH = Path('/content/drive/MyDrive/kaggle.json')  # <--- Ruta en la ra√≠z de Drive\n",
        "# KAGGLE_JSON_PATH = Path('/content/drive/MyDrive/keys/kaggle.json')  # Si est√° en carpeta keys\n",
        "\n",
        "if KAGGLE_JSON_PATH.exists():\n",
        "    kaggle_dir = Path('/root/.kaggle')\n",
        "    kaggle_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    shutil.copy(KAGGLE_JSON_PATH, kaggle_dir / 'kaggle.json')\n",
        "    subprocess.run([\"chmod\", \"600\", \"/root/.kaggle/kaggle.json\"], check=True)\n",
        "    \n",
        "    print(\"‚úÖ kaggle.json copiado desde Drive a /root/.kaggle/\")\n",
        "    print(\"üîë Credenciales de Kaggle configuradas correctamente\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è No se encontr√≥ kaggle.json en: {KAGGLE_JSON_PATH}\")\n",
        "    print(\"üí° Ajusta KAGGLE_JSON_PATH o sube kaggle.json manualmente antes del BLOQUE 9\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 9: PREPARAR DATASETS DE GANADO BOVINO (SOLUCI√ìN URGENTE)\n",
        "# ============================================================\n",
        "# üì• Prepara datasets de ganado bovino para entrenamiento\n",
        "# üö® SOLUCI√ìN PARA PRESENTACI√ìN: M√∫ltiples opciones para conseguir datos r√°pidamente\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì• PREPARANDO DATASETS DE GANADO BOVINO\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido\n",
        "if 'RAW_DIR' not in globals():\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "\n",
        "def download_images_from_unsplash(breed: str, count: int = 100, output_dir: Path = None):\n",
        "    \"\"\"Descarga im√°genes desde Unsplash (gratis, sin API key para uso b√°sico).\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = RAW_DIR / 'downloaded_images' / breed\n",
        "    \n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"üì• Descargando {count} im√°genes de '{breed}' desde Unsplash...\")\n",
        "    \n",
        "    # Unsplash Source API (gratis, sin autenticaci√≥n para uso b√°sico)\n",
        "    # Nota: Para producci√≥n, usa la API oficial con key\n",
        "    search_terms = {\n",
        "        'brahman': 'brahman cattle',\n",
        "        'nelore': 'nelore cattle',\n",
        "        'angus': 'angus cattle',\n",
        "        'cebuinas': 'zebu cattle',\n",
        "        'criollo': 'criollo cattle',\n",
        "        'pardo_suizo': 'brown swiss cattle',\n",
        "        'jersey': 'jersey cattle'\n",
        "    }\n",
        "    \n",
        "    search_term = search_terms.get(breed, breed + ' cattle')\n",
        "    \n",
        "    # URLs de ejemplo de Unsplash (en producci√≥n, usar API)\n",
        "    # Por ahora, creamos estructura y damos instrucciones\n",
        "    print(f\"üí° Para descargar im√°genes reales:\")\n",
        "    print(f\"   1. Ve a: https://unsplash.com/s/photos/{search_term.replace(' ', '-')}\")\n",
        "    print(f\"   2. Descarga {count} im√°genes manualmente\")\n",
        "    print(f\"   3. Sube a: {output_dir}\")\n",
        "    print(f\"   4. O usa el script de descarga autom√°tica (requiere API key)\")\n",
        "    \n",
        "    return output_dir\n",
        "\n",
        "\n",
        "def create_demo_dataset_with_instructions():\n",
        "    \"\"\"Crea estructura de dataset demo con instrucciones detalladas.\"\"\"\n",
        "    demo_dir = RAW_DIR / 'demo_dataset'\n",
        "    demo_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\nüéØ Creando estructura de dataset demo...\")\n",
        "    \n",
        "    breeds = ['brahman', 'nelore', 'angus', 'cebuinas', 'criollo', 'pardo_suizo', 'jersey']\n",
        "    metadata_rows = []\n",
        "    \n",
        "    for breed in breeds:\n",
        "        breed_dir = demo_dir / breed\n",
        "        breed_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Crear archivo de instrucciones\n",
        "        instructions = f\"\"\"INSTRUCCIONES PARA DATASET {breed.upper()}\n",
        "{'=' * 50}\n",
        "\n",
        "1. FUENTES DE IM√ÅGENES GRATIS:\n",
        "   - Unsplash: https://unsplash.com/s/photos/{breed}-cattle\n",
        "   - Pexels: https://www.pexels.com/search/{breed}%20cattle/\n",
        "   - Pixabay: https://pixabay.com/images/search/{breed}%20cattle/\n",
        "   - Google Images: Busca \"{breed} cattle\" y descarga manualmente\n",
        "\n",
        "2. ESTRUCTURA:\n",
        "   - Nombra im√°genes: {breed}_001.jpg, {breed}_002.jpg, etc.\n",
        "   - M√≠nimo recomendado: 100 im√°genes por raza\n",
        "   - Ideal para presentaci√≥n: 150+ im√°genes por raza\n",
        "\n",
        "3. METADATA:\n",
        "   - Crea metadata.csv con columnas:\n",
        "     * image_path: ruta relativa (ej: {breed}/{breed}_001.jpg)\n",
        "     * weight_kg: peso en kilogramos (ej: 450.5)\n",
        "     * breed: nombre de la raza (ej: {breed})\n",
        "     * age_category: ternero/vaquillona/toro/vaca\n",
        "\n",
        "4. PESOS PROMEDIO DE REFERENCIA:\n",
        "   - Brahman: 400-500 kg\n",
        "   - Nelore: 380-480 kg\n",
        "   - Angus: 500-600 kg\n",
        "   - Cebuinas: 350-450 kg\n",
        "   - Criollo: 300-400 kg\n",
        "   - Pardo Suizo: 550-650 kg\n",
        "   - Jersey: 300-400 kg\n",
        "\n",
        "5. PARA PRESENTACI√ìN ACAD√âMICA:\n",
        "   - Puedes usar pesos sint√©ticos basados en rangos reales\n",
        "   - Lo importante es tener im√°genes y estructura correcta\n",
        "   - El modelo se entrenar√° y funcionar√° para demostraci√≥n\n",
        "\"\"\"\n",
        "        \n",
        "        with open(breed_dir / 'INSTRUCCIONES.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(instructions)\n",
        "        \n",
        "        # Agregar filas de ejemplo al metadata\n",
        "        for i in range(1, 6):  # 5 ejemplos por raza\n",
        "            metadata_rows.append({\n",
        "                'image_path': f'{breed}/{breed}_{i:03d}.jpg',\n",
        "                'weight_kg': 400 + (i * 10),  # Pesos de ejemplo\n",
        "                'breed': breed,\n",
        "                'age_category': 'vaca' if i % 2 == 0 else 'toro'\n",
        "            })\n",
        "    \n",
        "    # Crear metadata.csv de ejemplo\n",
        "    df_metadata = pd.DataFrame(metadata_rows)\n",
        "    metadata_file = demo_dir / 'metadata_template.csv'\n",
        "    df_metadata.to_csv(metadata_file, index=False)\n",
        "    \n",
        "    print(f\"‚úÖ Estructura creada en: {demo_dir}\")\n",
        "    print(f\"üìã Template de metadata: {metadata_file}\")\n",
        "    print(f\"üìÅ Carpetas por raza creadas con instrucciones\")\n",
        "    \n",
        "    return demo_dir\n",
        "\n",
        "\n",
        "def prepare_local_dataset() -> Path | None:\n",
        "    \"\"\"Prepara dataset desde im√°genes locales.\"\"\"\n",
        "    local_dir = RAW_DIR / 'local_images'\n",
        "    \n",
        "    if not local_dir.exists():\n",
        "        return None\n",
        "    \n",
        "    img_files = (\n",
        "        list(local_dir.rglob('*.jpg')) + \n",
        "        list(local_dir.rglob('*.png')) + \n",
        "        list(local_dir.rglob('*.jpeg'))\n",
        "    )\n",
        "    \n",
        "    if not img_files:\n",
        "        return None\n",
        "    \n",
        "    print(f\"‚úÖ Dataset local: {len(img_files):,} im√°genes\")\n",
        "    return local_dir\n",
        "\n",
        "\n",
        "def prepare_kaggle_dataset_small() -> Path | None:\n",
        "    \"\"\"Intenta usar dataset peque√±o de Kaggle.\"\"\"\n",
        "    output_dir = RAW_DIR / 'kaggle'\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    zip_files = list(output_dir.glob('*.zip'))\n",
        "    if zip_files:\n",
        "        zip_file = zip_files[0]\n",
        "        file_size_gb = zip_file.stat().st_size / (1024**3)\n",
        "        \n",
        "        if file_size_gb <= 14:\n",
        "            print(f\"‚úÖ ZIP encontrado: {zip_file.name} ({file_size_gb:.2f}GB)\")\n",
        "            # Descomprimir si no est√° extra√≠do\n",
        "            if not any(output_dir.glob('**/*.jpg')):\n",
        "                print(f\"üì¶ Descomprimiendo...\")\n",
        "                subprocess.run(['unzip', '-q', str(zip_file), '-d', str(output_dir)], \n",
        "                             check=False)\n",
        "            return output_dir\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "def create_combined_dataset():\n",
        "    \"\"\"Crea dataset combinado desde todas las fuentes.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üîó BUSCANDO DATASETS DISPONIBLES\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    datasets_found = []\n",
        "    \n",
        "    # 1. Dataset local\n",
        "    local = prepare_local_dataset()\n",
        "    if local:\n",
        "        datasets_found.append(('local', local))\n",
        "    \n",
        "    # 2. Dataset Kaggle peque√±o\n",
        "    kaggle = prepare_kaggle_dataset_small()\n",
        "    if kaggle:\n",
        "        datasets_found.append(('kaggle', kaggle))\n",
        "    \n",
        "    # 3. Dataset scrapeado\n",
        "    scraped_dir = RAW_DIR / 'scraped'\n",
        "    if scraped_dir.exists():\n",
        "        imgs = list(scraped_dir.rglob('*.jpg')) + list(scraped_dir.rglob('*.png'))\n",
        "        if imgs:\n",
        "            datasets_found.append(('scraped', scraped_dir))\n",
        "    \n",
        "    if datasets_found:\n",
        "        print(f\"‚úÖ {len(datasets_found)} fuente(s) encontrada(s):\")\n",
        "        total = 0\n",
        "        for name, path in datasets_found:\n",
        "            count = len(list(path.rglob('*.jpg')) + list(path.rglob('*.png')))\n",
        "            print(f\"   - {name}: {count:,} im√°genes\")\n",
        "            total += count\n",
        "        print(f\"\\nüìä TOTAL: {total:,} im√°genes disponibles\")\n",
        "        return True\n",
        "    \n",
        "    # Si no hay datasets, crear estructura demo\n",
        "    print(\"‚ö†Ô∏è No se encontraron datasets\")\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üö® SOLUCI√ìN URGENTE PARA PRESENTACI√ìN\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    demo_dir = create_demo_dataset_with_instructions()\n",
        "    \n",
        "    print(f\"\\nüí° OPCIONES R√ÅPIDAS PARA CONSEGUIR IM√ÅGENES:\")\n",
        "    print(f\"\\nüì• OPCI√ìN 1: Descarga Manual (M√ÅS R√ÅPIDO - 30 minutos)\")\n",
        "    print(f\"   1. Ve a estos sitios (gratis, sin copyright):\")\n",
        "    print(f\"      ‚Ä¢ Unsplash: https://unsplash.com/s/photos/cattle\")\n",
        "    print(f\"      ‚Ä¢ Pexels: https://www.pexels.com/search/cattle/\")\n",
        "    print(f\"      ‚Ä¢ Pixabay: https://pixabay.com/images/search/cow/\")\n",
        "    print(f\"   2. Busca por raza: 'brahman cattle', 'nelore cattle', etc.\")\n",
        "    print(f\"   3. Descarga 100-150 im√°genes por raza\")\n",
        "    print(f\"   4. Organiza en: {RAW_DIR / 'local_images'}\")\n",
        "    print(f\"   5. Crea metadata.csv con pesos estimados\")\n",
        "    \n",
        "    print(f\"\\nüì∏ OPCI√ìN 2: Fotos Reales (MEJOR CALIDAD)\")\n",
        "    print(f\"   1. Toma 100+ fotos del ganado en la Hacienda Gamelera\")\n",
        "    print(f\"   2. Anota peso real de b√°scula para cada foto\")\n",
        "    print(f\"   3. Sube a: {RAW_DIR / 'local_images'}\")\n",
        "    \n",
        "    print(f\"\\nü§ñ OPCI√ìN 3: Usar BLOQUE 10 (Scraping Autom√°tico)\")\n",
        "    print(f\"   1. Ejecuta el BLOQUE 10 para scraping de Google Images\")\n",
        "    print(f\"   2. Ajusta l√≠mites seg√∫n necesidad\")\n",
        "    print(f\"   3. Verifica y organiza las im√°genes descargadas\")\n",
        "    \n",
        "    print(f\"\\nüìã ESTRUCTURA M√çNIMA PARA PRESENTACI√ìN:\")\n",
        "    print(f\"   - M√≠nimo viable: 100 im√°genes por raza (700 total)\")\n",
        "    print(f\"   - Recomendado: 150 im√°genes por raza (1050 total)\")\n",
        "    print(f\"   - Ideal: 200+ im√°genes por raza (1400+ total)\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Estructura demo creada en: {demo_dir}\")\n",
        "    print(f\"üí° Sigue las instrucciones en cada carpeta de raza\")\n",
        "    \n",
        "    return False\n",
        "\n",
        "\n",
        "# Ejecutar\n",
        "try:\n",
        "    success = create_combined_dataset()\n",
        "    \n",
        "    if success:\n",
        "        print(f\"\\n‚úÖ BLOQUE 9 COMPLETADO - Datasets listos\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è BLOQUE 9 COMPLETADO - Estructura demo creada\")\n",
        "        print(f\"üí° Sigue las instrucciones para agregar im√°genes\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 10: DESCARGAR IM√ÅGENES DESDE M√öLTIPLES FUENTES\n",
        "# ============================================================\n",
        "# üñºÔ∏è Descarga im√°genes de ganado bovino desde m√∫ltiples fuentes\n",
        "# üéØ Objetivo: 200+ im√°genes por raza para dataset ideal (1400+ total) ‚≠ê\n",
        "# üí° Configurable: Cambia IMAGES_PER_BREED seg√∫n necesidad\n",
        "# ‚ö†Ô∏è Respeta t√©rminos de uso y evita bloqueos\n",
        "#\n",
        "# üìÅ ESTRUCTURA DE ALMACENAMIENTO:\n",
        "#    data/raw/scraped/\n",
        "#    ‚îú‚îÄ‚îÄ brahman/\n",
        "#    ‚îÇ   ‚îú‚îÄ‚îÄ brahman_001.jpg\n",
        "#    ‚îÇ   ‚îú‚îÄ‚îÄ brahman_002.jpg\n",
        "#    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "#    ‚îú‚îÄ‚îÄ nelore/\n",
        "#    ‚îú‚îÄ‚îÄ angus/\n",
        "#    ‚îú‚îÄ‚îÄ cebuinas/\n",
        "#    ‚îú‚îÄ‚îÄ criollo/\n",
        "#    ‚îú‚îÄ‚îÄ pardo_suizo/\n",
        "#    ‚îú‚îÄ‚îÄ jersey/\n",
        "#    ‚îî‚îÄ‚îÄ metadata.csv  (generado autom√°ticamente)\n",
        "#\n",
        "# üìä CLASIFICACI√ìN DE DATASETS (7 razas):\n",
        "#    - Peque√±o: < 350 im√°genes (< 50 por raza promedio)\n",
        "#    - Mediano-Chico: 350-699 im√°genes (50-99 por raza promedio)\n",
        "#    - Mediano: 700-1049 im√°genes (100-149 por raza promedio) ‚≠ê M√çNIMO VIABLE\n",
        "#    - Mediano-Grande: 1050-1399 im√°genes (150-199 por raza promedio)\n",
        "#    - Grande: 1400+ im√°genes (200+ por raza promedio) ‚≠ê IDEAL\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "import time\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üñºÔ∏è DESCARGANDO IM√ÅGENES DE GANADO BOVINO\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"‚ö†Ô∏è NOTA IMPORTANTE:\")\n",
        "print(\"   - Algunos sitios bloquean descargas autom√°ticas (HTTP 403)\")\n",
        "print(\"   - Esto es normal y el script continuar√° con otras im√°genes\")\n",
        "print(\"   - Los errores se muestran de forma limitada para no saturar la salida\")\n",
        "print(\"   - Si se interrumpe, puedes ejecutar nuevamente (contin√∫a donde qued√≥)\")\n",
        "print()\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido\n",
        "if 'RAW_DIR' not in globals():\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "# Configuraci√≥n de razas con m√∫ltiples t√©rminos de b√∫squeda\n",
        "BREED_SEARCH_TERMS = {\n",
        "    'brahman': [\n",
        "        'brahman cattle',\n",
        "        'brahman cow',\n",
        "        'brahman bull',\n",
        "        'brahman ganado',\n",
        "        'brahman bovino'\n",
        "    ],\n",
        "    'nelore': [\n",
        "        'nelore cattle',\n",
        "        'nelore cow',\n",
        "        'nelore bull',\n",
        "        'nelore ganado',\n",
        "        'nelore bovino'\n",
        "    ],\n",
        "    'angus': [\n",
        "        'angus cattle',\n",
        "        'angus cow',\n",
        "        'angus bull',\n",
        "        'angus ganado',\n",
        "        'angus bovino'\n",
        "    ],\n",
        "    'cebuinas': [\n",
        "        'zebu cattle',\n",
        "        'cebu cattle',\n",
        "        'indicus cattle',\n",
        "        'cebuino ganado'\n",
        "    ],\n",
        "    'criollo': [\n",
        "        'criollo cattle',\n",
        "        'criollo cow',\n",
        "        'criollo ganado boliviano',\n",
        "        'criollo bovino'\n",
        "    ],\n",
        "    'pardo_suizo': [\n",
        "        'brown swiss cattle',\n",
        "        'pardo suizo cattle',\n",
        "        'brown swiss cow',\n",
        "        'pardo suizo ganado'\n",
        "    ],\n",
        "    'jersey': [\n",
        "        'jersey cattle',\n",
        "        'jersey cow',\n",
        "        'jersey bull',\n",
        "        'jersey ganado'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Objetivo: Configurable - Cambia seg√∫n necesidad\n",
        "# - 100 im√°genes por raza = 700 total (Dataset Mediano - M√≠nimo viable)\n",
        "# - 150 im√°genes por raza = 1050 total (Dataset Mediano-Grande - Recomendado)\n",
        "# - 200 im√°genes por raza = 1400 total (Dataset Grande - Ideal) ‚≠ê\n",
        "IMAGES_PER_BREED = 200  # ‚≠ê Cambia a 200 para dataset ideal (1400+ im√°genes)\n",
        "IMAGES_PER_SEARCH_TERM = 40  # 40 im√°genes por t√©rmino de b√∫squeda (para alcanzar 200 por raza)\n",
        "\n",
        "\n",
        "def scrape_with_bing_downloader(breed: str, search_terms: list, output_dir: Path, limit: int = 200):\n",
        "    \"\"\"Scraping usando bing-image-downloader con mejor manejo de errores.\"\"\"\n",
        "    breed_dir = output_dir / breed\n",
        "    breed_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Verificar im√°genes existentes (para continuar si se interrumpi√≥)\n",
        "    existing_imgs = list(breed_dir.glob('*.jpg')) + list(breed_dir.glob('*.png'))\n",
        "    already_downloaded = len(existing_imgs)\n",
        "    \n",
        "    print(f\"\\nüì• Descargando im√°genes para: {breed.upper()}\")\n",
        "    print(f\"üìÅ Destino: {breed_dir}\")\n",
        "    print(f\"üéØ Objetivo: {limit} im√°genes\")\n",
        "    \n",
        "    if already_downloaded > 0:\n",
        "        print(f\"‚úÖ Ya existen {already_downloaded} im√°genes (continuando desde aqu√≠)\")\n",
        "        if already_downloaded >= limit:\n",
        "            print(f\"‚úÖ Objetivo alcanzado ({already_downloaded} >= {limit})\")\n",
        "            return already_downloaded\n",
        "    \n",
        "    downloaded = already_downloaded\n",
        "    errors_count = 0\n",
        "    max_errors_per_term = 10  # Limitar errores mostrados\n",
        "    \n",
        "    # Intentar usar bing-image-downloader (m√°s confiable que google-images-download)\n",
        "    try:\n",
        "        from bing_image_downloader import downloader\n",
        "        import sys\n",
        "        from io import StringIO\n",
        "        \n",
        "        # Redirigir stderr para capturar errores sin mostrarlos todos\n",
        "        original_stderr = sys.stderr\n",
        "        \n",
        "        for search_term in search_terms:\n",
        "            if downloaded >= limit:\n",
        "                break\n",
        "            \n",
        "            remaining = limit - downloaded\n",
        "            batch_size = min(remaining, IMAGES_PER_SEARCH_TERM)\n",
        "            \n",
        "            print(f\"   üîç Buscando: '{search_term}' ({batch_size} im√°genes)...\", end=' ', flush=True)\n",
        "            \n",
        "            try:\n",
        "                # Capturar errores silenciosamente\n",
        "                sys.stderr = StringIO()\n",
        "                \n",
        "                downloader.download(\n",
        "                    search_term,\n",
        "                    limit=batch_size,\n",
        "                    output_dir=str(breed_dir.parent),\n",
        "                    adult_filter_off=True,\n",
        "                    force_replace=False,\n",
        "                    timeout=10,  # Reducir timeout para evitar esperas largas\n",
        "                    verbose=False\n",
        "                )\n",
        "                \n",
        "                # Restaurar stderr\n",
        "                sys.stderr = original_stderr\n",
        "                \n",
        "                # Contar im√°genes descargadas\n",
        "                # bing-image-downloader crea carpetas con el nombre del t√©rmino de b√∫squeda\n",
        "                # Buscar en m√∫ltiples ubicaciones posibles\n",
        "                possible_term_dirs = [\n",
        "                    breed_dir.parent / search_term.replace(' ', '_'),\n",
        "                    breed_dir.parent / search_term,\n",
        "                    breed_dir.parent / search_term.replace(' ', '-'),\n",
        "                ]\n",
        "                \n",
        "                term_dir = None\n",
        "                for possible_dir in possible_term_dirs:\n",
        "                    if possible_dir.exists() and any(possible_dir.iterdir()):\n",
        "                        term_dir = possible_dir\n",
        "                        break\n",
        "                \n",
        "                if term_dir and term_dir.exists():\n",
        "                    # Buscar im√°genes en la carpeta temporal\n",
        "                    imgs = list(term_dir.rglob('*.jpg')) + list(term_dir.rglob('*.png')) + list(term_dir.rglob('*.jpeg'))\n",
        "                    imgs = [img for img in imgs if img.is_file()]  # Solo archivos, no directorios\n",
        "                    \n",
        "                    # Mover a carpeta de raza\n",
        "                    moved = 0\n",
        "                    for img in imgs:\n",
        "                        if downloaded >= limit:\n",
        "                            break\n",
        "                        \n",
        "                        # Obtener el siguiente n√∫mero disponible\n",
        "                        next_num = downloaded + 1\n",
        "                        new_name = breed_dir / f\"{breed}_{next_num:03d}{img.suffix}\"\n",
        "                        \n",
        "                        # Si el archivo ya existe, saltarlo\n",
        "                        if new_name.exists():\n",
        "                            continue\n",
        "                        \n",
        "                        if img.exists():\n",
        "                            try:\n",
        "                                # Copiar en lugar de mover para evitar problemas\n",
        "                                shutil.copy2(img, new_name)\n",
        "                                downloaded += 1\n",
        "                                moved += 1\n",
        "                            except Exception as e:\n",
        "                                # Si falla, intentar mover\n",
        "                                try:\n",
        "                                    img.rename(new_name)\n",
        "                                    downloaded += 1\n",
        "                                    moved += 1\n",
        "                                except Exception:\n",
        "                                    continue\n",
        "                    \n",
        "                    # Limpiar carpeta temporal despu√©s de mover\n",
        "                    try:\n",
        "                        if term_dir.exists():\n",
        "                            shutil.rmtree(term_dir, ignore_errors=True)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    \n",
        "                    print(f\"‚úÖ {moved} descargadas\")\n",
        "                else:\n",
        "                    # Buscar im√°genes que puedan haberse descargado en otras ubicaciones\n",
        "                    all_imgs = list(breed_dir.parent.rglob('*.jpg')) + list(breed_dir.parent.rglob('*.png'))\n",
        "                    # Filtrar solo las que no est√°n en carpetas de raza\n",
        "                    temp_imgs = [img for img in all_imgs if breed not in str(img.parent)]\n",
        "                    \n",
        "                    if temp_imgs:\n",
        "                        # Intentar mover las im√°genes encontradas\n",
        "                        moved = 0\n",
        "                        for img in temp_imgs[:batch_size]:\n",
        "                            if downloaded >= limit:\n",
        "                                break\n",
        "                            next_num = downloaded + 1\n",
        "                            new_name = breed_dir / f\"{breed}_{next_num:03d}{img.suffix}\"\n",
        "                            if not new_name.exists() and img.exists():\n",
        "                                try:\n",
        "                                    shutil.copy2(img, new_name)\n",
        "                                    downloaded += 1\n",
        "                                    moved += 1\n",
        "                                except Exception:\n",
        "                                    continue\n",
        "                        if moved > 0:\n",
        "                            print(f\"‚úÖ {moved} descargadas (desde ubicaci√≥n alternativa)\")\n",
        "                        else:\n",
        "                            print(f\"‚ö†Ô∏è 0 descargadas\")\n",
        "                            errors_count += 1\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è 0 descargadas\")\n",
        "                        errors_count += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                sys.stderr = original_stderr\n",
        "                errors_count += 1\n",
        "                if errors_count <= max_errors_per_term:\n",
        "                    print(f\"‚ö†Ô∏è Error: {str(e)[:50]}...\")\n",
        "                continue\n",
        "            \n",
        "            # Pausa entre b√∫squedas para evitar bloqueos\n",
        "            time.sleep(1)  # Reducir pausa a 1 segundo\n",
        "        \n",
        "        if errors_count > max_errors_per_term:\n",
        "            print(f\"   ‚ö†Ô∏è ({errors_count - max_errors_per_term} errores adicionales ocultos)\")\n",
        "        \n",
        "        print(f\"   ‚úÖ {breed}: {downloaded} im√°genes descargadas (de {limit} objetivo)\")\n",
        "        return downloaded\n",
        "        \n",
        "    except ImportError:\n",
        "        print(f\"   ‚ö†Ô∏è bing-image-downloader no instalado\")\n",
        "        print(f\"   üí° Instalando...\")\n",
        "        subprocess.run(['pip', 'install', '-q', 'bing-image-downloader'], check=False)\n",
        "        \n",
        "        # Reintentar\n",
        "        try:\n",
        "            from bing_image_downloader import downloader\n",
        "            return scrape_with_bing_downloader(breed, search_terms, output_dir, limit)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Si no funciona, dar instrucciones manuales\n",
        "    print(f\"   ‚ö†Ô∏è Scraping autom√°tico no disponible o con muchos errores\")\n",
        "    print(f\"   üí° INSTRUCCIONES MANUALES:\")\n",
        "    print(f\"      1. Ve a: https://www.bing.com/images/search?q={breed}+cattle\")\n",
        "    print(f\"      2. Descarga {limit} im√°genes manualmente\")\n",
        "    print(f\"      3. Sube a: {breed_dir}\")\n",
        "    print(f\"      4. Nombra: {breed}_001.jpg, {breed}_002.jpg, etc.\")\n",
        "    \n",
        "    return downloaded  # Retornar lo que se haya descargado hasta ahora\n",
        "\n",
        "\n",
        "def scrape_google_images_improved():\n",
        "    \"\"\"Scraping mejorado con m√°s im√°genes y mejor manejo de errores.\"\"\"\n",
        "    print(\"üñºÔ∏è DESCARGANDO IM√ÅGENES DESDE M√öLTIPLES FUENTES\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    print(\"üí° NOTA: Algunos sitios bloquean descargas autom√°ticas (HTTP 403)\")\n",
        "    print(\"üí° El script continuar√° descargando lo que pueda y mostrar√° un resumen\")\n",
        "    print()\n",
        "    \n",
        "    output_dir = RAW_DIR / 'scraped'\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Instalar bing-image-downloader (m√°s confiable)\n",
        "    print(\"üì¶ Instalando herramientas de descarga...\")\n",
        "    try:\n",
        "        subprocess.run(['pip', 'install', '-q', 'bing-image-downloader'], check=False)\n",
        "        print(\"‚úÖ bing-image-downloader instalado\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Error instalando bing-image-downloader\")\n",
        "    \n",
        "    total_downloaded = 0\n",
        "    results_by_breed = {}\n",
        "    \n",
        "    # Descargar para cada raza\n",
        "    for breed, search_terms in BREED_SEARCH_TERMS.items():\n",
        "        try:\n",
        "            downloaded = scrape_with_bing_downloader(\n",
        "                breed, \n",
        "                search_terms, \n",
        "                output_dir, \n",
        "                limit=IMAGES_PER_BREED\n",
        "            )\n",
        "            results_by_breed[breed] = downloaded\n",
        "            total_downloaded += downloaded\n",
        "        except KeyboardInterrupt:\n",
        "            print(f\"\\n‚ö†Ô∏è Descarga interrumpida por el usuario\")\n",
        "            print(f\"üí° Hasta ahora: {total_downloaded:,} im√°genes descargadas\")\n",
        "            print(f\"üí° Puedes continuar ejecutando este bloque nuevamente\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error inesperado con {breed}: {e}\")\n",
        "            results_by_breed[breed] = 0\n",
        "            continue\n",
        "        \n",
        "        # Pausa entre razas para evitar bloqueos\n",
        "        if breed != list(BREED_SEARCH_TERMS.keys())[-1]:\n",
        "            print(\"   ‚è∏Ô∏è Pausa de 3 segundos...\")\n",
        "            time.sleep(3)  # Reducir pausa\n",
        "    \n",
        "    # Resumen\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä RESUMEN DE DESCARGA\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    for breed, count in results_by_breed.items():\n",
        "        status = \"‚úÖ\" if count >= 100 else \"‚ö†Ô∏è\" if count >= 50 else \"‚ùå\"\n",
        "        print(f\"{status} {breed.upper()}: {count} im√°genes\")\n",
        "    \n",
        "    print(f\"\\nüéØ TOTAL DESCARGADO: {total_downloaded:,} im√°genes\")\n",
        "    \n",
        "    # Verificar si alcanzamos el objetivo\n",
        "    breeds_with_100 = sum(1 for count in results_by_breed.values() if count >= 100)\n",
        "    breeds_with_50 = sum(1 for count in results_by_breed.values() if count >= 50)\n",
        "    \n",
        "    print(f\"\\nüìà An√°lisis:\")\n",
        "    print(f\"   - Razas con 50+ im√°genes: {breeds_with_50}/{len(BREED_SEARCH_TERMS)}\")\n",
        "    print(f\"   - Razas con 100+ im√°genes: {breeds_with_100}/{len(BREED_SEARCH_TERMS)}\")\n",
        "    \n",
        "    # Clasificaci√≥n de datasets seg√∫n tama√±o total\n",
        "    # 7 razas √ó umbrales:\n",
        "    # - Peque√±o: < 350 im√°genes (< 50 por raza promedio)\n",
        "    # - Mediano-Chico: 350-699 im√°genes (50-99 por raza promedio)\n",
        "    # - Mediano: 700-1049 im√°genes (100-149 por raza promedio)\n",
        "    # - Mediano-Grande: 1050-1399 im√°genes (150-199 por raza promedio)\n",
        "    # - Grande: 1400+ im√°genes (200+ por raza promedio)\n",
        "    \n",
        "    print(f\"\\nüìä CLASIFICACI√ìN DEL DATASET:\")\n",
        "    if total_downloaded >= 1400:  # 200+ por raza\n",
        "        print(f\"   ‚úÖ DATASET GRANDE ({total_downloaded:,} im√°genes)\")\n",
        "        print(f\"   üí° Excelente para entrenamiento robusto (200+ por raza)\")\n",
        "        print(f\"   üí° Ideal para producci√≥n y alta precisi√≥n\")\n",
        "    elif total_downloaded >= 1050:  # 150+ por raza\n",
        "        print(f\"   ‚úÖ DATASET MEDIANO-GRANDE ({total_downloaded:,} im√°genes)\")\n",
        "        print(f\"   üí° Muy bueno para entrenamiento (150+ por raza)\")\n",
        "        print(f\"   üí° Recomendado para presentaci√≥n acad√©mica\")\n",
        "    elif total_downloaded >= 700:  # 100+ por raza\n",
        "        print(f\"   ‚úÖ DATASET MEDIANO ({total_downloaded:,} im√°genes)\")\n",
        "        print(f\"   üí° Bueno para entrenamiento (100+ por raza)\")\n",
        "        print(f\"   üí° M√≠nimo viable para resultados confiables\")\n",
        "    elif total_downloaded >= 350:  # 50+ por raza\n",
        "        print(f\"   ‚ö†Ô∏è DATASET MEDIANO-CHICO ({total_downloaded:,} im√°genes)\")\n",
        "        print(f\"   üí° Aceptable para pruebas (50-99 por raza)\")\n",
        "        print(f\"   üí° Recomendado: Descargar m√°s para mejor precisi√≥n\")\n",
        "    else:  # < 350 im√°genes\n",
        "        print(f\"   ‚ùå DATASET PEQUE√ëO ({total_downloaded:,} im√°genes)\")\n",
        "        print(f\"   üí° Insuficiente para entrenamiento confiable (< 50 por raza)\")\n",
        "        print(f\"   üí° Ejecuta nuevamente o descarga manualmente\")\n",
        "    \n",
        "    # Crear metadata b√°sico\n",
        "    create_basic_metadata(output_dir, results_by_breed)\n",
        "    \n",
        "    return total_downloaded\n",
        "\n",
        "\n",
        "def create_basic_metadata(output_dir: Path, results_by_breed: dict):\n",
        "    \"\"\"Crea metadata.csv b√°sico con pesos estimados.\"\"\"\n",
        "    import pandas as pd\n",
        "    import random\n",
        "    \n",
        "    metadata_rows = []\n",
        "    \n",
        "    # Pesos promedio por raza (para metadata sint√©tica)\n",
        "    breed_weights = {\n",
        "        'brahman': {'min': 400, 'max': 500, 'avg': 450},\n",
        "        'nelore': {'min': 380, 'max': 480, 'avg': 430},\n",
        "        'angus': {'min': 500, 'max': 600, 'avg': 550},\n",
        "        'cebuinas': {'min': 350, 'max': 450, 'avg': 400},\n",
        "        'criollo': {'min': 300, 'max': 400, 'avg': 350},\n",
        "        'pardo_suizo': {'min': 550, 'max': 650, 'avg': 600},\n",
        "        'jersey': {'min': 300, 'max': 400, 'avg': 350}\n",
        "    }\n",
        "    \n",
        "    random.seed(42)  # Para reproducibilidad\n",
        "    \n",
        "    for breed, count in results_by_breed.items():\n",
        "        if count == 0:\n",
        "            continue\n",
        "        \n",
        "        breed_dir = output_dir / breed\n",
        "        img_files = sorted(list(breed_dir.glob('*.jpg')) + list(breed_dir.glob('*.png')))\n",
        "        \n",
        "        weights = breed_weights.get(breed, {'min': 400, 'max': 500, 'avg': 450})\n",
        "        \n",
        "        for i, img_file in enumerate(img_files[:count]):\n",
        "            # Generar peso sint√©tico basado en rango real\n",
        "            weight = random.uniform(weights['min'], weights['max'])\n",
        "            \n",
        "            metadata_rows.append({\n",
        "                'image_path': f'{breed}/{img_file.name}',\n",
        "                'weight_kg': round(weight, 1),\n",
        "                'breed': breed,\n",
        "                'age_category': random.choice(['ternero', 'vaquillona', 'toro', 'vaca'])\n",
        "            })\n",
        "    \n",
        "    if metadata_rows:\n",
        "        df_metadata = pd.DataFrame(metadata_rows)   \n",
        "        metadata_file = output_dir / 'metadata.csv'\n",
        "        df_metadata.to_csv(metadata_file, index=False)\n",
        "        print(f\"\\nüíæ Metadata creado: {metadata_file}\")\n",
        "        print(f\"üìä Registros: {len(metadata_rows):,}\")\n",
        "        print(f\"üí° NOTA: Pesos son estimados. Reemplaza con datos reales si es posible.\")\n",
        "\n",
        "\n",
        "# Ejecutar descarga\n",
        "try:\n",
        "    scraped_images = scrape_google_images_improved()\n",
        "    \n",
        "    if scraped_images > 0:\n",
        "        print(f\"\\n‚úÖ BLOQUE 10 COMPLETADO\")\n",
        "        print(f\"üìÅ Im√°genes en: {RAW_DIR / 'scraped'}\")\n",
        "        print(f\"üí° Vuelve a ejecutar el BLOQUE 9 para combinar datasets\")\n",
        "        \n",
        "        if scraped_images < 700:\n",
        "            print(f\"\\n‚ö†Ô∏è Dataset peque√±o ({scraped_images:,} im√°genes)\")\n",
        "            print(f\"üí° RECOMENDACIONES:\")\n",
        "            print(f\"   1. Ejecuta el BLOQUE 10 nuevamente (puede descargar m√°s)\")\n",
        "            print(f\"   2. O descarga manualmente desde:\")\n",
        "            print(f\"      ‚Ä¢ Unsplash: https://unsplash.com/s/photos/cattle\")\n",
        "            print(f\"      ‚Ä¢ Pexels: https://www.pexels.com/search/cattle/\")\n",
        "            print(f\"      ‚Ä¢ Pixabay: https://pixabay.com/images/search/cow/\")\n",
        "            print(f\"   3. Organiza en: {RAW_DIR / 'local_images'}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è BLOQUE 10 COMPLETADO CON ADVERTENCIAS\")\n",
        "        print(f\"üí° No se descargaron im√°genes autom√°ticamente\")\n",
        "        print(f\"\\nüîç Verificando si hay im√°genes descargadas manualmente...\")\n",
        "        \n",
        "        # Verificar si hay im√°genes en las carpetas de razas\n",
        "        scraped_dir = RAW_DIR / 'scraped'\n",
        "        total_manual = 0\n",
        "        if scraped_dir.exists():\n",
        "            for breed in BREED_SEARCH_TERMS.keys():\n",
        "                breed_dir = scraped_dir / breed\n",
        "                if breed_dir.exists():\n",
        "                    imgs = list(breed_dir.glob('*.jpg')) + list(breed_dir.glob('*.png'))\n",
        "                    if imgs:\n",
        "                        print(f\"   ‚úÖ {breed}: {len(imgs)} im√°genes encontradas\")\n",
        "                        total_manual += len(imgs)\n",
        "        \n",
        "        if total_manual > 0:\n",
        "            print(f\"\\n‚úÖ Se encontraron {total_manual:,} im√°genes descargadas manualmente\")\n",
        "            print(f\"üí° Ejecuta el BLOQUE 9 para combinar con otros datasets\")\n",
        "        else:\n",
        "            print(f\"\\nüí° SOLUCIONES RECOMENDADAS:\")\n",
        "            print(f\"   1. üì• Descarga manual desde sitios gratuitos:\")\n",
        "            print(f\"      ‚Ä¢ Unsplash: https://unsplash.com/s/photos/cattle\")\n",
        "            print(f\"      ‚Ä¢ Pexels: https://www.pexels.com/search/cattle/\")\n",
        "            print(f\"      ‚Ä¢ Pixabay: https://pixabay.com/images/search/cow/\")\n",
        "            print(f\"   2. üìÅ Organiza las im√°genes en: {RAW_DIR / 'local_images'}\")\n",
        "            print(f\"      Estructura: local_images/brahman/, local_images/nelore/, etc.\")\n",
        "            print(f\"   3. üì∏ Usa fotos reales de la Hacienda Gamelera\")\n",
        "            print(f\"   4. üîÑ Ejecuta el BLOQUE 9 para verificar y combinar datasets\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error en BLOQUE 10: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    scraped_images = 0\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 11: RESUMEN DE DATASETS DESCARGADOS\n",
        "# ============================================================\n",
        "# üìä Muestra resumen de todos los datasets disponibles\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 8, 9, 10 ejecutados (o al menos uno)\n",
        "\n",
        "def summarize_datasets(cid_df: pd.DataFrame | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Resumen de todos los datasets disponibles (solo datos reales).\"\"\"\n",
        "    print(\"üìä RESUMEN DE DATASETS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    datasets_info = []\n",
        "\n",
        "    if cid_df is not None:\n",
        "        datasets_info.append({\n",
        "            'name': 'CID Dataset',\n",
        "            'images': len(cid_df),\n",
        "            'description': 'Computer Vision Research - Cattle Image Database',\n",
        "            'status': '‚úÖ Disponible',\n",
        "        })\n",
        "    else:\n",
        "        datasets_info.append({\n",
        "            'name': 'CID Dataset',\n",
        "            'images': 0,\n",
        "            'description': 'CID sin metadata cargada',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    # Kaggle Dataset (maneja caso cuando a√∫n no existe)\n",
        "    try:\n",
        "        kaggle_path = kaggle_dataset_path if 'kaggle_dataset_path' in globals() else None\n",
        "        kaggle_id = KAGGLE_DATASET_ID if 'KAGGLE_DATASET_ID' in globals() else 'N/A'\n",
        "        if kaggle_path and Path(kaggle_path).exists():\n",
        "            kaggle_images = len(list(Path(kaggle_path).glob('**/*.jpg')))\n",
        "            datasets_info.append({\n",
        "                'name': 'Kaggle Cattle Weight',\n",
        "                'images': kaggle_images,\n",
        "                'description': f'Dataset Kaggle ({kaggle_id})',\n",
        "                'status': '‚úÖ Disponible' if kaggle_images > 0 else '‚ö†Ô∏è Vac√≠o',\n",
        "            })\n",
        "        else:\n",
        "            datasets_info.append({\n",
        "                'name': 'Kaggle Cattle Weight',\n",
        "                'images': 0,\n",
        "                'description': 'Requiere configuraci√≥n de API Kaggle',\n",
        "                'status': '‚ö†Ô∏è Pendiente',\n",
        "            })\n",
        "    except NameError:\n",
        "        datasets_info.append({\n",
        "            'name': 'Kaggle Cattle Weight',\n",
        "            'images': 0,\n",
        "            'description': 'Requiere ejecutar BLOQUE 9',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    # Google Images Scraped (maneja caso cuando a√∫n no existe)\n",
        "    try:\n",
        "        scraped_count = scraped_images if 'scraped_images' in globals() else 0\n",
        "        datasets_info.append({\n",
        "            'name': 'Google Images Scraped',\n",
        "            'images': scraped_count,\n",
        "            'description': 'Razas locales bolivianas',\n",
        "            'status': '‚úÖ Disponible' if scraped_count > 0 else '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "    except NameError:\n",
        "        datasets_info.append({\n",
        "            'name': 'Google Images Scraped',\n",
        "            'images': 0,\n",
        "            'description': 'Requiere ejecutar BLOQUE 10 (opcional)',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    df_datasets = pd.DataFrame(datasets_info)\n",
        "    print(df_datasets.to_string(index=False))\n",
        "\n",
        "    total_images = int(df_datasets['images'].sum())\n",
        "    print(f\"\\nüéØ TOTAL IM√ÅGENES DISPONIBLES: {total_images:,}\")\n",
        "\n",
        "    summary_path = DATA_DIR / 'datasets_summary.csv'\n",
        "    df_datasets.to_csv(summary_path, index=False)\n",
        "    print(f\"\\nüíæ Resumen guardado en: {summary_path}\")\n",
        "\n",
        "    return df_datasets\n",
        "\n",
        "# Ejecutar resumen (maneja caso cuando df_cid a√∫n no existe)\n",
        "# ‚ö†Ô∏è Nota: Si ejecutas ANTES del BLOQUE 12, df_cid ser√° None y solo mostrar√° datasets de Kaggle/Google\n",
        "try:\n",
        "    # Verificar si df_cid existe en el scope global\n",
        "    df_cid_temp = df_cid if 'df_cid' in globals() else None\n",
        "    datasets_summary = summarize_datasets(df_cid_temp)\n",
        "except NameError:\n",
        "    # Si df_cid no existe a√∫n, ejecutar resumen sin metadata del CID\n",
        "    print(\"‚ÑπÔ∏è df_cid a√∫n no cargado. Ejecuta BLOQUE 12 para cargar metadata del CID Dataset.\")\n",
        "    datasets_summary = summarize_datasets(None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä D√≠a 4: An√°lisis Exploratorio de Datos (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 12: AN√ÅLISIS EXPLORATORIO - CID DATASET (OPCIONAL)\n",
        "# ============================================================\n",
        "# üìä Carga y analiza metadata del CID Dataset si est√° disponible\n",
        "# ‚ö†Ô∏è OPCIONAL: Si no tienes CID Dataset, puedes continuar con otros datasets\n",
        "# üí° Este bloque analiza el CID si existe, pero no falla si no est√° disponible\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido (debe venir del BLOQUE 7)\n",
        "if 'RAW_DIR' not in globals():\n",
        "    # Intentar definir desde BASE_DIR si existe\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è RAW_DIR no est√° definido. Ejecuta el BLOQUE 7 primero.\")\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "# Verificar que BREEDS est√° definido (debe venir del BLOQUE 7)\n",
        "if 'BREEDS' not in globals():\n",
        "    BREEDS = ['brahman', 'nelore', 'angus', 'cebuinas', 'criollo', 'pardo_suizo', 'guzerat', 'holstein']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä AN√ÅLISIS EXPLORATORIO - CID DATASET (OPCIONAL)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar si CID Dataset est√° disponible\n",
        "CID_PATH = None\n",
        "\n",
        "# 1. Buscar en variable de entorno (configurada por BLOQUE 7.5)\n",
        "if 'CID_DATASET_PATH' in os.environ:\n",
        "    env_path = Path(os.environ['CID_DATASET_PATH'])\n",
        "    if env_path.exists() and any(env_path.iterdir()):\n",
        "        CID_PATH = env_path\n",
        "        print(f\"‚úÖ CID Dataset encontrado desde variable de entorno: {CID_PATH}\")\n",
        "\n",
        "# 2. Buscar en variables globales\n",
        "if CID_PATH is None and 'cid_dataset_path' in globals() and cid_dataset_path:\n",
        "    CID_PATH = Path(cid_dataset_path)\n",
        "\n",
        "# 3. Buscar en rutas est√°ndar (si BLOQUE 7.5 se ejecut√≥)\n",
        "if CID_PATH is None and RAW_DIR.exists():\n",
        "    possible_cid_paths = [\n",
        "        RAW_DIR / 'cid' / 'CID',  # Ruta est√°ndar del BLOQUE 7.5\n",
        "        RAW_DIR / 'cid',\n",
        "        RAW_DIR / 'cid_dataset' / 'CID',\n",
        "        RAW_DIR / 'cid_dataset',\n",
        "    ]\n",
        "    for path in possible_cid_paths:\n",
        "        if path.exists() and any(path.iterdir()):\n",
        "            CID_PATH = path\n",
        "            break\n",
        "\n",
        "df_cid = None\n",
        "\n",
        "if CID_PATH and CID_PATH.exists():\n",
        "    print(f\"üîç Explorando estructura de CID en: {CID_PATH}\")\n",
        "    \n",
        "    # Buscar archivo metadata\n",
        "    metadata_files = list(CID_PATH.rglob('*.csv'))\n",
        "    \n",
        "    if metadata_files:\n",
        "        # Cargar primer CSV encontrado\n",
        "        metadata_path = metadata_files[0]\n",
        "        print(f\"üìÑ Cargando metadata: {metadata_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            df_cid = pd.read_csv(metadata_path)\n",
        "            \n",
        "            print(f\"\\nüìä Dimensiones: {df_cid.shape[0]} filas √ó {df_cid.shape[1]} columnas\")\n",
        "            print(f\"\\nüìã Columnas disponibles:\")\n",
        "            for col in df_cid.columns:\n",
        "                print(f\"  - {col}\")\n",
        "            \n",
        "            # Mostrar primeras filas\n",
        "            print(f\"\\nüëÄ Primeras 3 filas:\")\n",
        "            print(df_cid.head(3))\n",
        "            \n",
        "            # Identificar columna de peso (buscar variaciones)\n",
        "            weight_col = None\n",
        "            for col in df_cid.columns:\n",
        "                if any(keyword in col.lower() for keyword in ['weight', 'peso', 'kg', 'kilogram']):\n",
        "                    weight_col = col\n",
        "                    break\n",
        "            \n",
        "            if weight_col:\n",
        "                print(f\"\\n‚úÖ Columna de peso encontrada: '{weight_col}'\")\n",
        "                \n",
        "                # Estad√≠sticas de peso\n",
        "                print(f\"\\nüìä Estad√≠sticas de Peso:\")\n",
        "                print(df_cid[weight_col].describe())\n",
        "            \n",
        "            # Identificar columna de raza\n",
        "            breed_col = None\n",
        "            for col in df_cid.columns:\n",
        "                if any(keyword in col.lower() for keyword in ['breed', 'raza', 'race', 'type']):\n",
        "                    breed_col = col\n",
        "                    break\n",
        "            \n",
        "            if breed_col:\n",
        "                print(f\"\\n‚úÖ Columna de raza encontrada: '{breed_col}'\")\n",
        "                \n",
        "                # Distribuci√≥n por raza\n",
        "                print(f\"\\nüìä Distribuci√≥n por Raza:\")\n",
        "                breed_counts = df_cid[breed_col].value_counts()\n",
        "                print(breed_counts.head(10))\n",
        "                \n",
        "                # Mapear a razas objetivo\n",
        "                print(f\"\\nüéØ Coincidencias con razas objetivo:\")\n",
        "                for target_breed in BREEDS:\n",
        "                    matches = df_cid[breed_col].str.contains(target_breed, case=False, na=False).sum()\n",
        "                    status = \"‚úÖ\" if matches > 50 else \"‚ö†Ô∏è\" if matches > 10 else \"‚ùå\"\n",
        "                    print(f\"{status} {target_breed.capitalize()}: {matches} im√°genes\")\n",
        "            \n",
        "            print(f\"\\n‚úÖ CID Dataset analizado exitosamente\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error cargando metadata: {e}\")\n",
        "            print(\"üí° Continuando sin CID Dataset...\")\n",
        "            df_cid = None\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se encontr√≥ archivo metadata CSV\")\n",
        "        print(\"üí° CID podr√≠a tener estructura diferente\")\n",
        "        \n",
        "        # Contar im√°genes por subcarpeta\n",
        "        subdirs = [d for d in CID_PATH.iterdir() if d.is_dir()]\n",
        "        if subdirs:\n",
        "            print(f\"\\nüìÅ Subdirectorios encontrados ({len(subdirs)}):\")\n",
        "            for subdir in subdirs[:10]:\n",
        "                img_count = len(list(subdir.glob('*.jpg'))) + len(list(subdir.glob('*.png')))\n",
        "                print(f\"   üìÇ {subdir.name}: {img_count} im√°genes\")\n",
        "        else:\n",
        "            # Contar im√°genes totales\n",
        "            img_count = len(list(CID_PATH.rglob('*.jpg'))) + len(list(CID_PATH.rglob('*.png')))\n",
        "            print(f\"\\nüìä Total de im√°genes en CID: {img_count:,}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CID Dataset no disponible (opcional)\")\n",
        "    print(\"üí° Para usar CID Dataset:\")\n",
        "    print(\"   1. Ejecuta el BLOQUE 7.5 para descargar el CID Dataset desde GitHub\")\n",
        "    print(\"   2. El BLOQUE 7.5 descarga autom√°ticamente desde: https://github.com/bhuiyanmobasshir94/CID.git\")\n",
        "    print(\"   3. Despu√©s de ejecutar BLOQUE 7.5, vuelve a ejecutar este BLOQUE 12\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "if df_cid is not None:\n",
        "    print(f\"‚úÖ CID Dataset disponible: {len(df_cid):,} registros\")\n",
        "    print(f\"üí° Contin√∫a con el BLOQUE 13 para visualizaciones EDA\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è CID Dataset no disponible (opcional)\")\n",
        "    print(f\"üí° Contin√∫a con otros datasets:\")\n",
        "    print(f\"   - BLOQUE 9: Descargar dataset de Kaggle\")\n",
        "    print(f\"   - BLOQUE 10: Scraping de Google Images (opcional)\")\n",
        "    print(f\"   - BLOQUE 11: Ver resumen de datasets disponibles\")\n",
        "    print(f\"   - O usa el c√≥digo de EDA flexible mostrado en el proyecto\")\n",
        "print(f\"{'=' * 60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 13: VISUALIZACIONES EDA\n",
        "# ============================================================\n",
        "# üìä Crea gr√°ficos interactivos del an√°lisis exploratorio\n",
        "# ‚ö†Ô∏è Funciona con CID Dataset (BLOQUE 12) o dataset scrapeado (BLOQUE 10)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä VISUALIZACIONES EDA\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido\n",
        "if 'RAW_DIR' not in globals():\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "# Intentar cargar dataset disponible\n",
        "df_eda = None\n",
        "dataset_name = \"Desconocido\"\n",
        "\n",
        "# 1. Intentar usar df_cid si est√° disponible (BLOQUE 12)\n",
        "if 'df_cid' in globals() and df_cid is not None:\n",
        "    df_eda = df_cid.copy()\n",
        "    dataset_name = \"CID Dataset\"\n",
        "    print(f\"‚úÖ Usando CID Dataset: {len(df_eda):,} registros\")\n",
        "else:\n",
        "    # 2. Intentar cargar metadata del dataset scrapeado (BLOQUE 10)\n",
        "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
        "    if scraped_metadata.exists():\n",
        "        try:\n",
        "            df_eda = pd.read_csv(scraped_metadata)\n",
        "            dataset_name = \"Dataset Scrapeado\"\n",
        "            print(f\"‚úÖ Usando Dataset Scrapeado: {len(df_eda):,} registros\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error cargando metadata scrapeado: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se encontr√≥ ning√∫n dataset disponible\")\n",
        "        print(\"üí° Ejecuta el BLOQUE 10 para descargar im√°genes o el BLOQUE 12 para CID Dataset\")\n",
        "\n",
        "if df_eda is None or len(df_eda) == 0:\n",
        "    print(\"\\n‚ùå No hay datos disponibles para visualizaci√≥n\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 10 para descargar im√°genes\")\n",
        "else:\n",
        "    print(f\"\\nüìã Columnas disponibles: {list(df_eda.columns)}\")\n",
        "    \n",
        "    # Verificar columnas requeridas\n",
        "    required_cols = ['weight_kg', 'breed']\n",
        "    missing_cols = [col for col in required_cols if col not in df_eda.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"‚ùå Faltan columnas requeridas: {missing_cols}\")\n",
        "        print(\"üí° El metadata debe tener al menos: weight_kg, breed\")\n",
        "    else:\n",
        "        def create_eda_visualizations(df, dataset_name=\"Dataset\"):\n",
        "            \"\"\"Crear visualizaciones completas del EDA (adaptado a columnas disponibles)\"\"\"\n",
        "            print(f\"\\nüìä Creando visualizaciones EDA para {dataset_name}...\")\n",
        "            \n",
        "            # Determinar qu√© visualizaciones podemos hacer seg√∫n columnas disponibles\n",
        "            has_age = 'age_category' in df.columns\n",
        "            has_quality = 'image_quality' in df.columns\n",
        "            has_lighting = 'lighting' in df.columns\n",
        "            has_angle = 'angle' in df.columns\n",
        "            \n",
        "            # Configurar subplots (2x2 es suficiente)\n",
        "            subplot_titles = ['Distribuci√≥n de Peso', 'Peso por Raza']\n",
        "            if has_age:\n",
        "                subplot_titles.append('Distribuci√≥n por Edad')\n",
        "            else:\n",
        "                subplot_titles.append('Im√°genes por Raza')\n",
        "            \n",
        "            if has_quality:\n",
        "                subplot_titles.append('Calidad de Im√°genes')\n",
        "            elif has_lighting:\n",
        "                subplot_titles.append('Peso vs Iluminaci√≥n')\n",
        "            elif has_angle:\n",
        "                subplot_titles.append('Peso vs √Ångulo')\n",
        "            else:\n",
        "                subplot_titles.append('Peso Promedio por Raza')\n",
        "            \n",
        "            fig = make_subplots(\n",
        "                rows=2, cols=2,\n",
        "                subplot_titles=subplot_titles[:4],\n",
        "                specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                       [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "            )\n",
        "            \n",
        "            # 1. Distribuci√≥n de peso\n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=df['weight_kg'], nbinsx=50, name='Peso (kg)',\n",
        "                            marker_color='lightblue', opacity=0.7),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            \n",
        "            # 2. Peso por raza\n",
        "            for breed in df['breed'].unique()[:10]:  # Limitar a 10 razas para legibilidad\n",
        "                breed_data = df[df['breed'] == breed]['weight_kg']\n",
        "                if len(breed_data) > 0:\n",
        "                    fig.add_trace(\n",
        "                        go.Box(y=breed_data, name=breed, boxpoints='outliers'),\n",
        "                        row=1, col=2\n",
        "                    )\n",
        "            \n",
        "            # 3. Distribuci√≥n por edad o por raza\n",
        "            if has_age:\n",
        "                age_counts = df['age_category'].value_counts()\n",
        "                fig.add_trace(\n",
        "                    go.Bar(x=age_counts.index, y=age_counts.values, name='Categor√≠as de Edad',\n",
        "                           marker_color='lightgreen'),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "            else:\n",
        "                # Si no hay edad, mostrar distribuci√≥n por raza\n",
        "                breed_counts = df['breed'].value_counts()\n",
        "                fig.add_trace(\n",
        "                    go.Bar(x=breed_counts.index, y=breed_counts.values, name='Im√°genes por Raza',\n",
        "                           marker_color='lightcoral'),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "            \n",
        "            # 4. Calidad, Iluminaci√≥n, √Ångulo o Peso Promedio\n",
        "            if has_quality:\n",
        "                quality_counts = df['image_quality'].value_counts()\n",
        "                fig.add_trace(\n",
        "                    go.Pie(labels=quality_counts.index, values=quality_counts.values,\n",
        "                           name='Calidad'),\n",
        "                    row=2, col=2\n",
        "                )\n",
        "            elif has_lighting:\n",
        "                for lighting in df['lighting'].unique()[:5]:  # Limitar a 5\n",
        "                    lighting_data = df[df['lighting'] == lighting]['weight_kg']\n",
        "                    if len(lighting_data) > 0:\n",
        "                        fig.add_trace(\n",
        "                            go.Box(y=lighting_data, name=lighting),\n",
        "                            row=2, col=2\n",
        "                        )\n",
        "            elif has_angle:\n",
        "                for angle in df['angle'].unique()[:5]:  # Limitar a 5\n",
        "                    angle_data = df[df['angle'] == angle]['weight_kg']\n",
        "                    if len(angle_data) > 0:\n",
        "                        fig.add_trace(\n",
        "                            go.Box(y=angle_data, name=angle),\n",
        "                            row=2, col=2\n",
        "                        )\n",
        "            else:\n",
        "                # Si no hay ninguna, mostrar estad√≠sticas de peso por raza\n",
        "                breed_weights = df.groupby('breed')['weight_kg'].mean().sort_values(ascending=False)\n",
        "                fig.add_trace(\n",
        "                    go.Bar(x=breed_weights.index, y=breed_weights.values, \n",
        "                           name='Peso Promedio por Raza', marker_color='lightseagreen'),\n",
        "                    row=2, col=2\n",
        "                )\n",
        "            \n",
        "            # Configurar layout\n",
        "            fig.update_layout(\n",
        "                height=1000,\n",
        "                title_text=f\"An√°lisis Exploratorio - {dataset_name}\",\n",
        "                title_x=0.5,\n",
        "                showlegend=True\n",
        "            )\n",
        "            \n",
        "            # Mostrar gr√°fico\n",
        "            fig.show()\n",
        "            \n",
        "            # Guardar gr√°fico\n",
        "            if 'DATA_DIR' in globals():\n",
        "                output_path = DATA_DIR / 'eda_visualizations.html'\n",
        "            else:\n",
        "                output_path = RAW_DIR.parent / 'processed' / 'eda_visualizations.html'\n",
        "                output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            fig.write_html(str(output_path))\n",
        "            print(f\"üíæ Visualizaciones guardadas en: {output_path}\")\n",
        "            \n",
        "            return fig\n",
        "        \n",
        "        # Ejecutar visualizaciones\n",
        "        try:\n",
        "            eda_fig = create_eda_visualizations(df_eda, dataset_name)\n",
        "            print(f\"\\n‚úÖ Visualizaciones EDA completadas exitosamente\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error creando visualizaciones: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 14: AN√ÅLISIS ESPEC√çFICO POR RAZA\n",
        "# ============================================================\n",
        "# üêÑ Analiza qu√© razas tienen suficientes datos para entrenamiento\n",
        "# ‚ö†Ô∏è Funciona con CID Dataset (BLOQUE 12) o dataset scrapeado (BLOQUE 10)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üêÑ AN√ÅLISIS POR RAZA PARA ENTRENAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido\n",
        "if 'RAW_DIR' not in globals():\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "# Intentar cargar dataset disponible\n",
        "df_breed_analysis = None\n",
        "dataset_name = \"Desconocido\"\n",
        "\n",
        "# 1. Intentar usar df_cid si est√° disponible (BLOQUE 12)\n",
        "if 'df_cid' in globals() and df_cid is not None:\n",
        "    df_breed_analysis = df_cid.copy()\n",
        "    dataset_name = \"CID Dataset\"\n",
        "    print(f\"‚úÖ Usando CID Dataset: {len(df_breed_analysis):,} registros\")\n",
        "else:\n",
        "    # 2. Intentar cargar metadata del dataset scrapeado (BLOQUE 10)\n",
        "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
        "    if scraped_metadata.exists():\n",
        "        try:\n",
        "            df_breed_analysis = pd.read_csv(scraped_metadata)\n",
        "            dataset_name = \"Dataset Scrapeado\"\n",
        "            print(f\"‚úÖ Usando Dataset Scrapeado: {len(df_breed_analysis):,} registros\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error cargando metadata scrapeado: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se encontr√≥ ning√∫n dataset disponible\")\n",
        "        print(\"üí° Ejecuta el BLOQUE 10 para descargar im√°genes o el BLOQUE 12 para CID Dataset\")\n",
        "\n",
        "if df_breed_analysis is None or len(df_breed_analysis) == 0:\n",
        "    print(\"\\n‚ùå No hay datos disponibles para an√°lisis\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 10 para descargar im√°genes\")\n",
        "else:\n",
        "    # Verificar columnas requeridas\n",
        "    if 'breed' not in df_breed_analysis.columns or 'weight_kg' not in df_breed_analysis.columns:\n",
        "        print(\"‚ùå El dataset debe tener columnas 'breed' y 'weight_kg'\")\n",
        "        print(f\"üí° Columnas disponibles: {list(df_breed_analysis.columns)}\")\n",
        "    else:\n",
        "        def analyze_breeds_for_training(df):\n",
        "            \"\"\"Analizar qu√© razas est√°n bien representadas para entrenamiento\"\"\"\n",
        "            print(f\"\\nüìä Analizando razas en {dataset_name}...\")\n",
        "            print(\"=\" * 50)\n",
        "            \n",
        "            # Razas objetivo del proyecto\n",
        "            target_breeds = ['brahman', 'nelore', 'angus', 'cebuinas', 'criollo', 'pardo_suizo', 'jersey']\n",
        "            \n",
        "            breed_analysis = []\n",
        "            \n",
        "            for breed in target_breeds:\n",
        "                # Buscar la raza en el dataset (case-insensitive)\n",
        "                breed_mask = df['breed'].str.lower() == breed.lower()\n",
        "                breed_data = df[breed_mask]\n",
        "                \n",
        "                if len(breed_data) > 0:\n",
        "                    count = len(breed_data)\n",
        "                    avg_weight = breed_data['weight_kg'].mean()\n",
        "                    std_weight = breed_data['weight_kg'].std()\n",
        "                    \n",
        "                    # Ajustar umbrales para dataset m√°s peque√±o (100+ es suficiente)\n",
        "                    status = \"‚úÖ Suficiente\" if count >= 200 else \"‚úÖ Bueno\" if count >= 100 else \"‚ö†Ô∏è Limitado\" if count >= 50 else \"‚ùå Insuficiente\"\n",
        "                    \n",
        "                    strategy = 'Direct training' if count >= 200 else 'Transfer learning' if count >= 100 else 'Data augmentation' if count >= 50 else 'Data collection'\n",
        "                else:\n",
        "                    # Si no se encuentra la raza exacta, buscar variaciones\n",
        "                    breed_variations = [breed, breed.replace('_', ' '), breed.capitalize()]\n",
        "                    count = 0\n",
        "                    avg_weight = 0\n",
        "                    std_weight = 0\n",
        "                    \n",
        "                    for variation in breed_variations:\n",
        "                        mask = df['breed'].str.lower().str.contains(variation.lower(), na=False)\n",
        "                        if mask.sum() > 0:\n",
        "                            breed_data = df[mask]\n",
        "                            count = len(breed_data)\n",
        "                            avg_weight = breed_data['weight_kg'].mean()\n",
        "                            std_weight = breed_data['weight_kg'].std()\n",
        "                            break\n",
        "                    \n",
        "                    status = \"‚úÖ Bueno\" if count >= 100 else \"‚ö†Ô∏è Limitado\" if count >= 50 else \"‚ùå No encontrado\"\n",
        "                    strategy = 'Transfer learning' if count >= 100 else 'Data augmentation' if count >= 50 else 'Data collection'\n",
        "                \n",
        "                breed_analysis.append({\n",
        "                    'breed': breed,\n",
        "                    'images_available': count,\n",
        "                    'avg_weight_kg': round(avg_weight, 1) if avg_weight > 0 else 0,\n",
        "                    'std_weight_kg': round(std_weight, 1) if std_weight > 0 else 0,\n",
        "                    'status': status,\n",
        "                    'strategy': strategy\n",
        "                })\n",
        "            \n",
        "            # Crear DataFrame\n",
        "            df_result = pd.DataFrame(breed_analysis)\n",
        "            \n",
        "            # Mostrar tabla\n",
        "            print(\"\\nüìä AN√ÅLISIS POR RAZA:\")\n",
        "            print(df_result.to_string(index=False))\n",
        "            \n",
        "            # Guardar an√°lisis\n",
        "            if 'DATA_DIR' in globals():\n",
        "                output_path = DATA_DIR / 'breed_analysis.csv'\n",
        "            else:\n",
        "                output_path = RAW_DIR.parent / 'processed' / 'breed_analysis.csv'\n",
        "                output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            df_result.to_csv(output_path, index=False)\n",
        "            print(f\"\\nüíæ An√°lisis por raza guardado en: {output_path}\")\n",
        "            \n",
        "            # Recomendaciones adaptadas a dataset m√°s peque√±o\n",
        "            print(f\"\\nüéØ RECOMENDACIONES PARA ENTRENAMIENTO:\")\n",
        "            \n",
        "            # Para dataset de 1,269 im√°genes, ajustar umbrales\n",
        "            sufficient_breeds = df_result[df_result['images_available'] >= 200]\n",
        "            if len(sufficient_breeds) > 0:\n",
        "                print(f\"‚úÖ Entrenamiento directo (200+ im√°genes): {', '.join(sufficient_breeds['breed'].tolist())}\")\n",
        "            \n",
        "            good_breeds = df_result[(df_result['images_available'] >= 100) & (df_result['images_available'] < 200)]\n",
        "            if len(good_breeds) > 0:\n",
        "                print(f\"‚úÖ Transfer learning recomendado (100-199 im√°genes): {', '.join(good_breeds['breed'].tolist())}\")\n",
        "            \n",
        "            limited_breeds = df_result[(df_result['images_available'] >= 50) & (df_result['images_available'] < 100)]\n",
        "            if len(limited_breeds) > 0:\n",
        "                print(f\"‚ö†Ô∏è Data augmentation necesario (50-99 im√°genes): {', '.join(limited_breeds['breed'].tolist())}\")\n",
        "            \n",
        "            insufficient_breeds = df_result[df_result['images_available'] < 50]\n",
        "            if len(insufficient_breeds) > 0:\n",
        "                print(f\"‚ùå Recolecci√≥n requerida (< 50 im√°genes): {', '.join(insufficient_breeds['breed'].tolist())}\")\n",
        "            \n",
        "            # Resumen general\n",
        "            total_images = df_result['images_available'].sum()\n",
        "            print(f\"\\nüìà RESUMEN GENERAL:\")\n",
        "            print(f\"   - Total im√°genes: {total_images:,}\")\n",
        "            print(f\"   - Razas con 100+ im√°genes: {len(df_result[df_result['images_available'] >= 100])}/{len(target_breeds)}\")\n",
        "            print(f\"   - Razas con 150+ im√°genes: {len(df_result[df_result['images_available'] >= 150])}/{len(target_breeds)}\")\n",
        "            \n",
        "            if total_images >= 700:\n",
        "                print(f\"\\n‚úÖ Dataset suficiente para entrenamiento ({total_images:,} im√°genes)\")\n",
        "                print(f\"üí° Puedes continuar con el BLOQUE 15 (Pipeline de Datos)\")\n",
        "            elif total_images >= 350:\n",
        "                print(f\"\\n‚ö†Ô∏è Dataset aceptable pero limitado ({total_images:,} im√°genes)\")\n",
        "                print(f\"üí° Considera descargar m√°s im√°genes o usar data augmentation agresivo\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå Dataset insuficiente ({total_images:,} im√°genes)\")\n",
        "                print(f\"üí° Ejecuta el BLOQUE 10 nuevamente para descargar m√°s im√°genes\")\n",
        "            \n",
        "            return df_result\n",
        "        \n",
        "        # Ejecutar an√°lisis por raza\n",
        "        try:\n",
        "            breed_analysis = analyze_breeds_for_training(df_breed_analysis)\n",
        "            print(f\"\\n‚úÖ An√°lisis por raza completado exitosamente\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error en an√°lisis por raza: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß D√≠a 5-6: Preparar Pipeline de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 15: PIPELINE DE DATOS OPTIMIZADO\n",
        "# ============================================================\n",
        "# üîß Crea pipeline de datos usando m√≥dulos del proyecto\n",
        "# ‚ö†Ô∏è Funciona con CID Dataset (BLOQUE 12) o dataset scrapeado (BLOQUE 10)\n",
        "# üí° Usa: CattleDataGenerator (data.data_loader) y get_aggressive_augmentation (data.augmentation)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
        "try:\n",
        "    from data.data_loader import CattleDataGenerator\n",
        "    from data.augmentation import get_aggressive_augmentation, get_validation_transform\n",
        "    print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importando m√≥dulos del proyecto: {e}\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
        "    raise\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîß PIPELINE DE DATOS OPTIMIZADO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que RAW_DIR est√° definido\n",
        "if 'RAW_DIR' not in globals():\n",
        "    if 'BASE_DIR' in globals():\n",
        "        RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "    else:\n",
        "        RAW_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation/data/raw')\n",
        "\n",
        "# Intentar cargar dataset disponible\n",
        "df_pipeline = None\n",
        "dataset_name = \"Desconocido\"\n",
        "base_data_dir = None\n",
        "\n",
        "# 1. Intentar usar df_cid si est√° disponible (BLOQUE 12)\n",
        "if 'df_cid' in globals() and df_cid is not None:\n",
        "    df_pipeline = df_cid.copy()\n",
        "    dataset_name = \"CID Dataset\"\n",
        "    base_data_dir = RAW_DIR / 'cid' / 'CID'\n",
        "    print(f\"‚úÖ Usando CID Dataset: {len(df_pipeline):,} registros\")\n",
        "else:\n",
        "    # 2. Intentar cargar metadata del dataset scrapeado (BLOQUE 10)\n",
        "    scraped_metadata = RAW_DIR / 'scraped' / 'metadata.csv'\n",
        "    if scraped_metadata.exists():\n",
        "        try:\n",
        "            df_pipeline = pd.read_csv(scraped_metadata)\n",
        "            dataset_name = \"Dataset Scrapeado\"\n",
        "            base_data_dir = RAW_DIR / 'scraped'\n",
        "            print(f\"‚úÖ Usando Dataset Scrapeado: {len(df_pipeline):,} registros\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error cargando metadata scrapeado: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se encontr√≥ ning√∫n dataset disponible\")\n",
        "        print(\"üí° Ejecuta el BLOQUE 10 para descargar im√°genes o el BLOQUE 12 para CID Dataset\")\n",
        "\n",
        "if df_pipeline is None or len(df_pipeline) == 0:\n",
        "    print(\"\\n‚ùå No hay datos disponibles para pipeline\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 10 para descargar im√°genes\")\n",
        "    raise ValueError(\"No hay dataset disponible para crear el pipeline\")\n",
        "\n",
        "# Verificar y ajustar columnas para CattleDataGenerator\n",
        "# CattleDataGenerator espera: 'image_filename', 'weight_kg', 'breed'\n",
        "if 'image_path' in df_pipeline.columns:\n",
        "    # Convertir image_path a image_filename (ruta relativa)\n",
        "    if base_data_dir:\n",
        "        def get_relative_path(path_str):\n",
        "            path = Path(path_str)\n",
        "            if path.is_absolute():\n",
        "                try:\n",
        "                    return path.relative_to(base_data_dir)\n",
        "                except ValueError:\n",
        "                    return Path(path.name)  # Si no es relativo, usar solo el nombre\n",
        "            return path\n",
        "        df_pipeline['image_filename'] = df_pipeline['image_path'].apply(get_relative_path)\n",
        "    else:\n",
        "        df_pipeline['image_filename'] = df_pipeline['image_path'].apply(lambda x: Path(x).name)\n",
        "\n",
        "# Verificar columnas requeridas\n",
        "required_cols = ['image_filename', 'weight_kg', 'breed']\n",
        "missing_cols = [col for col in required_cols if col not in df_pipeline.columns]\n",
        "if missing_cols:\n",
        "    print(f\"‚ùå Faltan columnas requeridas: {missing_cols}\")\n",
        "    print(f\"üí° Columnas disponibles: {list(df_pipeline.columns)}\")\n",
        "    raise ValueError(f\"Columnas requeridas faltantes: {missing_cols}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset cargado: {len(df_pipeline):,} registros\")\n",
        "print(f\"üìä Columnas: {list(df_pipeline.columns)}\")\n",
        "print(f\"üìÅ Directorio base de im√°genes: {base_data_dir}\")\n",
        "\n",
        "# Dividir datos en train/val/test\n",
        "print(\"\\nüìä Dividiendo datos en train/val/test...\")\n",
        "df_shuffled = df_pipeline.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "n_total = len(df_shuffled)\n",
        "n_train = int(n_total * (1 - CONFIG['validation_split'] - CONFIG['test_split']))\n",
        "n_val = int(n_total * CONFIG['validation_split'])\n",
        "\n",
        "df_train = df_shuffled[:n_train]\n",
        "df_val = df_shuffled[n_train:n_train + n_val]\n",
        "df_test = df_shuffled[n_train + n_val:]\n",
        "\n",
        "print(f\"üìà Train: {len(df_train):,} ({len(df_train)/n_total*100:.1f}%)\")\n",
        "print(f\"üìà Val: {len(df_val):,} ({len(df_val)/n_total*100:.1f}%)\")\n",
        "print(f\"üìà Test: {len(df_test):,} ({len(df_test)/n_total*100:.1f}%)\")\n",
        "\n",
        "# Crear generadores usando m√≥dulos del proyecto\n",
        "print(f\"\\nüîß Creando generadores de datos usando CattleDataGenerator...\")\n",
        "\n",
        "# Augmentation para entrenamiento (agresivo para dataset peque√±o)\n",
        "train_transform = get_aggressive_augmentation(image_size=CONFIG['image_size'])\n",
        "val_transform = get_validation_transform(image_size=CONFIG['image_size'])\n",
        "\n",
        "# Crear generadores\n",
        "train_generator = CattleDataGenerator(\n",
        "    annotations_df=df_train,\n",
        "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    image_size=CONFIG['image_size'],\n",
        "    transform=train_transform,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = CattleDataGenerator(\n",
        "    annotations_df=df_val,\n",
        "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    image_size=CONFIG['image_size'],\n",
        "    transform=val_transform,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = CattleDataGenerator(\n",
        "    annotations_df=df_test,\n",
        "    images_dir=base_data_dir if base_data_dir else RAW_DIR,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    image_size=CONFIG['image_size'],\n",
        "    transform=val_transform,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ BLOQUE 15 COMPLETADO\")\n",
        "print(f\"üìä Generadores creados usando m√≥dulos del proyecto:\")\n",
        "print(f\"   - Train: {len(df_train):,} im√°genes ({len(train_generator)} batches)\")\n",
        "print(f\"   - Val: {len(df_val):,} im√°genes ({len(val_generator)} batches)\")\n",
        "print(f\"   - Test: {len(df_test):,} im√°genes ({len(test_generator)} batches)\")\n",
        "print(f\"üí° Contin√∫a con el BLOQUE 16 para crear la arquitectura del modelo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 16: ARQUITECTURA DEL MODELO\n",
        "# ============================================================\n",
        "# üèóÔ∏è Crea modelo usando m√≥dulos del proyecto\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 15 ejecutado (generadores creados)\n",
        "# üí° Usa: BreedWeightEstimatorCNN.build_generic_model() (models.cnn_architecture)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üèóÔ∏è ARQUITECTURA DEL MODELO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
        "try:\n",
        "    from models.cnn_architecture import BreedWeightEstimatorCNN\n",
        "    print(\"‚úÖ M√≥dulo BreedWeightEstimatorCNN importado correctamente\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
        "    raise\n",
        "\n",
        "# Crear modelo gen√©rico usando m√≥dulo del proyecto\n",
        "print(\"üèóÔ∏è Creando modelo gen√©rico usando BreedWeightEstimatorCNN...\")\n",
        "print(f\"üìä Configuraci√≥n:\")\n",
        "print(f\"   - Image size: {CONFIG['image_size']}\")\n",
        "print(f\"   - Base architecture: EfficientNetB1 (desde m√≥dulo)\")\n",
        "\n",
        "# Usar build_generic_model del m√≥dulo del proyecto\n",
        "model = BreedWeightEstimatorCNN.build_generic_model(\n",
        "    input_shape=CONFIG['image_size'] + (3,),\n",
        "    base_architecture='efficientnetb1'  # O 'mobilenetv2' para m√°s r√°pido\n",
        ")\n",
        "\n",
        "# Re-compilar con learning rate personalizado\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
        "    loss='mse',\n",
        "    metrics=['mae', 'mse']\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo creado con {model.count_params():,} par√°metros\")\n",
        "print(f\"üìä Arquitectura: {model.name}\")\n",
        "print(f\"üí° Usando m√≥dulo del proyecto: models.cnn_architecture.BreedWeightEstimatorCNN\")\n",
        "\n",
        "# Mostrar resumen\n",
        "print(f\"\\nüìê Resumen del modelo:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 17: CONFIGURACI√ìN DE ENTRENAMIENTO\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Configura callbacks (EarlyStopping, ReduceLR, ModelCheckpoint, TensorBoard)\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 16 ejecutado (modelo creado)\n",
        "\n",
        "def setup_training_callbacks():\n",
        "    \"\"\"Configurar callbacks para entrenamiento\"\"\"\n",
        "    print(\"‚öôÔ∏è Configurando callbacks de entrenamiento...\")\n",
        "    \n",
        "    callbacks_list = [\n",
        "        # Early stopping\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=CONFIG['early_stopping_patience'],\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Reduce learning rate on plateau\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Model checkpoint\n",
        "        callbacks.ModelCheckpoint(\n",
        "            filepath=str(MODELS_DIR / 'best_model.h5'),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # TensorBoard\n",
        "        callbacks.TensorBoard(\n",
        "            log_dir=str(BASE_DIR / 'logs'),\n",
        "            histogram_freq=1,\n",
        "            write_graph=True,\n",
        "            write_images=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úÖ {len(callbacks_list)} callbacks configurados\")\n",
        "    return callbacks_list\n",
        "\n",
        "# Configurar callbacks\n",
        "training_callbacks = setup_training_callbacks()\n",
        "\n",
        "# Configurar MLflow\n",
        "def start_mlflow_run():\n",
        "    \"\"\"Iniciar run de MLflow\"\"\"\n",
        "    # Detectar nombre del dataset usado\n",
        "    dataset_used = 'Scraped'\n",
        "    if 'df_cid' in globals() and df_cid is not None:\n",
        "        dataset_used = 'CID'\n",
        "    elif 'dataset_name' in globals():\n",
        "        dataset_used = dataset_name.replace('Dataset', '').strip()\n",
        "    \n",
        "    run = mlflow.start_run(run_name=f\"cattle-weight-{dataset_used.lower()}-model\")\n",
        "\n",
        "    mlflow.log_params({\n",
        "        'dataset': dataset_used,\n",
        "        'model': 'EfficientNetB0',\n",
        "        'batch_size': CONFIG['batch_size'],\n",
        "        'learning_rate': CONFIG['learning_rate'],\n",
        "        'epochs': CONFIG['epochs'],\n",
        "        'image_size': CONFIG['image_size'],\n",
        "        'augmentation': 'Albumentations'\n",
        "    })\n",
        "\n",
        "    print(f\"üî¨ MLflow run iniciado: {run.info.run_id}\")\n",
        "    print(f\"üìä Dataset registrado: {dataset_used}\")\n",
        "    return run\n",
        "\n",
        "# Iniciar MLflow run\n",
        "mlflow_run = start_mlflow_run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 18: ENTRENAMIENTO DEL MODELO\n",
        "# ============================================================\n",
        "# üöÄ Entrena el modelo base (puede tardar horas con GPU)\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 17 ejecutado (callbacks configurados)\n",
        "# ‚ö†Ô∏è Tiempo estimado: 2-4 horas con GPU T4 (100 √©pocas)\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Entrenar modelo base usando generadores del proyecto\"\"\"\n",
        "    print(\"üöÄ Iniciando entrenamiento del modelo base...\")\n",
        "    print(f\"üìä Configuraci√≥n: {CONFIG}\")\n",
        "    \n",
        "    # Verificar que los generadores existen\n",
        "    if 'train_generator' not in globals() or 'val_generator' not in globals():\n",
        "        raise ValueError(\"Generadores no encontrados. Ejecuta el BLOQUE 15 primero.\")\n",
        "    \n",
        "    # Calcular steps por √©poca (usando generadores)\n",
        "    steps_per_epoch = len(train_generator)\n",
        "    validation_steps = len(val_generator)\n",
        "    \n",
        "    print(f\"üìà Steps por √©poca: {steps_per_epoch}\")\n",
        "    print(f\"üìà Validation steps: {validation_steps}\")\n",
        "    print(f\"üí° Usando generadores del proyecto (CattleDataGenerator)\")\n",
        "    \n",
        "    # Entrenar modelo usando generadores\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=CONFIG['epochs'],\n",
        "        validation_data=val_generator,\n",
        "        callbacks=training_callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Entrenamiento completado\")\n",
        "    return history\n",
        "\n",
        "# Entrenamiento real (requiere generadores preparados y tiempo de ejecuci√≥n con GPU)\n",
        "history = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 19: EVALUACI√ìN DEL MODELO\n",
        "# ============================================================\n",
        "# üìä Eval√∫a el modelo usando m√≥dulos del proyecto\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 18 ejecutado (modelo entrenado)\n",
        "# üí° Usa: MetricsCalculator (models.evaluation.metrics)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä EVALUACI√ìN DEL MODELO (USANDO M√ìDULOS DEL PROYECTO)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
        "try:\n",
        "    from models.evaluation.metrics import MetricsCalculator, ModelMetrics\n",
        "    print(\"‚úÖ M√≥dulo MetricsCalculator importado correctamente\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
        "    raise\n",
        "\n",
        "# Verificar que el generador de test existe\n",
        "if 'test_generator' not in globals():\n",
        "    raise ValueError(\"Generador de test no encontrado. Ejecuta el BLOQUE 15 primero.\")\n",
        "\n",
        "def evaluate_model():\n",
        "    \"\"\"Evaluar modelo en conjunto de test usando MetricsCalculator\"\"\"\n",
        "    print(\"üìä Evaluando modelo en conjunto de test...\")\n",
        "    \n",
        "    # Obtener predicciones y valores reales\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    print(\"üîç Generando predicciones...\")\n",
        "    for i in range(len(test_generator)):\n",
        "        batch_images, batch_targets = test_generator[i]\n",
        "        predictions = model.predict(batch_images, verbose=0)\n",
        "        y_true.extend(batch_targets.flatten())\n",
        "        y_pred.extend(predictions.flatten())\n",
        "    \n",
        "    # Convertir a numpy arrays\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    \n",
        "    print(f\"üìä Total predicciones: {len(y_true):,}\")\n",
        "    \n",
        "    # Calcular m√©tricas usando m√≥dulo del proyecto\n",
        "    print(\"\\nüìà Calculando m√©tricas usando MetricsCalculator...\")\n",
        "    metrics = MetricsCalculator.calculate_metrics(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        breed_type='generic'\n",
        "    )\n",
        "    \n",
        "    # Mostrar resultados\n",
        "    print(f\"\\nüìà RESULTADOS DE EVALUACI√ìN:\")\n",
        "    print(f\"   R¬≤: {metrics.r2_score:.4f}\")\n",
        "    print(f\"   MAE: {metrics.mae_kg:.2f} kg\")\n",
        "    print(f\"   MSE: {metrics.mse_kg:.2f}\")\n",
        "    print(f\"   MAPE: {metrics.mape_percent:.2f}%\")\n",
        "    print(f\"   Bias: {metrics.bias_kg:.2f} kg\")\n",
        "    \n",
        "    # Verificar objetivos (con validaci√≥n opcional)\n",
        "    print(f\"\\nüéØ VERIFICACI√ìN DE OBJETIVOS:\")\n",
        "    r2_ok = metrics.r2_score >= CONFIG['target_r2']\n",
        "    mae_ok = metrics.mae_kg < CONFIG['max_mae']\n",
        "    \n",
        "    print(f\"   R¬≤ ‚â• {CONFIG['target_r2']}: {'‚úÖ' if r2_ok else '‚ö†Ô∏è'} ({metrics.r2_score:.4f})\")\n",
        "    print(f\"   MAE < {CONFIG['max_mae']} kg: {'‚úÖ' if mae_ok else '‚ö†Ô∏è'} ({metrics.mae_kg:.2f} kg)\")\n",
        "    \n",
        "    if not (r2_ok and mae_ok):\n",
        "        print(f\"\\nüí° NOTA: Los objetivos no se cumplieron completamente\")\n",
        "        print(f\"üí° Esto es normal en un primer entrenamiento. Puedes:\")\n",
        "        print(f\"   - Ajustar hiperpar√°metros\")\n",
        "        print(f\"   - Entrenar m√°s √©pocas\")\n",
        "        print(f\"   - Usar fine-tuning\")\n",
        "    \n",
        "    # Log m√©tricas en MLflow\n",
        "    if 'mlflow' in globals() and ('mlflow_available' in globals() and mlflow_available):\n",
        "        mlflow.log_metrics({\n",
        "            'test_r2': metrics.r2_score,\n",
        "            'test_mae_kg': metrics.mae_kg,\n",
        "            'test_mse_kg': metrics.mse_kg,\n",
        "            'test_mape_percent': metrics.mape_percent,\n",
        "            'test_bias_kg': metrics.bias_kg\n",
        "        })\n",
        "    \n",
        "    return metrics.to_dict()\n",
        "\n",
        "# Evaluar modelo\n",
        "evaluation_results = evaluate_model()\n",
        "\n",
        "print(f\"\\n‚úÖ BLOQUE 19 COMPLETADO\")\n",
        "print(f\"üí° M√©tricas calculadas usando m√≥dulo del proyecto: MetricsCalculator\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 20: EXPORTAR A TFLITE\n",
        "# ============================================================\n",
        "# üì± Exporta modelo usando m√≥dulos del proyecto\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 19 ejecutado (modelo evaluado)\n",
        "# üí° Usa: TFLiteExporter (models.export.tflite_converter)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì± EXPORTAR A TFLITE (USANDO M√ìDULOS DEL PROYECTO)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Verificar que los m√≥dulos est√°n importados (BLOQUE 2)\n",
        "try:\n",
        "    from models.export.tflite_converter import TFLiteExporter\n",
        "    print(\"‚úÖ M√≥dulo TFLiteExporter importado correctamente\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importando m√≥dulo del proyecto: {e}\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 2 primero para importar los m√≥dulos\")\n",
        "    raise\n",
        "\n",
        "# Guardar modelo temporalmente para conversi√≥n\n",
        "print(\"üíæ Guardando modelo temporalmente para conversi√≥n...\")\n",
        "temp_model_path = MODELS_DIR / 'temp_model.h5'\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "model.save(str(temp_model_path))\n",
        "print(f\"‚úÖ Modelo guardado en: {temp_model_path}\")\n",
        "\n",
        "# Exportar usando TFLiteExporter del proyecto\n",
        "tflite_path = MODELS_DIR / 'generic-cattle-v1.0.0.tflite'\n",
        "\n",
        "print(f\"\\nüì± Exportando modelo a TFLite usando TFLiteExporter...\")\n",
        "print(f\"üìÅ Archivo de salida: {tflite_path}\")\n",
        "\n",
        "# Usar TFLiteExporter del proyecto (optimizaci√≥n FP16 por defecto)\n",
        "model_size_bytes = TFLiteExporter.convert_to_tflite(\n",
        "    saved_model_path=str(temp_model_path),\n",
        "    output_path=str(tflite_path),\n",
        "    optimization='default'  # FP16: reduce 2x el tama√±o, mantiene precisi√≥n\n",
        ")\n",
        "\n",
        "model_size_kb = model_size_bytes / 1024\n",
        "model_size_mb = model_size_kb / 1024\n",
        "\n",
        "# Log en MLflow\n",
        "if 'mlflow' in globals() and ('mlflow_available' in globals() and mlflow_available):\n",
        "    mlflow.log_artifact(str(tflite_path))\n",
        "    mlflow.log_metric('model_size_kb', model_size_kb)\n",
        "    mlflow.log_metric('model_size_mb', model_size_mb)\n",
        "\n",
        "# Limpiar modelo temporal\n",
        "try:\n",
        "    temp_model_path.unlink()\n",
        "    print(f\"üßπ Modelo temporal eliminado\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(f\"\\n‚úÖ BLOQUE 20 COMPLETADO\")\n",
        "print(f\"üéØ MODELO BASE LISTO PARA INTEGRACI√ìN\")\n",
        "print(f\"üìÅ Archivo: {tflite_path}\")\n",
        "print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
        "print(f\"üí° Usando m√≥dulo del proyecto: TFLiteExporter\")\n",
        "if 'mlflow_run' in globals():\n",
        "    print(f\"üî¨ MLflow run: {mlflow_run.info.run_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Resumen y Pr√≥ximos Pasos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 21: RESUMEN FINAL\n",
        "# ============================================================\n",
        "# üìã Genera resumen completo del trabajo realizado\n",
        "# ‚ö†Ô∏è Requiere: Todos los bloques anteriores ejecutados\n",
        "# üíæ Guarda resumen en DATA_DIR/final_summary.json\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def generate_final_summary():\n",
        "    \"\"\"Generar resumen final del trabajo realizado\"\"\"\n",
        "    print(\"üìã RESUMEN FINAL - PERSONA 2: SETUP ML\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Resumen de datasets\n",
        "    print(f\"\\nüì• DATASETS PROCESADOS:\")\n",
        "    \n",
        "    # CID Dataset\n",
        "    cid_images = 0\n",
        "    if 'df_cid' in globals() and df_cid is not None:\n",
        "        cid_images = len(df_cid)\n",
        "        print(f\"   ‚úÖ CID Dataset: {cid_images:,} im√°genes\")\n",
        "    elif 'datasets_summary' in globals():\n",
        "        try:\n",
        "            cid_row = datasets_summary[datasets_summary['name'] == 'CID Dataset']\n",
        "            cid_images = int(cid_row['images'].iloc[0]) if not cid_row.empty else 0\n",
        "            print(f\"   {'‚úÖ' if cid_images else '‚ö†Ô∏è'} CID Dataset: {cid_images:,} im√°genes\")\n",
        "        except:\n",
        "            print(f\"   ‚ö†Ô∏è CID Dataset: No disponible\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è CID Dataset: No disponible\")\n",
        "    \n",
        "    # Scraped Dataset\n",
        "    scraped_count = 0\n",
        "    if 'scraped_images' in globals():\n",
        "        scraped_count = scraped_images\n",
        "        print(f\"   ‚úÖ Google Images Scraped: {scraped_count:,} im√°genes\")\n",
        "    elif 'df_pipeline' in globals():\n",
        "        scraped_count = len(df_pipeline)\n",
        "        print(f\"   ‚úÖ Dataset Scrapeado: {scraped_count:,} im√°genes\")\n",
        "    elif 'datasets_summary' in globals():\n",
        "        try:\n",
        "            scraped_row = datasets_summary[datasets_summary['name'] == 'Google Images Scraped']\n",
        "            scraped_count = int(scraped_row['images'].iloc[0]) if not scraped_row.empty else 0\n",
        "            print(f\"   {'‚úÖ' if scraped_count else '‚ö†Ô∏è'} Google Images Scraped: {scraped_count:,} im√°genes\")\n",
        "        except:\n",
        "            print(f\"   ‚ö†Ô∏è Google Images Scraped: No disponible\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Google Images Scraped: No disponible\")\n",
        "    \n",
        "    # Kaggle Dataset\n",
        "    if 'kaggle_dataset_path' in globals() and kaggle_dataset_path and Path(kaggle_dataset_path).exists():\n",
        "        kaggle_images = len(list(Path(kaggle_dataset_path).glob('**/*.jpg')))\n",
        "        kaggle_id = KAGGLE_DATASET_ID if 'KAGGLE_DATASET_ID' in globals() else 'N/A'\n",
        "        status_icon = '‚úÖ' if kaggle_images else '‚ö†Ô∏è'\n",
        "        print(f\"   {status_icon} Kaggle Dataset ({kaggle_id}): {kaggle_images:,} im√°genes\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è Kaggle Dataset: Pendiente configuraci√≥n (opcional)\")\n",
        "    \n",
        "    # Resumen de an√°lisis\n",
        "    print(f\"\\nüìä AN√ÅLISIS COMPLETADO:\")\n",
        "    print(f\"   ‚úÖ EDA completo con visualizaciones\")\n",
        "    print(f\"   ‚úÖ An√°lisis por raza para estrategia de entrenamiento\")\n",
        "    print(f\"   ‚úÖ Pipeline de datos optimizado\")\n",
        "    \n",
        "    # Resumen de modelo\n",
        "    print(f\"\\nü§ñ MODELO BASE:\")\n",
        "    if 'model' in globals():\n",
        "        print(f\"   ‚úÖ Arquitectura: {model.name if hasattr(model, 'name') else 'EfficientNetB1'}\")\n",
        "        print(f\"   ‚úÖ Par√°metros: {model.count_params():,}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Modelo: No disponible\")\n",
        "    \n",
        "    if 'model_size_kb' in globals():\n",
        "        print(f\"   ‚úÖ TFLite exportado: {model_size_kb / 1024:.2f} MB ({model_size_kb:.1f} KB)\")\n",
        "    elif 'model_size' in globals():\n",
        "        print(f\"   ‚úÖ TFLite exportado: {model_size / 1024:.2f} MB ({model_size:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è TFLite: No exportado a√∫n\")\n",
        "    \n",
        "    if 'mlflow_run' in globals():\n",
        "        print(f\"   ‚úÖ MLflow tracking: {mlflow_run.info.run_id}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è MLflow: No disponible\")\n",
        "    \n",
        "    # Pr√≥ximos pasos\n",
        "    print(f\"\\nüéØ PR√ìXIMOS PASOS:\")\n",
        "    print(f\"   1. üîÑ Fine-tuning por raza (Semanas 3-6)\")\n",
        "    print(f\"   2. üì∏ Recolecci√≥n Criollo + Pardo Suizo (Semanas 7-8)\")\n",
        "    print(f\"   3. üß™ Entrenamiento final (Semanas 9-10)\")\n",
        "    print(f\"   4. üì± Integraci√≥n en app m√≥vil\")\n",
        "    \n",
        "    # Guardar resumen\n",
        "    total_images = cid_images + scraped_count\n",
        "    datasets_count = sum([1 if cid_images > 0 else 0, 1 if scraped_count > 0 else 0])\n",
        "    \n",
        "    # Obtener tama√±o del modelo\n",
        "    model_size_value = 0\n",
        "    if 'model_size_kb' in globals():\n",
        "        model_size_value = model_size_kb\n",
        "    elif 'model_size' in globals():\n",
        "        model_size_value = model_size\n",
        "    \n",
        "    # Obtener arquitectura del modelo\n",
        "    model_arch = 'EfficientNetB1'\n",
        "    if 'model' in globals() and hasattr(model, 'name'):\n",
        "        model_arch = model.name\n",
        "    \n",
        "    summary_data = {\n",
        "        'completion_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'datasets_processed': datasets_count,\n",
        "        'total_images': total_images,\n",
        "        'cid_images': cid_images,\n",
        "        'scraped_images': scraped_count,\n",
        "        'model_architecture': model_arch,\n",
        "        'model_size_kb': model_size_value,\n",
        "        'mlflow_run_id': mlflow_run.info.run_id if 'mlflow_run' in globals() else 'N/A',\n",
        "        'status': 'COMPLETADO'\n",
        "    }\n",
        "    \n",
        "    if 'DATA_DIR' in globals():\n",
        "        summary_path = DATA_DIR / 'final_summary.json'\n",
        "    else:\n",
        "        summary_path = RAW_DIR.parent / 'processed' / 'final_summary.json'\n",
        "        summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary_data, f, indent=2)\n",
        "\n",
        "    if 'mlflow_run' in globals():\n",
        "        mlflow.end_run()\n",
        "    \n",
        "    print(f\"\\nüíæ Resumen guardado en: {summary_path}\")\n",
        "    print(f\"\\nüéâ PERSONA 2: SETUP ML COMPLETADO EXITOSAMENTE\")\n",
        "\n",
        "# Generar resumen final\n",
        "generate_final_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Notas Importantes\n",
        "\n",
        "### ‚ö†Ô∏è Configuraci√≥n Requerida\n",
        "1. **Kaggle API**: Subir `kaggle.json` para descargar datasets\n",
        "2. **CID Dataset**: Reemplazar URL simulada con URL real\n",
        "3. **CattleEyeView**: Solicitar acceso a autores del paper\n",
        "\n",
        "### üîß Optimizaciones Implementadas\n",
        "- **Mixed Precision**: FP16 para acelerar entrenamiento\n",
        "- **Data Pipeline**: Cache + prefetch + shuffle optimizado\n",
        "- **Augmentation**: Albumentations espec√≠fico para ganado\n",
        "- **TFLite Export**: Optimizado para m√≥vil\n",
        "\n",
        "### üìä M√©tricas Objetivo\n",
        "- **R¬≤ ‚â• 0.95**: Explicaci√≥n 95% de varianza\n",
        "- **MAE < 5 kg**: Error absoluto promedio\n",
        "- **Inference < 3s**: Tiempo en m√≥vil\n",
        "\n",
        "### üéØ Estado Actual\n",
        "- ‚úÖ **Infraestructura ML**: Completada\n",
        "- ‚úÖ **Pipeline de datos**: Optimizado\n",
        "- ‚úÖ **Modelo base**: Listo para fine-tuning\n",
        "- üîÑ **Pr√≥ximo**: Fine-tuning por raza espec√≠fica\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

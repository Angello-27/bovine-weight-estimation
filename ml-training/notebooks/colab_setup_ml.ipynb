{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêÑ Sistema de Estimaci√≥n de Peso Bovino - Setup ML\n",
        "\n",
        "> **BLOQUE 0**: Informaci√≥n del proyecto (markdown - solo lectura)\n",
        "\n",
        "**Proyecto**: Hacienda Gamelera - Bruno Brito Macedo  \n",
        "**Responsable**: Persona 2 - Setup Infraestructura ML  \n",
        "**Objetivo**: Preparar datasets y pipeline para entrenamiento de 7 modelos por raza  \n",
        "**Duraci√≥n**: 5-6 d√≠as  \n",
        "\n",
        "---\n",
        "\n",
        "## üìë √çndice de Bloques (Referencia R√°pida)\n",
        "\n",
        "| Bloque | Nombre | Descripci√≥n | Requisitos |\n",
        "|--------|--------|-------------|------------|\n",
        "| **0** | Informaci√≥n | Markdown introductorio | Ninguno |\n",
        "| **1** | Clonar Repositorio | Monta Drive y clona desde GitHub (persistente) | Ninguno (requiere internet) |\n",
        "| **2** | Importar M√≥dulos | Importa m√≥dulos internos | Bloque 1 |\n",
        "| **3** | Ejemplo Modelo | Bloque de prueba (opcional) | Bloque 2 |\n",
        "| **4** | Instalar Dependencias Cr√≠ticas | TensorFlow 2.17, NumPy 1.26.4, MLflow, ml_dtypes | Ninguno |\n",
        "| **5** | Instalar Complementos | Albumentations, OpenCV, herramientas ML y verificaciones | Bloque 4 |\n",
        "| **6** | Imports Generales | Pandas, numpy, tensorflow | Bloque 5 |\n",
        "| **7** | Configuraci√≥n Proyecto | Crea carpetas en Drive (Drive ya montado en Bloque 1) | Bloque 6 |\n",
        "| **7.5** | Configurar Variables CID | Configura rutas CID Dataset | Bloque 7 (opcional) |\n",
        "| **8** | CID Dataset | Extrae CID Dataset | Bloque 7.5 + archivo comprimido |\n",
        "| **8.5** | Configurar Kaggle.json | Copia kaggle.json desde Drive | Bloque 7 (opcional) |\n",
        "| **9** | Kaggle Dataset | Descarga dataset Kaggle | Bloque 8.5 + kaggle.json |\n",
        "| **10** | Google Images | Scraping opcional | Bloque 7 (opcional) |\n",
        "| **11** | Resumen Datasets | Muestra resumen | Bloques 8-10 |\n",
        "| **12** | EDA CID Dataset | An√°lisis exploratorio | Bloque 8 + metadata.csv |\n",
        "| **13** | Visualizaciones EDA | Gr√°ficos interactivos | Bloque 12 |\n",
        "| **14** | An√°lisis por Raza | An√°lisis por raza | Bloque 12 |\n",
        "| **15** | Pipeline de Datos | Pipeline con augmentation | Bloque 12 |\n",
        "| **16** | Arquitectura Modelo | Crea modelo EfficientNetB0 | Bloque 15 |\n",
        "| **17** | Configurar Entrenamiento | Callbacks y MLflow | Bloque 16 |\n",
        "| **18** | Entrenamiento | Entrena modelo (2-4h) | Bloque 17 + GPU |\n",
        "| **19** | Evaluaci√≥n | Eval√∫a modelo (R¬≤, MAE) | Bloque 18 |\n",
        "| **20** | Exportar TFLite | Exporta a TFLite | Bloque 19 |\n",
        "| **21** | Resumen Final | Genera resumen completo | Todos los bloques |\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Checklist de Tareas\n",
        "- [x] D√≠a 1: Setup Google Colab Pro + dependencias\n",
        "- [ ] D√≠a 2-3: Descargar y organizar datasets cr√≠ticos\n",
        "- [ ] D√≠a 4: An√°lisis exploratorio de datos (EDA)\n",
        "- [ ] D√≠a 5-6: Preparar pipeline de datos optimizado\n",
        "\n",
        "## üéØ Razas Objetivo (7 razas)\n",
        "1. **Brahman** - Bos indicus robusto\n",
        "2. **Nelore** - Bos indicus\n",
        "3. **Angus** - Bos taurus, buena carne\n",
        "4. **Cebuinas** - Bos indicus general\n",
        "5. **Criollo** - Adaptado local\n",
        "6. **Pardo Suizo** - Bos taurus grande\n",
        "7. **Jersey** - Lechera, menor tama√±o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 1: CONFIGURAR RUTA DEL PROYECTO Y CLONAR REPOSITORIO EN DRIVE\n",
        "# ============================================================\n",
        "# üìÅ Clona el repositorio desde GitHub a Google Drive (persistente entre sesiones)\n",
        "# üîó Repositorio: https://github.com/Angello-27/bovine-weight-estimation.git\n",
        "# üíæ Se clona en Drive para que persista entre desconexiones del runtime\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# üîó URL del repositorio de GitHub\n",
        "GITHUB_REPO_URL = 'https://github.com/Angello-27/bovine-weight-estimation.git'\n",
        "\n",
        "# üîë Montar Google Drive primero (si no est√° montado)\n",
        "print(\"üîó Montando Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    \n",
        "    # Verificar si Drive ya est√° montado\n",
        "    drive_path = Path('/content/drive')\n",
        "    if not drive_path.exists() or not any(drive_path.iterdir()):\n",
        "        print(\"üìÅ Google Drive no est√° montado. Montando ahora...\")\n",
        "        print(\"üí° Se solicitar√° autorizaci√≥n para acceder a Google Drive\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive montado exitosamente\")\n",
        "    else:\n",
        "        print(\"‚úÖ Google Drive ya est√° montado\")\n",
        "    \n",
        "    # Usar Drive como ubicaci√≥n predeterminada (persistente)\n",
        "    DRIVE_BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
        "    USE_DRIVE = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è No se puede montar Google Drive (no estamos en Colab)\")\n",
        "    print(\"üí° Usando /content/ como ubicaci√≥n temporal\")\n",
        "    DRIVE_BASE_DIR = None\n",
        "    USE_DRIVE = False\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error al montar Google Drive: {e}\")\n",
        "    print(\"üí° Usando /content/ como ubicaci√≥n temporal\")\n",
        "    DRIVE_BASE_DIR = None\n",
        "    USE_DRIVE = False\n",
        "\n",
        "# ‚úÖ Ruta donde se clonar√° o est√° el repositorio\n",
        "#    - Preferencia 1: Google Drive (persistente entre sesiones)\n",
        "#    - Preferencia 2: /content/ (temporal, se pierde al desconectar)\n",
        "if USE_DRIVE and DRIVE_BASE_DIR:\n",
        "    BASE_DIR = DRIVE_BASE_DIR\n",
        "    print(f\"\\nüíæ Usando Google Drive: {BASE_DIR} (persistente entre sesiones)\")\n",
        "else:\n",
        "    BASE_DIR = Path('/content/bovine-weight-estimation')\n",
        "    print(f\"\\n‚ö†Ô∏è Usando ubicaci√≥n temporal: {BASE_DIR} (se pierde al desconectar)\")\n",
        "    print(\"üí° Para persistir, monta Google Drive primero\")\n",
        "\n",
        "# Ruta del proyecto ML Training\n",
        "ML_TRAINING_DIR = BASE_DIR / 'ml-training'\n",
        "\n",
        "# Validamos que la estructura del proyecto exista antes de continuar.\n",
        "if ML_TRAINING_DIR.exists() and (ML_TRAINING_DIR / 'src').exists():\n",
        "    print(f\"\\n‚úÖ Proyecto ya existe en: {ML_TRAINING_DIR}\")\n",
        "    print(\"üìÇ Subcarpetas clave detectadas:\")\n",
        "    print(f\"   - C√≥digo fuente: {ML_TRAINING_DIR / 'src'}\")\n",
        "    print(f\"   - Scripts utilitarios: {ML_TRAINING_DIR / 'scripts'}\")\n",
        "    print(f\"   - Configuraci√≥n: {ML_TRAINING_DIR / 'config'}\")\n",
        "    if USE_DRIVE:\n",
        "        print(f\"\\nüíæ El proyecto persiste entre sesiones porque est√° en Google Drive\")\n",
        "else:\n",
        "    print(f\"\\nüì• Proyecto no encontrado en: {ML_TRAINING_DIR}\")\n",
        "    print(f\"üîó Clonando repositorio desde GitHub: {GITHUB_REPO_URL}\")\n",
        "    if USE_DRIVE:\n",
        "        print(f\"üíæ Se clonar√° en Google Drive para que persista entre sesiones\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Se clonar√° en /content/ (temporal, se perder√° al desconectar)\")\n",
        "    \n",
        "    # Clonar repositorio si no existe\n",
        "    try:\n",
        "        # Crear directorio padre si no existe\n",
        "        BASE_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Eliminar directorio si existe pero est√° vac√≠o o incompleto\n",
        "        if BASE_DIR.exists() and not (ML_TRAINING_DIR / 'src').exists():\n",
        "            print(f\"‚ö†Ô∏è Directorio existe pero incompleto. Eliminando {BASE_DIR}...\")\n",
        "            import shutil\n",
        "            shutil.rmtree(BASE_DIR, ignore_errors=True)\n",
        "        \n",
        "        # Clonar repositorio\n",
        "        print(f\"üì• Clonando repositorio...\")\n",
        "        result = subprocess.run(\n",
        "            ['git', 'clone', GITHUB_REPO_URL, str(BASE_DIR)],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Repositorio clonado exitosamente en: {BASE_DIR}\")\n",
        "        print(f\"üìÇ Estructura del proyecto:\")\n",
        "        print(f\"   - C√≥digo fuente: {ML_TRAINING_DIR / 'src'}\")\n",
        "        print(f\"   - Scripts utilitarios: {ML_TRAINING_DIR / 'scripts'}\")\n",
        "        print(f\"   - Configuraci√≥n: {ML_TRAINING_DIR / 'config'}\")\n",
        "        \n",
        "        if USE_DRIVE:\n",
        "            print(f\"\\nüíæ El proyecto est√° en Google Drive y persistir√° entre sesiones\")\n",
        "            print(f\"üí° No necesitar√°s volver a clonarlo en futuras sesiones\")\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è El proyecto est√° en /content/ y se perder√° al desconectar el runtime\")\n",
        "            print(f\"üí° Considera montar Google Drive y volver a ejecutar este bloque\")\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error al clonar repositorio: {e}\")\n",
        "        print(f\"   stdout: {e.stdout}\")\n",
        "        print(f\"   stderr: {e.stderr}\")\n",
        "        print(\"\\nüí° Soluciones:\")\n",
        "        print(\"   1. Verifica que tienes conexi√≥n a internet en Colab\")\n",
        "        print(\"   2. Verifica que el repositorio existe: https://github.com/Angello-27/bovine-weight-estimation\")\n",
        "        print(\"   3. Si el repositorio es privado, ejecuta el BLOQUE 0.5 primero para configurar el token\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error inesperado: {e}\")\n",
        "        raise\n",
        "\n",
        "# A√±adimos la carpeta src al PYTHONPATH para que todos los m√≥dulos internos sean importables.\n",
        "sys.path.insert(0, str(ML_TRAINING_DIR / 'src'))\n",
        "\n",
        "# Verificaci√≥n final\n",
        "if ML_TRAINING_DIR.exists() and (ML_TRAINING_DIR / 'src').exists():\n",
        "    print(f\"\\n‚úÖ Configuraci√≥n completada correctamente\")\n",
        "    print(f\"üìÅ Directorio base: {BASE_DIR}\")\n",
        "    print(f\"üìÅ ML Training: {ML_TRAINING_DIR}\")\n",
        "    print(f\"üêç PYTHONPATH actualizado: {ML_TRAINING_DIR / 'src'}\")\n",
        "    if USE_DRIVE:\n",
        "        print(f\"üíæ Ubicaci√≥n: Google Drive (persistente entre sesiones)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Ubicaci√≥n: /content/ (temporal, se pierde al desconectar)\")\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        f\"No se pudo configurar el proyecto. Verifica que {ML_TRAINING_DIR} existe y contiene 'src'.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 2: IMPORTAR M√ìDULOS DEL PROYECTO\n",
        "# ============================================================\n",
        "# ‚úÖ Importa m√≥dulos internos del proyecto (requiere BLOQUE 1 exitoso)\n",
        "\n",
        "# Data Augmentation\n",
        "from data.augmentation import get_training_transform, get_aggressive_augmentation, get_validation_transform\n",
        "\n",
        "# Modelos\n",
        "from models.cnn_architecture import BreedWeightEstimatorCNN, BREED_CONFIGS\n",
        "\n",
        "# Evaluaci√≥n\n",
        "from models.evaluation.metrics import MetricsCalculator, ModelMetrics\n",
        "\n",
        "# Exportaci√≥n TFLite\n",
        "from models.export.tflite_converter import TFLiteExporter\n",
        "\n",
        "print(\"‚úÖ Todos los m√≥dulos importados correctamente\")\n",
        "print(\"\\nüì¶ M√≥dulos disponibles:\")\n",
        "print(\"   - Data augmentation (Albumentations 2.0.8)\")\n",
        "print(\"   - CNN architectures (MobileNetV2, EfficientNet)\")\n",
        "print(\"   - Metrics calculator (R¬≤, MAE, MAPE)\")\n",
        "print(\"   - TFLite exporter (optimizado para m√≥vil)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 3: EJEMPLO - CREAR MODELO PARA UNA RAZA (OPCIONAL)\n",
        "# ============================================================\n",
        "# üéì Bloque de prueba para verificar que los m√≥dulos funcionan\n",
        "# ‚ö†Ô∏è Puedes omitir este bloque si ya sabes que todo funciona\n",
        "\n",
        "# Ejemplo 1: Crear modelo para Brahman\n",
        "model_brahman = BreedWeightEstimatorCNN.build_model(\n",
        "    breed_name='brahman',\n",
        "    base_architecture='mobilenetv2'  # M√°s r√°pido que EfficientNet\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Modelo creado: {model_brahman.name}\")\n",
        "print(f\"üìä Par√°metros: {model_brahman.count_params():,}\")\n",
        "\n",
        "# Ver arquitectura\n",
        "print(\"\\nüìê Arquitectura del modelo:\")\n",
        "model_brahman.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Pr√≥ximos Pasos\n",
        "\n",
        "1. **Descargar datasets** (CID, CattleEyeView, etc.)\n",
        "2. **Preprocesar datos** con nuestros m√≥dulos\n",
        "3. **Entrenar modelo base** gen√©rico\n",
        "4. **Fine-tuning por raza** (5 razas)\n",
        "5. **Recolecci√≥n propia** (Criollo, Pardo Suizo)\n",
        "6. **Exportar a TFLite** e integrar en app m√≥vil\n",
        "\n",
        "> Ver `README.md` y `scripts/train_all_breeds.py` para m√°s ejemplos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## üöÄ D√≠a 1: Setup Google Colab Pro + Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 3.5: LIMPIEZA DE DEPENDENCIAS CONFLICTIVAS\n",
        "# ============================================================\n",
        "# üßπ Limpia versiones antiguas/conflictivas de TensorFlow y dependencias\n",
        "# ‚ö†Ô∏è Ejecuta este bloque ANTES del BLOQUE 4\n",
        "# üí° Esto evitar√° advertencias de compatibilidad en pip\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üßπ INICIANDO LIMPIEZA DE DEPENDENCIAS CONFLICTIVAS...\\n\")\n",
        "\n",
        "# Paso 1: Limpiar cach√© de pip\n",
        "print(\"üì¶ Limpiando cach√© de pip...\")\n",
        "!pip cache purge\n",
        "print(\"   ‚úÖ Cach√© de pip limpiado\\n\")\n",
        "\n",
        "# Paso 2: Desinstalar versiones antiguas de TensorFlow\n",
        "print(\"üì¶ Desinstalando versiones antiguas de TensorFlow...\")\n",
        "!pip uninstall -y -q tensorflow tensorflow-gpu tf-keras 2>/dev/null || true\n",
        "print(\"   ‚úÖ TensorFlow antiguo desinstalado\\n\")\n",
        "\n",
        "# Paso 3: Desinstalar paquetes problem√°ticos que dependen de versiones espec√≠ficas\n",
        "print(\"üì¶ Desinstalando paquetes con dependencias r√≠gidas...\")\n",
        "packages_to_remove = [\n",
        "    \"tensorflow-decision-forests\",\n",
        "    \"dopamine-rl\", \n",
        "    \"tensorflow-text\",\n",
        "    \"ydf\"\n",
        "]\n",
        "\n",
        "for package in packages_to_remove:\n",
        "    print(f\"   - Desinstalando {package}...\")\n",
        "    !pip uninstall -y -q {package} 2>/dev/null || true\n",
        "\n",
        "print(\"   ‚úÖ Paquetes problem√°ticos desinstalados\\n\")\n",
        "\n",
        "# Paso 4: Reinstalar TensorFlow 2.19 (versi√≥n de Colab)\n",
        "print(\"üì¶ Reinstalando TensorFlow 2.19.0 (versi√≥n limpia)...\")\n",
        "!pip install -q --force-reinstall --no-cache-dir \"tensorflow==2.19.0\"\n",
        "print(\"   ‚úÖ TensorFlow 2.19.0 reinstalado limpiamente\\n\")\n",
        "\n",
        "# Paso 5: Actualizar ml_dtypes y protobuf a versiones compatibles\n",
        "print(\"üì¶ Actualizando ml_dtypes y protobuf...\")\n",
        "!pip install -q --upgrade --no-cache-dir \"ml_dtypes>=0.5.0\"\n",
        "!pip install -q --upgrade --no-cache-dir \"protobuf>=5.26.1,<6.0\"\n",
        "print(\"   ‚úÖ ml_dtypes y protobuf actualizados\\n\")\n",
        "\n",
        "# Paso 6: Verificar instalaci√≥n limpia\n",
        "print(\"üîç Verificando instalaci√≥n limpia...\")\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import ml_dtypes\n",
        "    ml_dtypes_version = ml_dtypes.__version__\n",
        "except:\n",
        "    ml_dtypes_version = \"No disponible\"\n",
        "\n",
        "try:\n",
        "    import google.protobuf\n",
        "    protobuf_version = google.protobuf.__version__\n",
        "except:\n",
        "    protobuf_version = \"No disponible\"\n",
        "\n",
        "print(\"\\n‚úÖ ESTADO DESPU√âS DE LA LIMPIEZA:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "print(f\"   - NumPy: {np.__version__}\")\n",
        "print(f\"   - ml_dtypes: {ml_dtypes_version}\")\n",
        "print(f\"   - Protobuf: {protobuf_version}\")\n",
        "\n",
        "print(\"\\n‚úÖ LIMPIEZA COMPLETADA EXITOSAMENTE\")\n",
        "print(\"üí° Ahora ejecuta el BLOQUE 4 para instalar las dependencias sin conflictos\")\n",
        "print(\"üí° Si a√∫n hay advertencias menores, puedes ignorarlas - no afectar√°n el funcionamiento\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 4: INSTALACI√ìN DE DEPENDENCIAS CR√çTICAS (VERSI√ìN LIMPIA)\n",
        "# ============================================================\n",
        "# üîß Instala dependencias cr√≠ticas despu√©s de la limpieza del BLOQUE 3.5\n",
        "# ‚ö†Ô∏è Ejecuta SOLO despu√©s del BLOQUE 3.5\n",
        "# ‚úÖ Sin conflictos de versiones\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üì¶ INSTALANDO DEPENDENCIAS CR√çTICAS (ENTORNO LIMPIO)...\\n\")\n",
        "\n",
        "# Verificar que TensorFlow est√° limpio\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(f\"üîç Verificando versiones base:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "print(f\"   - NumPy: {np.__version__}\\n\")\n",
        "\n",
        "# Paso 1: Instalar MLflow con dependencias compatibles\n",
        "print(\"üì¶ Instalando MLflow (compatible con NumPy 2.x y Protobuf 5.x)...\")\n",
        "!pip install -q --no-cache-dir \"mlflow==2.16.2\"\n",
        "print(\"   ‚úÖ MLflow instalado\\n\")\n",
        "\n",
        "# Paso 2: Instalar DVC (opcional, para versionado de datos)\n",
        "print(\"üì¶ Instalando DVC...\")\n",
        "!pip install -q --no-cache-dir \"dvc[gs,s3]==3.51.1\"\n",
        "print(\"   ‚úÖ DVC instalado\\n\")\n",
        "\n",
        "# Paso 3: Actualizar scikit-learn\n",
        "print(\"üì¶ Actualizando scikit-learn...\")\n",
        "!pip install -q --upgrade --no-cache-dir \"scikit-learn>=1.6\"\n",
        "print(\"   ‚úÖ Scikit-learn actualizado\\n\")\n",
        "\n",
        "# Configurar mixed precision para GPU\n",
        "print(\"üîç Configurando TensorFlow...\")\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"   ‚úÖ Mixed precision (FP16) activado\\n\")\n",
        "\n",
        "# Verificar versiones finales\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ DEPENDENCIAS CR√çTICAS INSTALADAS (ENTORNO LIMPIO):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüì¶ VERSIONES INSTALADAS:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "print(f\"   - NumPy: {np.__version__}\")\n",
        "\n",
        "try:\n",
        "    import mlflow\n",
        "    print(f\"   - MLflow: {mlflow.__version__} ‚úÖ\")\n",
        "    mlflow_ok = True\n",
        "except Exception as e:\n",
        "    print(f\"   - MLflow: Error - {e} ‚ùå\")\n",
        "    mlflow_ok = False\n",
        "\n",
        "try:\n",
        "    import sklearn\n",
        "    print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
        "except:\n",
        "    print(f\"   - Scikit-learn: No disponible\")\n",
        "\n",
        "try:\n",
        "    import google.protobuf\n",
        "    print(f\"   - Protobuf: {google.protobuf.__version__}\")\n",
        "except:\n",
        "    print(f\"   - Protobuf: No disponible\")\n",
        "\n",
        "try:\n",
        "    import ml_dtypes\n",
        "    print(f\"   - ml_dtypes: {ml_dtypes.__version__}\")\n",
        "except:\n",
        "    print(f\"   - ml_dtypes: No disponible\")\n",
        "\n",
        "# Verificar compatibilidad\n",
        "print(f\"\\nüîç VERIFICACI√ìN DE COMPATIBILIDAD:\")\n",
        "tf_version_ok = tf.__version__.startswith('2.19')\n",
        "numpy_version_ok = np.__version__.startswith('2.0') or np.__version__.startswith('2.1')\n",
        "\n",
        "print(f\"   - TensorFlow 2.19.x: {'‚úÖ' if tf_version_ok else '‚ö†Ô∏è'}\")\n",
        "print(f\"   - NumPy 2.x: {'‚úÖ' if numpy_version_ok else '‚ö†Ô∏è'}\")\n",
        "print(f\"   - MLflow funcional: {'‚úÖ' if mlflow_ok else '‚ùå'}\")\n",
        "\n",
        "if tf_version_ok and numpy_version_ok and mlflow_ok:\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ INSTALACI√ìN COMPLETADA EXITOSAMENTE (SIN CONFLICTOS)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nüí° Contin√∫a con el BLOQUE 5 para instalar complementos\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Hay problemas de compatibilidad. Verifica los errores anteriores.\")\n",
        "    print(f\"üí° Intenta reiniciar el runtime: Entorno de ejecuci√≥n > Reiniciar sesi√≥n\")\n",
        "\n",
        "print(f\"\\nüìù NOTAS:\")\n",
        "print(f\"   - Entorno limpio sin paquetes conflictivos\")\n",
        "print(f\"   - TensorFlow 2.19 + NumPy 2.x + MLflow 2.16.2\")\n",
        "print(f\"   - Mixed precision (FP16) configurado para GPU\")\n",
        "print(f\"   - Sin advertencias de dependencias conflictivas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 5: INSTALACI√ìN DE COMPLEMENTOS Y VERIFICACIONES FINALES\n",
        "# ============================================================\n",
        "# üîß Instala complementos: Albumentations, OpenCV, herramientas ML, etc.\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 4 ejecutado exitosamente\n",
        "# üí° Este bloque instala herramientas adicionales y verifica todo el entorno\n",
        "\n",
        "# ‚ö†Ô∏è Ejecuta este bloque DESPU√âS del BLOQUE 4. Instala herramientas adicionales\n",
        "#    necesarias para el pipeline de datos y entrenamiento.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paso 1: Instalar Albumentations y OpenCV (compatibles con numpy 1.26.4)\n",
        "print(\"üì¶ Instalando Albumentations y OpenCV...\")\n",
        "!pip install -q \"albumentations==2.0.8\" \"opencv-python-headless==4.10.0.84\"\n",
        "\n",
        "# Paso 2: Instalar herramientas de ML y datos\n",
        "print(\"üì¶ Instalando herramientas de ML y datos...\")\n",
        "!pip install -q kaggle gdown plotly seaborn\n",
        "\n",
        "# Paso 3: Instalar dependencias adicionales\n",
        "print(\"üì¶ Instalando dependencias adicionales...\")\n",
        "!pip install -q \"pillow>=11.0.0\" \"packaging>=24.0\" google-images-download==2.8.0\n",
        "\n",
        "# Verificar versiones instaladas\n",
        "print(\"\\nüîç Verificando complementos instalados...\")\n",
        "import numpy as np\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import sklearn\n",
        "\n",
        "print(\"\\n‚úÖ COMPLEMENTOS INSTALADOS:\")\n",
        "print(f\"   - NumPy: {np.__version__}\")\n",
        "print(f\"   - OpenCV: {cv2.__version__}\")\n",
        "print(f\"   - Albumentations: {A.__version__}\")\n",
        "print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
        "\n",
        "# Verificar TensorFlow y GPU (configuraci√≥n final)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Verificar versi√≥n de TensorFlow instalada\n",
        "print(f\"\\n‚úÖ TensorFlow: {tf.__version__}\")\n",
        "\n",
        "# Configurar GPU si est√° disponible\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(\"‚úÖ GPU detectada y configurada correctamente\")\n",
        "        print(f\"   Dispositivos GPU: {len(gpus)}\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"   - GPU {i}: {gpu.name}\")\n",
        "        print(\"\\nüéÆ GPU lista para entrenamiento.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ö†Ô∏è Error configurando GPU: {e}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se detect√≥ GPU.\")\n",
        "    print(\"üí° Activa GPU desde: Entorno de ejecuci√≥n > Cambiar tipo de entorno > Acelerador de hardware > GPU\")\n",
        "    print(\"üí° Sin GPU, el entrenamiento ser√° m√°s lento pero funcionar√° correctamente.\")\n",
        "\n",
        "# Verificar que mixed precision est√° activado\n",
        "print(f\"\\n‚úÖ Mixed precision (FP16) activado para acelerar entrenamiento en GPU\")\n",
        "\n",
        "print(f\"\\n‚úÖ TODAS LAS DEPENDENCIAS INSTALADAS CORRECTAMENTE\")\n",
        "print(f\"üí° Puedes continuar con el BLOQUE 6 (Imports Generales)\")\n",
        "print(f\"üí° Las advertencias de pip sobre otros paquetes (shap, jax, etc.) no afectan el funcionamiento.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 6: IMPORTS Y CONFIGURACI√ìN GENERAL\n",
        "# ============================================================\n",
        "# üîç Importa todas las librer√≠as necesarias (pandas, numpy, tensorflow, mlflow, etc.)\n",
        "\n",
        "# üîç Conjunto completo de librer√≠as usadas en el pipeline: utilidades del sistema,\n",
        "#    ciencia de datos, visualizaci√≥n, ML y tracking de experimentos.\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from pathlib import Path\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# MLflow para tracking reproducible de experimentos.\n",
        "import mlflow\n",
        "import mlflow.tensorflow\n",
        "\n",
        "# Albumentations y OpenCV (requieren BLOQUE 5 ejecutado)\n",
        "try:\n",
        "    import cv2\n",
        "    import albumentations as A\n",
        "    cv2_version = cv2.__version__\n",
        "    albumentations_version = A.__version__\n",
        "    cv2_available = True\n",
        "except ImportError as e:\n",
        "    cv2_version = \"No disponible (ejecuta BLOQUE 5)\"\n",
        "    albumentations_version = \"No disponible (ejecuta BLOQUE 5)\"\n",
        "    cv2_available = False\n",
        "\n",
        "# Configurar matplotlib para que todas las gr√°ficas se vean consistentes en Colab.\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"‚úÖ Todas las dependencias importadas correctamente\")\n",
        "print(f\"üìä Versiones:\")\n",
        "print(f\"   - TensorFlow: {tf.__version__}\")\n",
        "if cv2_available:\n",
        "    print(f\"   - OpenCV: {cv2_version}\")\n",
        "    print(f\"   - Albumentations: {albumentations_version}\")\n",
        "else:\n",
        "    print(f\"   - OpenCV: {cv2_version}\")\n",
        "    print(f\"   - Albumentations: {albumentations_version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 7: CONFIGURACI√ìN DEL PROYECTO Y ESTRUCTURA DE CARPETAS\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Crea estructura de carpetas para datos y modelos en Drive\n",
        "# üìÅ Usa el mismo BASE_DIR del BLOQUE 1 (proyecto ya clonado en Drive)\n",
        "# üí° Drive ya est√° montado en el BLOQUE 1, as√≠ que solo verificamos\n",
        "\n",
        "from pathlib import Path\n",
        "import mlflow\n",
        "\n",
        "# üîó Verificar que Drive est√° montado (ya deber√≠a estar montado en el BLOQUE 1)\n",
        "drive_path = Path('/content/drive')\n",
        "if not drive_path.exists() or not any(drive_path.iterdir()):\n",
        "    print(\"‚ö†Ô∏è Google Drive no est√° montado. Montando ahora...\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive montado exitosamente\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al montar Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive debe estar montado. Ejecuta el BLOQUE 1 primero.\")\n",
        "else:\n",
        "    print('‚úÖ Google Drive ya est√° montado (montado en el BLOQUE 1)')\n",
        "\n",
        "# üìÅ Directorio base dentro de tu Drive (mismo que el BLOQUE 1)\n",
        "#    El proyecto ya est√° clonado aqu√≠ desde el BLOQUE 1\n",
        "BASE_DIR = Path('/content/drive/MyDrive/bovine-weight-estimation')\n",
        "\n",
        "# Verificar que el proyecto existe (deber√≠a existir desde el BLOQUE 1)\n",
        "if not BASE_DIR.exists():\n",
        "    print(f\"‚ö†Ô∏è El proyecto no existe en {BASE_DIR}\")\n",
        "    print(\"üí° Ejecuta el BLOQUE 1 primero para clonar el repositorio en Drive\")\n",
        "    raise RuntimeError(f\"El proyecto debe existir en {BASE_DIR}. Ejecuta el BLOQUE 1 primero.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Proyecto encontrado en: {BASE_DIR}\")\n",
        "\n",
        "# üìÇ Creamos (si no existen) las carpetas est√°ndar para datos crudos, procesados y modelos.\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "AUGMENTED_DIR = DATA_DIR / 'augmented'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "MLRUNS_DIR = BASE_DIR / 'mlruns'\n",
        "\n",
        "for dir_path in [DATA_DIR, RAW_DIR, PROCESSED_DIR, AUGMENTED_DIR, MODELS_DIR, MLRUNS_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üìä Configuraci√≥n de MLflow (tracking local persistente)\n",
        "# ------------------------------------------------------------\n",
        "mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")\n",
        "mlflow.set_experiment(\"bovine-weight-estimation\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ‚öôÔ∏è Configuraci√≥n general del entrenamiento (hiperpar√°metros base)\n",
        "# ------------------------------------------------------------\n",
        "CONFIG = {\n",
        "    'image_size': (224, 224),\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 0.001,\n",
        "    'validation_split': 0.2,\n",
        "    'test_split': 0.1,\n",
        "    'early_stopping_patience': 10,\n",
        "    'target_r2': 0.95,\n",
        "    'max_mae': 5.0,\n",
        "    'max_inference_time': 3.0\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üêÑ Razas objetivo (Santa Cruz, Chiquitan√≠a y Pampa)\n",
        "# ------------------------------------------------------------\n",
        "BREEDS = [\n",
        "    'brahman', 'nelore', 'angus', 'cebuinas',\n",
        "    'criollo', 'pardo_suizo', 'guzerat', 'holstein'\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completada correctamente\")\n",
        "print(f\"üìÅ Directorio base: {BASE_DIR}\")\n",
        "print(f\"üéØ Razas objetivo: {len(BREEDS)} razas -> {BREEDS}\")\n",
        "print(f\"üìä MLflow tracking: {MLRUNS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• D√≠a 2-3: Descargar y Organizar Datasets Cr√≠ticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 7.5: CONFIGURAR VARIABLES DE ENTORNO PARA DATASETS\n",
        "# ============================================================\n",
        "# üìÅ Configura rutas del CID Dataset y metadata antes de ejecutar BLOQUE 8\n",
        "# ‚ö†Ô∏è Ejecuta ANTES del BLOQUE 8 si tienes el CID Dataset en Drive\n",
        "# üëâ Ajusta las rutas seg√∫n donde est√©n tus archivos en Google Drive\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# üëâ Ruta del archivo comprimido del CID Dataset (zip, tar.gz, etc.)\n",
        "#    Ajusta seg√∫n donde subiste el archivo en Drive\n",
        "CID_DATASET_ARCHIVE_PATH = '/content/drive/MyDrive/bovine-weight-estimation/data/raw/cid_dataset.zip'\n",
        "# CID_DATASET_ARCHIVE_PATH = '/content/drive/MyDrive/datasets/cid_dataset.tar.gz'  # Ejemplo alternativo\n",
        "\n",
        "# üëâ Ruta del archivo metadata.csv del CID Dataset\n",
        "#    Ajusta seg√∫n donde est√© tu metadata.csv en Drive\n",
        "CID_METADATA_FILE = '/content/drive/MyDrive/bovine-weight-estimation/data/raw/cid/metadata.csv'\n",
        "# CID_METADATA_FILE = '/content/drive/MyDrive/datasets/cid_metadata.csv'  # Ejemplo alternativo\n",
        "\n",
        "# Configurar variables de entorno\n",
        "os.environ['CID_DATASET_ARCHIVE_PATH'] = CID_DATASET_ARCHIVE_PATH\n",
        "os.environ['CID_METADATA_FILE'] = CID_METADATA_FILE\n",
        "\n",
        "# Verificar que los archivos existan\n",
        "archive_path = Path(CID_DATASET_ARCHIVE_PATH)\n",
        "metadata_path = Path(CID_METADATA_FILE)\n",
        "\n",
        "if archive_path.exists():\n",
        "    print(f\"‚úÖ Archivo comprimido CID encontrado: {archive_path}\")\n",
        "    print(f\"   Tama√±o: {archive_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Archivo comprimido CID NO encontrado en: {archive_path}\")\n",
        "    print(\"üí° Sube el archivo comprimido del CID Dataset a Drive antes de ejecutar BLOQUE 8\")\n",
        "\n",
        "if metadata_path.exists():\n",
        "    print(f\"‚úÖ Metadata CSV encontrado: {metadata_path}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Metadata CSV NO encontrado en: {metadata_path}\")\n",
        "    print(\"üí° Sube el archivo metadata.csv del CID Dataset a Drive antes de ejecutar BLOQUE 12\")\n",
        "\n",
        "print(\"\\nüìã Variables de entorno configuradas:\")\n",
        "print(f\"   CID_DATASET_ARCHIVE_PATH = {CID_DATASET_ARCHIVE_PATH}\")\n",
        "print(f\"   CID_METADATA_FILE = {CID_METADATA_FILE}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 8.5: CONFIGURAR KAGGLE.JSON DESDE DRIVE (OPCIONAL)\n",
        "# ============================================================\n",
        "# üîë Copia kaggle.json desde Google Drive a /root/.kaggle/\n",
        "# ‚ö†Ô∏è Ejecuta SOLO si tienes kaggle.json en Drive y no lo has configurado a√∫n\n",
        "# üìÅ Ajusta la ruta KAGGLE_JSON_PATH seg√∫n donde est√© tu archivo en Drive\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# üëâ Ajusta esta ruta a donde est√© tu kaggle.json en Google Drive\n",
        "KAGGLE_JSON_PATH = Path('/content/drive/MyDrive/keys/kaggle.json')  # <--- CAMBIA ESTA RUTA\n",
        "# KAGGLE_JSON_PATH = Path('/content/drive/MyDrive/bovine-weight-estimation/kaggle.json')  # Ejemplo alternativo\n",
        "\n",
        "if KAGGLE_JSON_PATH.exists():\n",
        "    kaggle_dir = Path('/root/.kaggle')\n",
        "    kaggle_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    shutil.copy(KAGGLE_JSON_PATH, kaggle_dir / 'kaggle.json')\n",
        "    subprocess.run([\"chmod\", \"600\", \"/root/.kaggle/kaggle.json\"], check=True)\n",
        "    \n",
        "    print(\"‚úÖ kaggle.json copiado desde Drive a /root/.kaggle/\")\n",
        "    print(\"üîë Credenciales de Kaggle configuradas correctamente\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è No se encontr√≥ kaggle.json en: {KAGGLE_JSON_PATH}\")\n",
        "    print(\"üí° Ajusta KAGGLE_JSON_PATH o sube kaggle.json manualmente antes del BLOQUE 9\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 8: CID DATASET (17,899 im√°genes)\n",
        "# ============================================================\n",
        "# üì• Extrae el CID Dataset desde archivo comprimido subido a Drive\n",
        "# ‚ö†Ô∏è Requiere: Variable CID_DATASET_ARCHIVE_PATH apuntando al archivo .zip/.tar.gz\n",
        "\n",
        "CID_DATASET_ARCHIVE_PATH = os.environ.get('CID_DATASET_ARCHIVE_PATH')\n",
        "\n",
        "\n",
        "def download_cid_dataset(archive_path: str | None = CID_DATASET_ARCHIVE_PATH) -> Path:\n",
        "    \"\"\"Prepara el CID Dataset desde un archivo previamente descargado.\n",
        "\n",
        "    Requisitos antes de ejecutar:\n",
        "    1. Sube el archivo comprimido real (zip/tar) al directorio definido en BASE_DIR o /content.\n",
        "    2. Establece la variable de entorno CID_DATASET_ARCHIVE_PATH apuntando a ese archivo.\n",
        "\n",
        "    No se generan datos sint√©ticos: si falta el archivo, se detendr√° con un error.\n",
        "    \"\"\"\n",
        "    cid_dir = RAW_DIR / 'cid'\n",
        "    cid_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(cid_dir.iterdir()):\n",
        "        print(f\"‚ÑπÔ∏è CID Dataset ya est√° disponible en {cid_dir}. Se omite extracci√≥n.\")\n",
        "        return cid_dir\n",
        "\n",
        "    if archive_path is None:\n",
        "        raise RuntimeError(\n",
        "            \"Configura la variable de entorno CID_DATASET_ARCHIVE_PATH con la ruta del \"\n",
        "            \"archivo comprimido del CID Dataset (por ejemplo .zip o .tar.gz) antes de ejecutar esta celda.\"\n",
        "        )\n",
        "\n",
        "    archive_path = Path(archive_path)\n",
        "    if not archive_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontr√≥ el archivo comprimido del CID Dataset en {archive_path}. \"\n",
        "            \"Sube el dataset real a tu Google Drive y vuelve a ejecutar.\"\n",
        "        )\n",
        "\n",
        "    print(f\"üì• Extrayendo CID Dataset desde: {archive_path}\")\n",
        "    try:\n",
        "        shutil.unpack_archive(str(archive_path), str(cid_dir))\n",
        "    except shutil.ReadError as exc:\n",
        "        raise RuntimeError(\n",
        "            \"No se pudo desempaquetar el CID Dataset. Verifica que el archivo est√© en un formato soportado \"\n",
        "            \"(.zip, .tar, .tar.gz, .tar.bz2, etc.).\"\n",
        "        ) from exc\n",
        "\n",
        "    if not any(cid_dir.iterdir()):\n",
        "        raise RuntimeError(\n",
        "            \"La extracci√≥n del CID Dataset no produjo archivos. Verifica que el archivo comprimido contenga datos v√°lidos.\"\n",
        "        )\n",
        "\n",
        "    print(f\"‚úÖ CID Dataset preparado en: {cid_dir}\")\n",
        "    return cid_dir\n",
        "\n",
        "\n",
        "# Ejecutar preparaci√≥n (requerir√° archivo real previamente cargado)\n",
        "cid_dataset_path = download_cid_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 9: KAGGLE CATTLE WEIGHT DATASET (12k im√°genes)\n",
        "# ============================================================\n",
        "# üì• Descarga dataset de Kaggle usando API\n",
        "# ‚ö†Ô∏è Requiere: kaggle.json subido a /root/.kaggle/ (ver instrucciones)\n",
        "\n",
        "KAGGLE_DATASET_ID = os.environ.get(\n",
        "    'KAGGLE_DATASET_ID', 'sadhliroomyprime/cattle-weight-detection-model-dataset-12k'\n",
        ")\n",
        "\n",
        "\n",
        "def setup_kaggle_api() -> Path:\n",
        "    \"\"\"Configura la API de Kaggle para descargas reales.\"\"\"\n",
        "    print(\"üîë Configurando API de Kaggle...\")\n",
        "\n",
        "    kaggle_dir = Path('/root/.kaggle')\n",
        "    kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    kaggle_json = kaggle_dir / 'kaggle.json'\n",
        "    if not kaggle_json.exists():\n",
        "        raise FileNotFoundError(\n",
        "            \"No se encontr√≥ /root/.kaggle/kaggle.json. Descarga tu token desde \"\n",
        "            \"https://www.kaggle.com/account, s√∫belo al notebook y vuelve a ejecutar.\"\n",
        "        )\n",
        "\n",
        "    subprocess.run([\"chmod\", \"600\", \"/root/.kaggle/kaggle.json\"], check=True)\n",
        "    return kaggle_dir\n",
        "\n",
        "\n",
        "def download_kaggle_dataset(dataset_id: str = KAGGLE_DATASET_ID) -> Path:\n",
        "    \"\"\"Descarga el dataset de Kaggle indicado.\n",
        "\n",
        "    Requisitos:\n",
        "    - Subir `kaggle.json` (token API) a este notebook y colocarlo en /root/.kaggle/\n",
        "    - Definir KAGGLE_DATASET_ID si deseas descargar un dataset distinto al preset.\n",
        "    \"\"\"\n",
        "    if not dataset_id:\n",
        "        raise RuntimeError(\"Define la variable de entorno KAGGLE_DATASET_ID con el dataset a descargar.\")\n",
        "\n",
        "    kaggle_dir = setup_kaggle_api()\n",
        "    output_dir = RAW_DIR / 'kaggle'\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(output_dir.glob('**/*')):\n",
        "        print(f\"‚ÑπÔ∏è Dataset de Kaggle ya presente en {output_dir}. Se omite descarga.\")\n",
        "        return output_dir\n",
        "\n",
        "    print(f\"üì• Descargando dataset de Kaggle: {dataset_id}\")\n",
        "    subprocess.run([\n",
        "        \"kaggle\",\n",
        "        \"datasets\",\n",
        "        \"download\",\n",
        "        \"-d\",\n",
        "        dataset_id,\n",
        "        \"-p\",\n",
        "        str(output_dir),\n",
        "    ], check=True)\n",
        "\n",
        "    archive_files = list(output_dir.glob('*.zip'))\n",
        "    if not archive_files:\n",
        "        raise RuntimeError(\"La descarga de Kaggle no produjo archivos .zip. Verifica el ID del dataset.\")\n",
        "\n",
        "    for archive_file in archive_files:\n",
        "        print(f\"üì¶ Descomprimiendo {archive_file.name}\")\n",
        "        subprocess.run([\n",
        "            \"unzip\",\n",
        "            \"-q\",\n",
        "            str(archive_file),\n",
        "            \"-d\",\n",
        "            str(output_dir),\n",
        "        ], check=True)\n",
        "        archive_file.unlink()\n",
        "\n",
        "    if not any(output_dir.glob('**/*')):\n",
        "        raise RuntimeError(\"La extracci√≥n del dataset de Kaggle no produjo archivos. Revisa el contenido descargado.\")\n",
        "\n",
        "    print(f\"‚úÖ Kaggle dataset disponible en: {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "# Ejecutar descarga (requiere credenciales reales)\n",
        "kaggle_dataset_path = download_kaggle_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 10: GOOGLE IMAGES SCRAPING (OPCIONAL)\n",
        "# ============================================================\n",
        "# üñºÔ∏è Descarga im√°genes de Google Images para razas locales\n",
        "# ‚ö†Ô∏è Opcional: Solo ejecuta si necesitas complementar datasets\n",
        "# ‚ö†Ô∏è Cuidado: Respeta t√©rminos de uso y evita bloqueos\n",
        "\n",
        "def scrape_google_images():\n",
        "    \"\"\"Scraping de Google Images para razas locales.\n",
        "\n",
        "    Uso opcional para complementar razas poco representadas. Respeta los t√©rminos de uso\n",
        "    del motor de b√∫squeda y evita ejecutar m√∫ltiples veces para no ser bloqueado.\n",
        "    \"\"\"\n",
        "    print(\"üñºÔ∏è Scraping Google Images para razas locales...\")\n",
        "    \n",
        "    from google_images_download import google_images_download\n",
        "    \n",
        "    # Razas locales espec√≠ficas\n",
        "    breeds_local = [\n",
        "        'ganado criollo boliviano',\n",
        "        'guzerat bolivia', \n",
        "        'brahman chiquitania',\n",
        "        'nelore pantanal',\n",
        "        'angus bolivia',\n",
        "        'pardo suizo bolivia',\n",
        "        'jersey bolivia'\n",
        "    ]\n",
        "    \n",
        "    response = google_images_download.googleimagesdownload()\n",
        "    \n",
        "    scraped_count = 0\n",
        "    \n",
        "    for breed in breeds_local:\n",
        "        try:\n",
        "            print(f\"üì∏ Scraping: {breed}\")\n",
        "            \n",
        "            # Configuraci√≥n de descarga\n",
        "            arguments = {\n",
        "                \"keywords\": breed,\n",
        "                \"limit\": 50,  # L√≠mite por t√©rmino\n",
        "                \"print_urls\": False,\n",
        "                \"output_directory\": str(RAW_DIR / 'scraped'),\n",
        "                \"image_directory\": breed.replace(' ', '_'),\n",
        "                \"format\": \"jpg\",\n",
        "                \"size\": \"medium\",\n",
        "                \"aspect_ratio\": \"wide\"\n",
        "            }\n",
        "            \n",
        "            # Descargar im√°genes\n",
        "            paths = response.download(arguments)\n",
        "            \n",
        "            if paths:\n",
        "                count = len(paths[0])\n",
        "                scraped_count += count\n",
        "                print(f\"‚úÖ {breed}: {count} im√°genes descargadas\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error con {breed}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"üéØ Total im√°genes scraped: {scraped_count}\")\n",
        "    return scraped_count\n",
        "\n",
        "# Ejecutar scraping\n",
        "scraped_images = scrape_google_images()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 11: RESUMEN DE DATASETS DESCARGADOS\n",
        "# ============================================================\n",
        "# üìä Muestra resumen de todos los datasets disponibles\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 8, 9, 10 ejecutados (o al menos uno)\n",
        "\n",
        "def summarize_datasets(cid_df: pd.DataFrame | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Resumen de todos los datasets disponibles (solo datos reales).\"\"\"\n",
        "    print(\"üìä RESUMEN DE DATASETS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    datasets_info = []\n",
        "\n",
        "    if cid_df is not None:\n",
        "        datasets_info.append({\n",
        "            'name': 'CID Dataset',\n",
        "            'images': len(cid_df),\n",
        "            'description': 'Computer Vision Research - Cattle Image Database',\n",
        "            'status': '‚úÖ Disponible',\n",
        "        })\n",
        "    else:\n",
        "        datasets_info.append({\n",
        "            'name': 'CID Dataset',\n",
        "            'images': 0,\n",
        "            'description': 'CID sin metadata cargada',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    # Kaggle Dataset (maneja caso cuando a√∫n no existe)\n",
        "    try:\n",
        "        kaggle_path = kaggle_dataset_path if 'kaggle_dataset_path' in globals() else None\n",
        "        kaggle_id = KAGGLE_DATASET_ID if 'KAGGLE_DATASET_ID' in globals() else 'N/A'\n",
        "        if kaggle_path and Path(kaggle_path).exists():\n",
        "            kaggle_images = len(list(Path(kaggle_path).glob('**/*.jpg')))\n",
        "            datasets_info.append({\n",
        "                'name': 'Kaggle Cattle Weight',\n",
        "                'images': kaggle_images,\n",
        "                'description': f'Dataset Kaggle ({kaggle_id})',\n",
        "                'status': '‚úÖ Disponible' if kaggle_images > 0 else '‚ö†Ô∏è Vac√≠o',\n",
        "            })\n",
        "        else:\n",
        "            datasets_info.append({\n",
        "                'name': 'Kaggle Cattle Weight',\n",
        "                'images': 0,\n",
        "                'description': 'Requiere configuraci√≥n de API Kaggle',\n",
        "                'status': '‚ö†Ô∏è Pendiente',\n",
        "            })\n",
        "    except NameError:\n",
        "        datasets_info.append({\n",
        "            'name': 'Kaggle Cattle Weight',\n",
        "            'images': 0,\n",
        "            'description': 'Requiere ejecutar BLOQUE 9',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    # Google Images Scraped (maneja caso cuando a√∫n no existe)\n",
        "    try:\n",
        "        scraped_count = scraped_images if 'scraped_images' in globals() else 0\n",
        "        datasets_info.append({\n",
        "            'name': 'Google Images Scraped',\n",
        "            'images': scraped_count,\n",
        "            'description': 'Razas locales bolivianas',\n",
        "            'status': '‚úÖ Disponible' if scraped_count > 0 else '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "    except NameError:\n",
        "        datasets_info.append({\n",
        "            'name': 'Google Images Scraped',\n",
        "            'images': 0,\n",
        "            'description': 'Requiere ejecutar BLOQUE 10 (opcional)',\n",
        "            'status': '‚ö†Ô∏è Pendiente',\n",
        "        })\n",
        "\n",
        "    df_datasets = pd.DataFrame(datasets_info)\n",
        "    print(df_datasets.to_string(index=False))\n",
        "\n",
        "    total_images = int(df_datasets['images'].sum())\n",
        "    print(f\"\\nüéØ TOTAL IM√ÅGENES DISPONIBLES: {total_images:,}\")\n",
        "\n",
        "    summary_path = DATA_DIR / 'datasets_summary.csv'\n",
        "    df_datasets.to_csv(summary_path, index=False)\n",
        "    print(f\"\\nüíæ Resumen guardado en: {summary_path}\")\n",
        "\n",
        "    return df_datasets\n",
        "\n",
        "# Ejecutar resumen (maneja caso cuando df_cid a√∫n no existe)\n",
        "# ‚ö†Ô∏è Nota: Si ejecutas ANTES del BLOQUE 12, df_cid ser√° None y solo mostrar√° datasets de Kaggle/Google\n",
        "try:\n",
        "    # Verificar si df_cid existe en el scope global\n",
        "    df_cid_temp = df_cid if 'df_cid' in globals() else None\n",
        "    datasets_summary = summarize_datasets(df_cid_temp)\n",
        "except NameError:\n",
        "    # Si df_cid no existe a√∫n, ejecutar resumen sin metadata del CID\n",
        "    print(\"‚ÑπÔ∏è df_cid a√∫n no cargado. Ejecuta BLOQUE 12 para cargar metadata del CID Dataset.\")\n",
        "    datasets_summary = summarize_datasets(None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä D√≠a 4: An√°lisis Exploratorio de Datos (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 12: AN√ÅLISIS EXPLORATORIO - CID DATASET\n",
        "# ============================================================\n",
        "# üìä Carga y analiza metadata del CID Dataset\n",
        "# ‚ö†Ô∏è Requiere: CID_METADATA_FILE apuntando a metadata.csv\n",
        "\n",
        "CID_METADATA_FILE = Path(os.environ.get('CID_METADATA_FILE', cid_dataset_path / 'metadata.csv'))\n",
        "\n",
        "\n",
        "def analyze_cid_dataset(metadata_file: Path) -> pd.DataFrame:\n",
        "    \"\"\"An√°lisis exploratorio utilizando datos reales del CID Dataset.\"\"\"\n",
        "    if not metadata_file.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontr√≥ el archivo de metadata del CID Dataset en {metadata_file}. \"\n",
        "            \"Genera o coloca un CSV con las columnas ['image_path', 'weight_kg', 'breed', 'age_category', 'image_quality', 'lighting', 'angle'].\"\n",
        "        )\n",
        "\n",
        "    df_cid = pd.read_csv(metadata_file)\n",
        "\n",
        "    required_columns = {\n",
        "        'image_path',\n",
        "        'weight_kg',\n",
        "        'breed',\n",
        "        'age_category',\n",
        "        'image_quality',\n",
        "        'lighting',\n",
        "        'angle',\n",
        "    }\n",
        "    missing_columns = required_columns.difference(df_cid.columns)\n",
        "    if missing_columns:\n",
        "        raise ValueError(\n",
        "            f\"La metadata del CID Dataset no contiene las columnas requeridas: {sorted(missing_columns)}\"\n",
        "        )\n",
        "\n",
        "    print(\"üìä AN√ÅLISIS EXPLORATORIO - CID DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìà Total im√°genes: {len(df_cid):,}\")\n",
        "    print(f\"üìä Dimensiones: {df_cid.shape}\")\n",
        "\n",
        "    print(\"\\nüìã Columnas disponibles:\")\n",
        "    for col in df_cid.columns:\n",
        "        print(f\"  - {col}\")\n",
        "\n",
        "    print(\"\\n‚öñÔ∏è DISTRIBUCI√ìN DE PESO:\")\n",
        "    print(df_cid['weight_kg'].describe())\n",
        "\n",
        "    print(\"\\nüêÑ DISTRIBUCI√ìN POR RAZA:\")\n",
        "    print(df_cid['breed'].value_counts())\n",
        "\n",
        "    print(\"\\nüì∏ CALIDAD DE IM√ÅGENES:\")\n",
        "    print(df_cid['image_quality'].value_counts())\n",
        "\n",
        "    return df_cid\n",
        "\n",
        "\n",
        "# Ejecutar an√°lisis (requiere metadata real)\n",
        "df_cid = analyze_cid_dataset(CID_METADATA_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 13: VISUALIZACIONES EDA\n",
        "# ============================================================\n",
        "# üìä Crea gr√°ficos interactivos del an√°lisis exploratorio\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 12 ejecutado (df_cid cargado)\n",
        "\n",
        "def create_eda_visualizations(df):\n",
        "    \"\"\"Crear visualizaciones completas del EDA\"\"\"\n",
        "    print(\"üìä Creando visualizaciones EDA...\")\n",
        "    \n",
        "    # Configurar subplots\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=2,\n",
        "        subplot_titles=(\n",
        "            'Distribuci√≥n de Peso', 'Peso por Raza',\n",
        "            'Distribuci√≥n por Edad', 'Calidad de Im√°genes',\n",
        "            'Peso vs Iluminaci√≥n', 'Peso vs √Ångulo'\n",
        "        ),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "    )\n",
        "    \n",
        "    # 1. Distribuci√≥n de peso\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=df['weight_kg'], nbinsx=50, name='Peso (kg)',\n",
        "                    marker_color='lightblue', opacity=0.7),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Peso por raza\n",
        "    for breed in df['breed'].unique():\n",
        "        breed_data = df[df['breed'] == breed]['weight_kg']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=breed_data, name=breed, boxpoints='outliers'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "    \n",
        "    # 3. Distribuci√≥n por edad\n",
        "    age_counts = df['age_category'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=age_counts.index, y=age_counts.values, name='Categor√≠as de Edad',\n",
        "               marker_color='lightgreen'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Calidad de im√°genes\n",
        "    quality_counts = df['image_quality'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=quality_counts.index, values=quality_counts.values,\n",
        "               name='Calidad'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # 5. Peso vs Iluminaci√≥n\n",
        "    for lighting in df['lighting'].unique():\n",
        "        lighting_data = df[df['lighting'] == lighting]['weight_kg']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=lighting_data, name=lighting),\n",
        "            row=3, col=1\n",
        "        )\n",
        "    \n",
        "    # 6. Peso vs √Ångulo\n",
        "    for angle in df['angle'].unique():\n",
        "        angle_data = df[df['angle'] == angle]['weight_kg']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=angle_data, name=angle),\n",
        "            row=3, col=2\n",
        "        )\n",
        "    \n",
        "    # Configurar layout\n",
        "    fig.update_layout(\n",
        "        height=1200,\n",
        "        title_text=\"An√°lisis Exploratorio - CID Dataset\",\n",
        "        title_x=0.5,\n",
        "        showlegend=True\n",
        "    )\n",
        "    \n",
        "    # Mostrar gr√°fico\n",
        "    fig.show()\n",
        "    \n",
        "    # Guardar gr√°fico\n",
        "    fig.write_html(DATA_DIR / 'eda_visualizations.html')\n",
        "    print(f\"üíæ Visualizaciones guardadas en: {DATA_DIR / 'eda_visualizations.html'}\")\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Ejecutar visualizaciones\n",
        "eda_fig = create_eda_visualizations(df_cid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 14: AN√ÅLISIS ESPEC√çFICO POR RAZA\n",
        "# ============================================================\n",
        "# üêÑ Analiza qu√© razas tienen suficientes datos para entrenamiento\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 12 ejecutado (df_cid cargado)\n",
        "\n",
        "def analyze_breeds_for_training(df):\n",
        "    \"\"\"Analizar qu√© razas est√°n bien representadas para entrenamiento\"\"\"\n",
        "    print(\"üêÑ AN√ÅLISIS POR RAZA PARA ENTRENAMIENTO\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Razas objetivo del proyecto\n",
        "    target_breeds = ['brahman', 'nelore', 'angus', 'cebuinas', 'criollo', 'pardo_suizo', 'jersey']\n",
        "    \n",
        "    breed_analysis = []\n",
        "    \n",
        "    for breed in target_breeds:\n",
        "        # Buscar razas similares en el dataset\n",
        "        if breed in df['breed'].values:\n",
        "            breed_data = df[df['breed'] == breed]\n",
        "            count = len(breed_data)\n",
        "            avg_weight = breed_data['weight_kg'].mean()\n",
        "            std_weight = breed_data['weight_kg'].std()\n",
        "            \n",
        "            status = \"‚úÖ Suficiente\" if count >= 1000 else \"‚ö†Ô∏è Limitado\" if count >= 100 else \"‚ùå Insuficiente\"\n",
        "            \n",
        "        else:\n",
        "            # Buscar razas similares\n",
        "            similar_breeds = []\n",
        "            if breed in ['brahman', 'nelore', 'cebuinas']:\n",
        "                similar_breeds = ['mixed']  # Bos indicus\n",
        "            elif breed in ['angus']:\n",
        "                similar_breeds = ['mixed']  # Bos taurus\n",
        "            \n",
        "            count = sum(len(df[df['breed'] == sb]) for sb in similar_breeds)\n",
        "            avg_weight = df[df['breed'].isin(similar_breeds)]['weight_kg'].mean() if similar_breeds else 0\n",
        "            std_weight = df[df['breed'].isin(similar_breeds)]['weight_kg'].std() if similar_breeds else 0\n",
        "            \n",
        "            status = \"üîÑ Transfer Learning\" if count >= 1000 else \"‚ùå Recolecci√≥n requerida\"\n",
        "        \n",
        "        breed_analysis.append({\n",
        "            'breed': breed,\n",
        "            'images_available': count,\n",
        "            'avg_weight_kg': round(avg_weight, 1),\n",
        "            'std_weight_kg': round(std_weight, 1),\n",
        "            'status': status,\n",
        "            'strategy': 'Direct training' if count >= 1000 else 'Transfer learning' if count >= 100 else 'Data collection'\n",
        "        })\n",
        "    \n",
        "    # Crear DataFrame\n",
        "    df_breed_analysis = pd.DataFrame(breed_analysis)\n",
        "    \n",
        "    # Mostrar tabla\n",
        "    print(df_breed_analysis.to_string(index=False))\n",
        "    \n",
        "    # Guardar an√°lisis\n",
        "    df_breed_analysis.to_csv(DATA_DIR / 'breed_analysis.csv', index=False)\n",
        "    print(f\"\\nüíæ An√°lisis por raza guardado en: {DATA_DIR / 'breed_analysis.csv'}\")\n",
        "    \n",
        "    # Recomendaciones\n",
        "    print(f\"\\nüéØ RECOMENDACIONES:\")\n",
        "    \n",
        "    sufficient_breeds = df_breed_analysis[df_breed_analysis['images_available'] >= 1000]\n",
        "    if len(sufficient_breeds) > 0:\n",
        "        print(f\"‚úÖ Entrenamiento directo: {', '.join(sufficient_breeds['breed'].tolist())}\")\n",
        "    \n",
        "    transfer_breeds = df_breed_analysis[(df_breed_analysis['images_available'] >= 100) & (df_breed_analysis['images_available'] < 1000)]\n",
        "    if len(transfer_breeds) > 0:\n",
        "        print(f\"üîÑ Transfer learning: {', '.join(transfer_breeds['breed'].tolist())}\")\n",
        "    \n",
        "    collection_breeds = df_breed_analysis[df_breed_analysis['images_available'] < 100]\n",
        "    if len(collection_breeds) > 0:\n",
        "        print(f\"üì∏ Recolecci√≥n requerida: {', '.join(collection_breeds['breed'].tolist())}\")\n",
        "    \n",
        "    return df_breed_analysis\n",
        "\n",
        "# Ejecutar an√°lisis por raza\n",
        "breed_analysis = analyze_breeds_for_training(df_cid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß D√≠a 5-6: Preparar Pipeline de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 15: PIPELINE DE DATOS OPTIMIZADO\n",
        "# ============================================================\n",
        "# üîß Crea pipeline de datos con augmentation para entrenamiento\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 12 ejecutado (df_cid cargado)\n",
        "\n",
        "class CattleDataPipeline:\n",
        "    \"\"\"Pipeline de datos para entrenamiento de modelos de estimaci√≥n de peso\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, breeds_mapping=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.breeds_mapping = breeds_mapping or {}\n",
        "        \n",
        "        # Augmentation agresivo para datasets peque√±os\n",
        "        self.augmentation = A.Compose([\n",
        "            # Variaciones de iluminaci√≥n\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n",
        "            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, p=0.5),\n",
        "            \n",
        "            # Ruido y desenfoque\n",
        "            A.GaussNoise(var_limit=(5, 15), p=0.3),\n",
        "            A.Blur(blur_limit=3, p=0.25),\n",
        "            \n",
        "            # Efectos atmosf√©ricos\n",
        "            A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), p=0.4),\n",
        "            A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.2),\n",
        "            \n",
        "            # Transformaciones geom√©tricas\n",
        "            A.RandomRotate90(p=0.3),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.1, scale_limit=0.15, \n",
        "                rotate_limit=15, border_mode=cv2.BORDER_REFLECT, p=0.5\n",
        "            ),\n",
        "            \n",
        "            # Augmentation espec√≠fico para ganado\n",
        "            A.RandomCrop(height=200, width=200, p=0.3),  # Simular diferentes distancias\n",
        "            A.ElasticTransform(alpha=1, sigma=50, p=0.2),  # Deformaciones naturales\n",
        "            A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),\n",
        "        ])\n",
        "        \n",
        "        print(f\"‚úÖ Pipeline inicializado para: {self.data_dir}\")\n",
        "    \n",
        "    def load_and_preprocess(self, img_path: Path, weight: float) -> tuple[np.ndarray, float]:\n",
        "        \"\"\"Carga imagen, aplica augmentation y retorna tensores listos para el modelo.\"\"\"\n",
        "        if not img_path.exists():\n",
        "            raise FileNotFoundError(f\"Imagen no encontrada: {img_path}\")\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise ValueError(f\"No se pudo cargar la imagen: {img_path}\")\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        augmented = self.augmentation(image=img)\n",
        "        img = augmented['image']\n",
        "\n",
        "        img = cv2.resize(img, CONFIG['image_size'])\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        return img, float(weight)\n",
        "\n",
        "    def create_tf_dataset(self, df, split='train'):\n",
        "        \"\"\"Crea un tf.data.Dataset a partir de rutas reales.\"\"\"\n",
        "        print(f\"üîß Creando dataset TensorFlow para split: {split}\")\n",
        "\n",
        "        required_columns = {'image_path', 'weight_kg'}\n",
        "        missing_columns = required_columns.difference(df.columns)\n",
        "        if missing_columns:\n",
        "            raise ValueError(\n",
        "                f\"El DataFrame para el split '{split}' no contiene las columnas requeridas: {sorted(missing_columns)}\"\n",
        "            )\n",
        "\n",
        "        def data_generator():\n",
        "            for _, row in df.iterrows():\n",
        "                raw_path = Path(row['image_path'])\n",
        "                img_path = raw_path if raw_path.is_absolute() else self.data_dir / raw_path\n",
        "\n",
        "                img, weight = self.load_and_preprocess(img_path, row['weight_kg'])\n",
        "                yield img, weight\n",
        "\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            data_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=CONFIG['image_size'] + (3,), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.float32),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "        if split == 'train':\n",
        "            dataset = dataset.shuffle(1000)\n",
        "\n",
        "        dataset = dataset.batch(CONFIG['batch_size'])\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        print(f\"‚úÖ Dataset {split} creado con optimizaciones\")\n",
        "        return dataset\n",
        "    \n",
        "    def split_data(self, df):\n",
        "        \"\"\"Divide datos en train/val/test\"\"\"\n",
        "        print(\"üìä Dividiendo datos en train/val/test...\")\n",
        "        \n",
        "        # Shuffle datos\n",
        "        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        \n",
        "        # Calcular splits\n",
        "        n_total = len(df_shuffled)\n",
        "        n_train = int(n_total * (1 - CONFIG['validation_split'] - CONFIG['test_split']))\n",
        "        n_val = int(n_total * CONFIG['validation_split'])\n",
        "        \n",
        "        # Dividir\n",
        "        df_train = df_shuffled[:n_train]\n",
        "        df_val = df_shuffled[n_train:n_train + n_val]\n",
        "        df_test = df_shuffled[n_train + n_val:]\n",
        "        \n",
        "        print(f\"üìà Train: {len(df_train):,} ({len(df_train)/n_total*100:.1f}%)\")\n",
        "        print(f\"üìà Val: {len(df_val):,} ({len(df_val)/n_total*100:.1f}%)\")\n",
        "        print(f\"üìà Test: {len(df_test):,} ({len(df_test)/n_total*100:.1f}%)\")\n",
        "        \n",
        "        return df_train, df_val, df_test\n",
        "\n",
        "# Crear pipeline\n",
        "pipeline = CattleDataPipeline(RAW_DIR)\n",
        "\n",
        "# Dividir datos\n",
        "df_train, df_val, df_test = pipeline.split_data(df_cid)\n",
        "\n",
        "# Crear datasets TensorFlow\n",
        "train_dataset = pipeline.create_tf_dataset(df_train, 'train')\n",
        "val_dataset = pipeline.create_tf_dataset(df_val, 'val')\n",
        "test_dataset = pipeline.create_tf_dataset(df_test, 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 16: ARQUITECTURA DEL MODELO\n",
        "# ============================================================\n",
        "# üèóÔ∏è Crea modelo EfficientNetB0 con transfer learning\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 15 ejecutado (pipeline creado)\n",
        "\n",
        "def create_weight_estimation_model():\n",
        "    \"\"\"Crear modelo para estimaci√≥n de peso\"\"\"\n",
        "    print(\"üèóÔ∏è Creando arquitectura del modelo...\")\n",
        "    \n",
        "    # Base model con transfer learning\n",
        "    base_model = EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=CONFIG['image_size'] + (3,)\n",
        "    )\n",
        "    \n",
        "    # Congelar capas iniciales\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Custom head para regresi√≥n\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(256, activation='relu', name='dense_1')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu', name='dense_2')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    # Salida: peso estimado en kg\n",
        "    output = layers.Dense(1, activation='linear', name='weight_output')(x)\n",
        "    \n",
        "    # Crear modelo\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "    \n",
        "    # Compilar modelo\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
        "        loss='mse',\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Modelo creado con {model.count_params():,} par√°metros\")\n",
        "    print(f\"üìä Arquitectura: EfficientNetB0 + Custom Head\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Crear modelo\n",
        "model = create_weight_estimation_model()\n",
        "\n",
        "# Mostrar resumen\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 17: CONFIGURACI√ìN DE ENTRENAMIENTO\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Configura callbacks (EarlyStopping, ReduceLR, ModelCheckpoint, TensorBoard)\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 16 ejecutado (modelo creado)\n",
        "\n",
        "def setup_training_callbacks():\n",
        "    \"\"\"Configurar callbacks para entrenamiento\"\"\"\n",
        "    print(\"‚öôÔ∏è Configurando callbacks de entrenamiento...\")\n",
        "    \n",
        "    callbacks_list = [\n",
        "        # Early stopping\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=CONFIG['early_stopping_patience'],\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Reduce learning rate on plateau\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Model checkpoint\n",
        "        callbacks.ModelCheckpoint(\n",
        "            filepath=str(MODELS_DIR / 'best_model.h5'),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # TensorBoard\n",
        "        callbacks.TensorBoard(\n",
        "            log_dir=str(BASE_DIR / 'logs'),\n",
        "            histogram_freq=1,\n",
        "            write_graph=True,\n",
        "            write_images=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úÖ {len(callbacks_list)} callbacks configurados\")\n",
        "    return callbacks_list\n",
        "\n",
        "# Configurar callbacks\n",
        "training_callbacks = setup_training_callbacks()\n",
        "\n",
        "# Configurar MLflow\n",
        "def start_mlflow_run():\n",
        "    \"\"\"Iniciar run de MLflow\"\"\"\n",
        "    run = mlflow.start_run(run_name=\"cattle-weight-base-model\")\n",
        "\n",
        "    mlflow.log_params({\n",
        "        'dataset': 'CID',\n",
        "        'model': 'EfficientNetB0',\n",
        "        'batch_size': CONFIG['batch_size'],\n",
        "        'learning_rate': CONFIG['learning_rate'],\n",
        "        'epochs': CONFIG['epochs'],\n",
        "        'image_size': CONFIG['image_size'],\n",
        "        'augmentation': 'Albumentations'\n",
        "    })\n",
        "\n",
        "    print(f\"üî¨ MLflow run iniciado: {run.info.run_id}\")\n",
        "    return run\n",
        "\n",
        "# Iniciar MLflow run\n",
        "mlflow_run = start_mlflow_run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 18: ENTRENAMIENTO DEL MODELO\n",
        "# ============================================================\n",
        "# üöÄ Entrena el modelo base (puede tardar horas con GPU)\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 17 ejecutado (callbacks configurados)\n",
        "# ‚ö†Ô∏è Tiempo estimado: 2-4 horas con GPU T4 (100 √©pocas)\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Entrenar modelo base\"\"\"\n",
        "    print(\"üöÄ Iniciando entrenamiento del modelo base...\")\n",
        "    print(f\"üìä Configuraci√≥n: {CONFIG}\")\n",
        "    \n",
        "    # Calcular steps por √©poca\n",
        "    steps_per_epoch = len(df_train) // CONFIG['batch_size']\n",
        "    validation_steps = len(df_val) // CONFIG['batch_size']\n",
        "    \n",
        "    print(f\"üìà Steps por √©poca: {steps_per_epoch}\")\n",
        "    print(f\"üìà Validation steps: {validation_steps}\")\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=CONFIG['epochs'],\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=training_callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Entrenamiento completado\")\n",
        "    return history\n",
        "\n",
        "# Entrenamiento real (requiere datasets preparados y tiempo de ejecuci√≥n con GPU)\n",
        "history = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 19: EVALUACI√ìN DEL MODELO\n",
        "# ============================================================\n",
        "# üìä Eval√∫a el modelo en conjunto de test (calcula R¬≤, MAE, MSE)\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 18 ejecutado (modelo entrenado)\n",
        "\n",
        "def evaluate_model():\n",
        "    \"\"\"Evaluar modelo en conjunto de test\"\"\"\n",
        "    print(\"üìä Evaluando modelo en conjunto de test...\")\n",
        "    \n",
        "    # Evaluar modelo\n",
        "    test_loss, test_mae, test_mse = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "    # Calcular R¬≤ real con predicciones sobre el conjunto de test\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for batch_images, batch_targets in test_dataset:\n",
        "        predictions = model.predict(batch_images, verbose=0)\n",
        "        y_true.extend(batch_targets.numpy().astype(float))\n",
        "        y_pred.extend(predictions.squeeze().astype(float))\n",
        "\n",
        "    test_r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"üìà RESULTADOS DE EVALUACI√ìN:\")\n",
        "    print(f\"   Loss: {test_loss:.2f}\")\n",
        "    print(f\"   MAE: {test_mae:.2f} kg\")\n",
        "    print(f\"   MSE: {test_mse:.2f}\")\n",
        "    print(f\"   R¬≤: {test_r2:.3f}\")\n",
        "    \n",
        "    # Verificar objetivos\n",
        "    print(f\"\\nüéØ VERIFICACI√ìN DE OBJETIVOS:\")\n",
        "    print(f\"   R¬≤ ‚â• {CONFIG['target_r2']}: {'‚úÖ' if test_r2 >= CONFIG['target_r2'] else '‚ùå'} ({test_r2:.3f})\")\n",
        "    print(f\"   MAE < {CONFIG['max_mae']} kg: {'‚úÖ' if test_mae < CONFIG['max_mae'] else '‚ùå'} ({test_mae:.2f} kg)\")\n",
        "    \n",
        "    # Log m√©tricas en MLflow\n",
        "    mlflow.log_metrics({\n",
        "        'test_loss': test_loss,\n",
        "        'test_mae': test_mae,\n",
        "        'test_mse': test_mse,\n",
        "        'test_r2': test_r2\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        'loss': test_loss,\n",
        "        'mae': test_mae,\n",
        "        'mse': test_mse,\n",
        "        'r2': test_r2\n",
        "    }\n",
        "\n",
        "# Evaluar modelo\n",
        "evaluation_results = evaluate_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 20: EXPORTAR A TFLITE\n",
        "# ============================================================\n",
        "# üì± Exporta modelo entrenado a TFLite optimizado para m√≥vil\n",
        "# ‚ö†Ô∏è Requiere: BLOQUE 19 ejecutado (modelo evaluado)\n",
        "# üìÅ Guarda modelo en MODELS_DIR/generic-cattle-v1.0.0.tflite\n",
        "\n",
        "def export_to_tflite(model, output_path):\n",
        "    \"\"\"Exporta modelo a TFLite optimizado para m√≥vil\"\"\"\n",
        "    print(f\"üì± Exportando modelo a TFLite: {output_path}\")\n",
        "    \n",
        "    # Configurar conversor\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    \n",
        "    # Optimizaciones para m√≥vil\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]  # FP16 para velocidad\n",
        "    \n",
        "    # Cuantizaci√≥n INT8 (opcional, m√°s agresiva)\n",
        "    # converter.representative_dataset = representative_data_gen\n",
        "    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    \n",
        "    # Convertir\n",
        "    tflite_model = converter.convert()\n",
        "    \n",
        "    # Guardar\n",
        "    with open(output_path, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    \n",
        "    # Informaci√≥n del modelo\n",
        "    model_size_kb = len(tflite_model) / 1024\n",
        "    model_size_mb = model_size_kb / 1024\n",
        "    print(\"‚úÖ Modelo exportado exitosamente\")\n",
        "    print(f\"üìè Tama√±o: {model_size_mb:.2f} MB ({model_size_kb:.1f} KB)\")\n",
        "    print(\"üì± Optimizado para m√≥vil: FP16\")\n",
        "    \n",
        "    # Log en MLflow\n",
        "    mlflow.log_artifact(output_path)\n",
        "    mlflow.log_metric('model_size_kb', model_size_kb)\n",
        "    mlflow.log_metric('model_size_mb', model_size_mb)\n",
        "    \n",
        "    return model_size_kb\n",
        "\n",
        "# Exportar modelo base\n",
        "tflite_path = MODELS_DIR / 'generic-cattle-v1.0.0.tflite'\n",
        "model_size = export_to_tflite(model, tflite_path)\n",
        "\n",
        "print(\"\\nüéØ MODELO BASE LISTO PARA INTEGRACI√ìN\")\n",
        "print(f\"üìÅ Archivo: {tflite_path}\")\n",
        "print(f\"üìè Tama√±o: {model_size / 1024:.2f} MB ({model_size:.1f} KB)\")\n",
        "print(f\"üî¨ MLflow run: {mlflow_run.info.run_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Resumen y Pr√≥ximos Pasos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOQUE 21: RESUMEN FINAL\n",
        "# ============================================================\n",
        "# üìã Genera resumen completo del trabajo realizado\n",
        "# ‚ö†Ô∏è Requiere: Todos los bloques anteriores ejecutados\n",
        "# üíæ Guarda resumen en DATA_DIR/final_summary.json\n",
        "\n",
        "def generate_final_summary():\n",
        "    \"\"\"Generar resumen final del trabajo realizado\"\"\"\n",
        "    print(\"üìã RESUMEN FINAL - PERSONA 2: SETUP ML\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Resumen de datasets\n",
        "    print(f\"\\nüì• DATASETS PROCESADOS:\")\n",
        "    cid_row = datasets_summary[datasets_summary['name'] == 'CID Dataset']\n",
        "    cid_images = int(cid_row['images'].iloc[0]) if not cid_row.empty else 0\n",
        "    print(f\"   {'‚úÖ' if cid_images else '‚ö†Ô∏è'} CID Dataset: {cid_images:,} im√°genes\")\n",
        "    print(f\"   {'‚úÖ' if scraped_images else '‚ö†Ô∏è'} Google Images: {scraped_images:,} im√°genes locales\")\n",
        "\n",
        "    if kaggle_dataset_path and kaggle_dataset_path.exists():\n",
        "        kaggle_images = len(list(kaggle_dataset_path.glob('**/*.jpg')))\n",
        "        status_icon = '‚úÖ' if kaggle_images else '‚ö†Ô∏è'\n",
        "        print(f\"   {status_icon} Kaggle Dataset ({KAGGLE_DATASET_ID}): {kaggle_images:,} im√°genes\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è Kaggle Dataset: Pendiente configuraci√≥n (sube kaggle.json y ejecuta la celda correspondiente)\")\n",
        "    \n",
        "    # Resumen de an√°lisis\n",
        "    print(f\"\\nüìä AN√ÅLISIS COMPLETADO:\")\n",
        "    print(f\"   ‚úÖ EDA completo con visualizaciones\")\n",
        "    print(f\"   ‚úÖ An√°lisis por raza para estrategia de entrenamiento\")\n",
        "    print(f\"   ‚úÖ Pipeline de datos optimizado\")\n",
        "    \n",
        "    # Resumen de modelo\n",
        "    print(f\"\\nü§ñ MODELO BASE:\")\n",
        "    print(f\"   ‚úÖ Arquitectura: EfficientNetB0 + Custom Head\")\n",
        "    print(f\"   ‚úÖ Par√°metros: {model.count_params():,}\")\n",
        "    print(f\"   ‚úÖ TFLite exportado: {model_size / 1024:.2f} MB ({model_size:.1f} KB)\")\n",
        "    print(f\"   ‚úÖ MLflow tracking: {mlflow_run.info.run_id}\")\n",
        "    \n",
        "    # Pr√≥ximos pasos\n",
        "    print(f\"\\nüéØ PR√ìXIMOS PASOS:\")\n",
        "    print(f\"   1. üîÑ Fine-tuning por raza (Semanas 3-6)\")\n",
        "    print(f\"   2. üì∏ Recolecci√≥n Criollo + Pardo Suizo (Semanas 7-8)\")\n",
        "    print(f\"   3. üß™ Entrenamiento final (Semanas 9-10)\")\n",
        "    print(f\"   4. üì± Integraci√≥n en app m√≥vil\")\n",
        "    \n",
        "    # Guardar resumen\n",
        "    summary_data = {\n",
        "        'completion_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'datasets_processed': len(datasets_summary),\n",
        "        'total_images': datasets_summary['images'].sum(),\n",
        "        'model_architecture': 'EfficientNetB0',\n",
        "        'model_size_kb': model_size,\n",
        "        'mlflow_run_id': mlflow_run.info.run_id,\n",
        "        'status': 'COMPLETADO'\n",
        "    }\n",
        "    \n",
        "    with open(DATA_DIR / 'final_summary.json', 'w') as f:\n",
        "        json.dump(summary_data, f, indent=2)\n",
        "\n",
        "    mlflow.end_run()\n",
        "    \n",
        "    print(f\"\\nüíæ Resumen guardado en: {DATA_DIR / 'final_summary.json'}\")\n",
        "    print(f\"\\nüéâ PERSONA 2: SETUP ML COMPLETADO EXITOSAMENTE\")\n",
        "\n",
        "# Generar resumen final\n",
        "generate_final_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Notas Importantes\n",
        "\n",
        "### ‚ö†Ô∏è Configuraci√≥n Requerida\n",
        "1. **Kaggle API**: Subir `kaggle.json` para descargar datasets\n",
        "2. **CID Dataset**: Reemplazar URL simulada con URL real\n",
        "3. **CattleEyeView**: Solicitar acceso a autores del paper\n",
        "\n",
        "### üîß Optimizaciones Implementadas\n",
        "- **Mixed Precision**: FP16 para acelerar entrenamiento\n",
        "- **Data Pipeline**: Cache + prefetch + shuffle optimizado\n",
        "- **Augmentation**: Albumentations espec√≠fico para ganado\n",
        "- **TFLite Export**: Optimizado para m√≥vil\n",
        "\n",
        "### üìä M√©tricas Objetivo\n",
        "- **R¬≤ ‚â• 0.95**: Explicaci√≥n 95% de varianza\n",
        "- **MAE < 5 kg**: Error absoluto promedio\n",
        "- **Inference < 3s**: Tiempo en m√≥vil\n",
        "\n",
        "### üéØ Estado Actual\n",
        "- ‚úÖ **Infraestructura ML**: Completada\n",
        "- ‚úÖ **Pipeline de datos**: Optimizado\n",
        "- ‚úÖ **Modelo base**: Listo para fine-tuning\n",
        "- üîÑ **Pr√≥ximo**: Fine-tuning por raza espec√≠fica\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
